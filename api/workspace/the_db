{"filename":"/Users/eduardoboucas/Sites/publish-dev/api/workspace/the_db","collections":[{"name":"client-store","data":[{"clientId":"api-client","secret":"client-secret","accessType":"admin","_id":"1a332a0d-bed8-4974-9b5c-95b2c1fcdafd","meta":{"revision":0,"created":1530490848550,"version":0},"$loki":1}],"idIndex":[1],"binaryIndices":{},"constraints":null,"uniqueNames":[],"transforms":{},"objType":"client-store","dirty":false,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":false,"cloneMethod":"parse-stringify","asyncListeners":false,"disableChangesApi":true,"autoupdate":false,"ttl":null,"maxId":0,"DynamicViews":[],"events":{"insert":[null],"update":[null],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[]},{"name":"token-store","data":[{"created":"2018-07-02T01:17:29.504Z","token":"56f217b2-cc65-427e-be84-f19ab475626e","tokenExpire":1532294249504,"value":{"clientId":"api-client","secret":"client-secret","accessType":"admin","_id":"1a332a0d-bed8-4974-9b5c-95b2c1fcdafd","meta":{"revision":0,"created":1530490848550,"version":0},"$loki":1},"_id":"1bc96564-54b6-40d1-9c5b-7ad6e24096e5","meta":{"revision":0,"created":1530494249505,"version":0},"$loki":1},{"created":"2018-07-02T01:23:54.790Z","token":"b041bf04-b957-49a8-837f-2eba7191f840","tokenExpire":1532294634790,"value":{"clientId":"api-client","secret":"client-secret","accessType":"admin","_id":"1a332a0d-bed8-4974-9b5c-95b2c1fcdafd","meta":{"revision":0,"created":1530490848550,"version":0},"$loki":1},"_id":"34bef8fa-7dcb-4f4b-a4ff-bf7fafc1bdc4","meta":{"revision":0,"created":1530494634790,"version":0},"$loki":2},{"created":"2018-07-03T10:08:32.118Z","token":"042eab1b-3011-48ec-a15d-3c7c188cd1b5","tokenExpire":1532412512118,"value":{"clientId":"api-client","secret":"client-secret","accessType":"admin","_id":"1a332a0d-bed8-4974-9b5c-95b2c1fcdafd","meta":{"revision":0,"created":1530490848550,"version":0},"$loki":1},"_id":"851888ff-2885-4c6d-8243-c4ca4c8adbc8","meta":{"revision":0,"created":1530612512119,"version":0},"$loki":3},{"created":"2018-07-03T10:08:33.019Z","token":"e3885670-729f-482d-abbd-3aafc838df47","tokenExpire":1532412513019,"value":{"clientId":"api-client","secret":"client-secret","accessType":"admin","_id":"1a332a0d-bed8-4974-9b5c-95b2c1fcdafd","meta":{"revision":0,"created":1530490848550,"version":0},"$loki":1},"_id":"7c2cf638-174a-42a7-8645-452de1c3ef9f","meta":{"revision":0,"created":1530612513019,"version":0},"$loki":4}],"idIndex":[1,2,3,4],"binaryIndices":{"token":{"name":"token","dirty":false,"values":[2,0,1,3]}},"constraints":null,"uniqueNames":[],"transforms":{},"objType":"token-store","dirty":true,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":false,"cloneMethod":"parse-stringify","asyncListeners":false,"disableChangesApi":true,"autoupdate":false,"ttl":null,"maxId":4,"DynamicViews":[],"events":{"insert":[null],"update":[null],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[]},{"name":"users","data":[],"idIndex":[],"binaryIndices":{"name":{"name":"name","dirty":false,"values":[]}},"constraints":null,"uniqueNames":[],"transforms":{},"objType":"users","dirty":false,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":false,"cloneMethod":"parse-stringify","asyncListeners":false,"disableChangesApi":true,"autoupdate":false,"ttl":null,"maxId":0,"DynamicViews":[],"events":{"insert":[null],"update":[null],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[]},{"name":"articles","data":[{"_apiVersion":"1.0","_createdAt":1530494364853,"_lastModifiedAt":1526417940515,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"body":"## Overview\n\nTraditional product design is channel and device centric. But users inhabit a multi-channel, multi-device world.\n\nChannel and/or device centric product design results in duplicated effort and wasted engineering work. API-first development is focused on removing this technical debt through the separation of the data backend and the data consuming front end.\n\nAPI-first development is the idea that whenever you are developing a piece of shared functionality for your organization it should be exposed as a RESTful HTTP(S) API to all of your other developers. Rather than creating a library or module that needs to be added to all code bases requiring the functionality, developers can consume all the necessary functionality through the API. Having developers consume all functionality through an API enforces separation of concerns and hides internal complexity.\n\nCOPE stands for Create Once, Publish Everywhere. It is about reducing editorial overhead by freeing content for use in multiple different contexts. Simply put, COPE separates data from design, making your content reusable and future-proof for new devices or platforms.\n\nTaking an API-first development approach enables COPE and brings several additional benefits:\n\n1. [Separation of concerns](#1-separation-of-concerns)\n2. [Scalability](#2-scalability)\n3. [Reduction of language barriers](#3-reduction-of-language-barriers)\n4. [Developer liberation and specialization](#4-developer-liberation-and-specialization)\n5. [Openness and future consumer availability](#5-openness-and-future-consumer-availability)\n6. [Modularity](#6-modularity)\n\n## 1. Separation of concerns\n\nAPI-first development is the formal separation of the front end from the back end.\n\nSimilar to the Model View Controller paradigm, by decoupling data from logic from presentation, it forces a better code architecture, which in the long term decreases your technical debt. API-first development makes it easy to push data to multiple views, regardless of size or functionality.\n\n## 2. Scalability\n\nCompletely separating your front end and back end codebases helps to simplify future scalability by enabling you to scale platform components independently of each other. It allows for the client and server to sit behind their own load balancers and in their own infrastructure, giving you the ability to scale on a micro-level which brings flexibility (for example your data could be stored centrally while your client is hosted in multiple geographical locations) and cost savings.\n\n## 3. Reduction of language barriers\n\nYour API should be a reflection of your business logic. Separating it out gives you the capability of expanding into different channels and in support of different devices while utilising the same backend.\n\nYour API acts as a universal language, which any of your clients can interact with. Even as you expand, every team will be speaking and understanding the same language. The expectations are always the same: same successes, same errors. Better yet, everybody knows JSON and almost everyone is up to speed with REST, so the API is globally understood.\n\n## 4. Developer liberation and specialization\n\nAPI-first development liberates developers. The only thing application developers need to know is the request/response sequences of each API endpoint and any potential error codes. The same goes for mobile developers, and any other type of developer for that matter.\n\nIndustries move forward when knowledge can be ‘black boxed’. Imagine if, to build a web application, you had to know how to build a microchip from scratch. Thanks to specialization and division of labor all you need to focus on is the code. This is the advantage of API-first development.\n\nThe approach frees up the front end development team to focus on a few specific ways to interact with the data, and the back end team can focus on providing it in a RESTful manner.\n\n## 5. Openness and future consumer availability\n\nAPI-first makes opening your API for public consumption simple. And as a client of our own API, as you add more functionality you will be in a position to offer it to consumers without any additional overhead.\n\n## 6. Modularity\n\nWhy limit yourself to just one source of data? With modern web practices you can easily combine multiple APIs to make a powerful product quickly. And if your needs change, so can the your platform, by simply adding or removing an API.","category":["6d898244-6df5-40ed-adaf-7378f1fd0a55"],"excerpt":"API-first development is the idea that whenever you are developing a piece of shared functionality for your organization it should be exposed as a RESTful HTTP(S) API to all of your other developers.","published":true,"publishedAt":null,"slug":"api-first-and-cope","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"API First & COPE","_createdBy":"api-client","_id":"efdb0878-f390-4621-a34d-48f785e03581","meta":{"revision":0,"created":1530494364854,"version":0},"$loki":1},{"_apiVersion":"1.0","_createdAt":1530494365868,"_lastModifiedAt":1526417972476,"_lastModifiedBy":"cloud-client","_refAuthor":{},"_refCategory":{},"_refSub-category":{},"_version":1,"author":["cdd7a16d-b068-40c7-8b22-6c1e39c35145"],"body":"## Overview\n\nThe term \"Microservice Architecture\" describes a particular way of designing software applications as suites of independently deployable services. While there is no precise definition of this architectural style, there are certain common characteristics, including organization around business capability, automated deployment, intelligence in the endpoints, and decentralized control of languages and data.\n\nWe see microservices as being a logical partner of the API-first development approach, in which complex applications are composed of small, independent processes that communicate with each other using language-agnostic APIs. These services are small, highly decoupled and focus on performing a small task.\n\n## Properties of the Microservices architecture:\n\n* The services are easy to replace\n* Services are organized around capabilities, e.g. user interface frontend, recommendation, logistics, billing, etc.\n\n## A microservices-based architecture:\n\n* Lends itself to a continuous delivery software development process\n* Is distinct from a Service-oriented architecture (SOA) in that the latter aims at integrating various (business) applications whereas several microservices belong to one application only\n\n## Products not projects\n\nMost application development efforts that we see use a project model where the aim is to deliver some piece of software which is then considered to be completed. On completion the software is handed over to a maintenance organization and the project team that built it is disbanded.\n\nMicroservice proponents tend to avoid this model, preferring instead the notion that a team should own a product over its full lifetime. A common inspiration for this is Amazon's notion of \"you build, you run it\" where a development team takes full responsibility for the software in production. This brings developers into day-to-day contact with how their software behaves in production, and increases contact with their users, as they have to take on at least some of the support burden.\n\nThe product mentality ties in with the linkage to business capabilities. Rather than looking at the software as a set of functionality to be completed, there is an ongoing relationship where the question is how can software assist its users to enhance the business capability.\n\n## How big is a microservice?\n\nAlthough \"microservice\" describes an architectural style, it's name does lead to an unfortunate focus on the literal size of a service, and arguments about what constitutes \"micro\".\n\nThe largest sizes we see follow Amazon's notion of the Two Pizza Team (i.e. the whole team can be fed by two pizzas), meaning no more than a dozen people. On the other end of the spectrum, a single developer could easily be responsible for multiple services.","category":["6d898244-6df5-40ed-adaf-7378f1fd0a55"],"excerpt":"Microservices describes a method of architecting complex applications as a series of small, independent processes that communicate with each other using language-agnostic APIs.","metaDescription":"","metaTitle":"","published":true,"publishedAt":null,"slug":"microservices","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Microservices","_createdBy":"api-client","_id":"d16f58d0-d345-4cc9-adbe-0e79020467cc","meta":{"revision":0,"created":1530494365869,"version":0},"$loki":2},{"_apiVersion":"1.0","_createdAt":1530494366887,"_lastModifiedAt":1526418011196,"_lastModifiedBy":"cloud-client","_refAuthor":{},"_refCategory":{},"_refSub-category":{},"_refWeb-service":{},"_version":1,"author":["cdd7a16d-b068-40c7-8b22-6c1e39c35145"],"body":"## Overview\n\nDADI is unique in its approach to data: the entire platform is built to facilitate Data Driven Experiences.\n\nAt the heart of this is a series of cognitive apps that provide predictive analysis to enable the creation of unique experiences targeted at the individual.\n\n## Cognitive insight with DADI Predict\n\nDADI Predict is the first of our machine learning apps. It is an API that simplifies audience-based predictions by using specific machine learning techniques.\n\nVery basically, events go into DADI Predict and predictions about future events come out.\n\n## A working example\n\nDADI Predict uses the concepts of events - familiar to anyone that has used GA - to allow for the simple collection of data points to model against.\n\nAn event is a grouping of four types:\n\n- Person\n- Action\n- Object\n- Weighting\n\nFor example: user A (the person) may add a Porsche 911 (the object) to their wishlist (the action).\n\nPredict uses our identity tools - specifically the guarantee of an individual and an issued UUID - to populate the identifier for the person in the event.\n\nThe actions and the objects are based on the predictions that we want to make within a product.\n\nGoing back to the example:\n\n1. User A adds a Porsche 911 to their wishlist\n2. User B adds a Porsche 911 and a Mercedes S-Class to their wishlist\n\nThis enables us to predict that User A is more likely to be interested in a Mercedes S-Class than in a randomly selected model.\n\nThe analytical approach being used is called collaborative filtering: the prediction of interests for a single user (filtering), calculated based on the interests of many users (collaborating).\n\nThe system works best at scale. Generally speaking, it needs at least 20x more data points than variables.\n\nAnd the more data you throw at it, the better it gets.","category":["6d898244-6df5-40ed-adaf-7378f1fd0a55"],"excerpt":"Machine Learning is not really a machine. Rather it's a mathematical model capable of learning patterns in large data sets and then predicting similar patterns in new data.","published":true,"publishedAt":null,"slug":"machine-learning","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Machine Learning","web-service":["b3618f0a-f72c-44b3-95a4-7f304384fa5d"],"_createdBy":"api-client","_id":"e90779f9-dc88-4f4c-8f44-912c7bca6bc5","meta":{"revision":0,"created":1530494366888,"version":0},"$loki":3},{"_apiVersion":"1.0","_createdAt":1530494367907,"_lastModifiedAt":1526418040342,"_lastModifiedBy":"cloud-client","_version":1,"body":"Stargates provide vital functions in the network in the form of domain name mapping and Consumer-DADI contract negotiation. They are rewarded with a percentage of all Consumer fees.\n\nStargates announce themselves by fully committing a very large volume of DADI tokens – **a buy-in system that ensures a high level of capacity and consistency across the network**.\n\nStargates are intended for very high bandwidth environments, such as data center owners/operators. Their initial deployment will be handled directly by DADI, who will install Stargates in key regions ahead of the network launch.\n\nApplication for status as a Stargate is manual, but like Gateways, Stargates are assessed under the reputation system, which grades and prefers nodes based on performance.","excerpt":"Provide domain routing and negotiate rates with Consumers.","published":false,"publishedAt":null,"slug":"stargates","title":"Stargates","_createdBy":"api-client","_id":"4a96c1c6-7f05-45b5-b255-754d2fd86d9b","meta":{"revision":0,"created":1530494367907,"version":0},"$loki":4},{"_apiVersion":"1.0","_createdAt":1530494368919,"_lastModifiedAt":1526418076459,"_lastModifiedBy":"cloud-client","_version":1,"body":"DADI Gateways - network node owners who contribute bandwidth - are incentivized for their participation in the network. Gateways handle network negotiation and provide edge caching for Host-processed content. Gateways are rewarded with a percentage of the fees from all of the Hosts connected to them, with rewards provided in the form of DADI tokens.\n\nThe availability of Gateways is managed through a proof of bandwidth system that periodically reports activity against specific service contracts and Host connectivity, updating a reputation system that is used to favour the most performant Gateways.\n\nGateways announce themselves to the network by time locking a large volume of DADI tokens (an order of magnitude larger than that required of Hosts). As with Hosts, this feeds the reputation system and helps to mitigate the risk of Sybil attacks.\n\n**If you have high bandwidth availability, DADI can become a significant profit center for your business.**","excerpt":"Act as network aggregation points for Host capacity and negotiate rates with Hosts.","published":false,"publishedAt":null,"slug":"gateways","title":"Gateways","_createdBy":"api-client","_id":"e4dd38c1-a648-4979-bee0-af7381300309","meta":{"revision":0,"created":1530494368920,"version":0},"$loki":5},{"_apiVersion":"1.0","_createdAt":1530494369930,"_lastModifiedAt":1526418083330,"_lastModifiedBy":"cloud-client","_version":1,"body":"DADI Hosts - network node owners who contribute computational power - are incentivized for their participation in the network. You can think of Hosts as miners, with rewards provided in the form of DADI tokens.\n\nThe availability of Host resources is managed through a proof of work system that periodically reports activity against specific service contracts, updating a reputation system that is used to favour the most performant Hosts.\n\nHosts announce themselves to the network by time locking a small volume of DADI tokens. This also feeds the reputation system and helps to mitigate the risk of [Sybil attacks](https://en.wikipedia.org/wiki/Sybil_attack), as valuable locks are not trivial to create.","excerpt":"Provide processing power for multiple Consumer app bundles","published":false,"publishedAt":null,"slug":"hosts","title":"Hosts","_createdBy":"api-client","_id":"fea8ae7f-3183-4621-8256-3a32b996b883","meta":{"revision":0,"created":1530494369931,"version":0},"$loki":6},{"_apiVersion":"1.0","_createdAt":1530494370942,"_lastModifiedAt":1526417323896,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["5a7cde69-353b-4718-88d4-29a4cd2608a5"],"body":"Web, in line with most of DADI products, is delivered as an [npm package](https://www.npmjs.com/package/@dadi/web) that you add as a dependency to your project. npm is the standard package manager in the Node.js community and it allows our users to efficiently install and update the software following the rules of the semantic versioning convention.\n\nThe downside to this is that when you install a product for the first time, you'll need a basic skeleton in place for the app to run. This typically consists of  an app-specific (JSON) configuration file, and a bootstrap script that will `require` the module and start the application.\n\nSome applications might also have additional requirements, such as versions 3.0 and above of DADI Web, where you need to install the template engines you wish to use and summon them from the bootstrap file.\n\nTo make this process quicker and easier for new users, we launched DADI CLI, a command-line tool to help with the installation and maintenance of DADI products.\n\nTo install Web using CLI, you first need to install the tool itself (you only need to do this once):\n\n```\nnpm install @dadi/cli -g\n```\n\nAnd then run the following command, which will create a new installation of Web in a directory called `my-new-site`:\n\n```\ndadi new my-new-site\n```\n\nThis will launch an interactive setup process, letting you choose which template engines to install. When it finishes, you just enter the newly created directory, start the app and you're up and running. It's that simple.\n\n<script type=\"text/javascript\" src=\"https://asciinema.org/a/137663.js\" id=\"asciicast-137663\" data-cols=\"150\" data-rows=\"40\" async></script>\n\nCLI is still in its early stages and we're planning many exciting features for the coming months.\n\nHit us on [@dadi](https://twitter.com/dadi) – or me directly at [@eduardoboucas](https://twitter.com/eduardoboucas) – if you have any questions or general feedback!","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"To make the install process quicker and easier for new users, we launched DADI CLI, a command-line tool to help with the installation and maintenance of DADI products.","published":true,"publishedAt":1505384220000,"slug":"installing-web-using-dadi-cli","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Installing Web using DADI CLI","_refWeb-service":{},"web-service":["f0d957ef-7cc5-4c71-bbb6-978c1cb43951"],"_createdBy":"api-client","_id":"d6ba9739-4bc6-4a76-a10a-60f7bb060b14","meta":{"revision":0,"created":1530494370943,"version":0},"$loki":7},{"_apiVersion":"1.0","_createdAt":1530494371965,"_lastModifiedAt":1526062882831,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["5a7cde69-353b-4718-88d4-29a4cd2608a5"],"body":"Responsive images on the web are an incredibly powerful tool. The most basic use case is the ability to load different versions of an image, with different sizes and resolutions, based on the user's viewport dimensions, which can avoid sending huge images across the wire to a device with a small screen.\n\nBut even more exciting than that is doing art direction on images, which is what I'll cover here.\n\nIf all you need is to display the same exact image with different sizes or resolutions, then the plain old `<img>` element with [the srcset attribute](https://css-tricks.com/responsive-images-youre-just-changing-resolutions-use-srcset/) is what you're after. The syntax allows you to define multiple source files for an image and helps the browser figure out which one to deliver.\n\nBut if you want to deliver different crops of an image based on certain conditions, such as viewport dimensions or orientation, then the [&lt;picture&gt; element](https://developer.mozilla.org/en/docs/Web/HTML/Element/picture) is the answer.\n\nDisplaying a landscape image on a portrait device is a common challenge developers are faced with, as simply displaying the original image in full width often produces suboptimal results — the image looks tiny and detail is lost. We want to have multiple crops of the image and display the one that best fits the user's device.\n\n&nbsp;\n\n![Showing different image crops based on screen aspect ratio](https://eduardoboucas.com/assets/posts/2016-07-18-image-art-direction-using-the-html-picture-element-and-dadi-cdn/responsive-images-demo.jpg)\n*Showing different image crops based on screen aspect ratio*\n\n&nbsp;\n\nThis can be achieved with the `<picture>` element, and the markup would look like this:\n\n```html\n<picture>\n  <source media=\"(orientation: portrait)\" \n          srcset=\"http://my-site.com/img-portrait.jpg\">\n  <img src=\"http://my-site.com/img-landscape.jpg\">\n</picture>\n```\n\nThis is great, but it comes with a challenge: we now have to generate multiple crops for each image that is to be displayed on the site. This creates a huge overhead and can quickly become a heavy burden to an editorial workflow.\n\n## Cropping on-the-fly\n\nIdeally, we would have a single image file in its original dimensions and then generate the various crops as needed, on-the-fly. This can be done with the *just in time* image manipulation system of [DADI CDN](https://github.com/dadi/cdn), an open-source asset manipulation and delivery platform.\n\nA crop can be obtained by requesting the image with specific URL parameters. The following URL would get a `400x600` crop, with the top left corner on the coordinates `600,150`:\n\n`http://cdn.url/img.jpg?width=400&height=600&resizeStyle=crop&crop=600,150,1000,750`\n\nWe can use this on the `<picture>` element to generate the various crops from a single file:\n\n```html\n<picture>\n  <source media=\"(orientation: portrait)\" \n          srcset=\"http://cdn.url/img.jpg?width=400&height=600&resizeStyle=crop&crop=600,150,1000,750\">\n  <img src=\"http://cdn.url/img.jpg\">\n</picture>\n```\n\nThis eliminates the need to manually generate different image files, so the only overhead we're adding to the workflow is having to find the coordinates for each image crop. But even that can be automated.\n\n## Automating crop generation\n\nThe latest version of DADI CDN introduced a [content aware cropping mode](https://github.com/dadi/cdn/pull/50), which analyses an image and provides crop coordinates for a given width and height based on the image contents.\n\nFor example, a `400x600` crop can be obtained with this syntax:\n\n`http://cdn.url/img.jpg?width=400&height=600&resizeStyle=entropy`\n\nFinally, by applying this to the `<picture>` element markup we can provide as many variations of an image as we want, without having to do any of it manually.\n\n```html\n<picture>\n  <source media=\"(orientation: portrait)\" \n          srcset=\"http://cdn.url/img.jpg?width=400&height=600&resizeStyle=entropy\">\n  <img src=\"http://cdn.url/img.jpg\">\n</picture>\n```\n\nCheck out [this pen](http://codepen.io/eduardoboucas/full/ZOApOK/) for a demo.","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"Deliver different crops of an image based on certain conditions, such as viewport dimensions automatically.","published":true,"publishedAt":1505643540000,"slug":"image-art-direction-using-the-html-picture-element-and-dadi-cdn","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Image art direction using the HTML picture element and DADI CDN","_refWeb-service":{},"web-service":["4693150d-f75c-4563-bb2d-934214b9e9bc"],"_createdBy":"api-client","_id":"07a5b408-73cd-4c35-858b-13ea17291a55","meta":{"revision":0,"created":1530494371966,"version":0},"$loki":8},{"_apiVersion":"1.0","_createdAt":1530494372988,"_lastModifiedAt":1526062825143,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["2647a07e-0b1f-45c0-8e90-0b55475d838b"],"body":"The early releases of DADI Web relied on a single templating engine, [DustJS](http://www.dustjs.com/). It's powerful, lightweight, extendable, and has just the right amount of logic. But it's not to everyone's taste. So in Web 3.0 we introduced template engine support, giving the engineer a choice of templating languages and making it quick and easy for them to roll their own.\n\nWe recently built an RSS feed using DADI Web. This is arelatively straight forward task, so rather than using DustJS we decided to make use of the [new ES6 engine](https://github.com/dadi/web-es6-templates).\n\n**Here's a snippet of the item iterator:**\n\n```js\n${articles.results\n  .map(i =>\n    `<item>\n      <guid isPermaLink=\"false\">${i._id}</guid>\n      <title>${clean(i.title)}</title>\n      <link>${i.url}</link>\n      <pubDate>${utcDate(i.publicationDate)}</pubDate>\n      <category>${clean(i.category.name)}</category>\n      <category domain=\"${i.category.domain}\">${clean(i.category.name)}</category>\n   </item>`\n})\n```\nJust like DustJS, we have _helpers_. Small JS files that are called during render to mutate values. In this example I created two.\n1. **clean** which cleans html special characters, similar to PHP's _htmlspecialchars_\n2. **utcDate** which converts an epoch timestamp into an RSS compatible utc date string\n\nThe total line count for the templates is just 37, with another 12 for the helpers.\n\nWant to try it yourself? [Read more about choosing an engine](https://docs.dadi.tech/#web/views).","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"The early releases of DADI Web relied on a single templating engine, but in Web 3.0 we introduced template engine support, giving the engineer a choice of templating languages.","published":true,"publishedAt":1507717380000,"slug":"using-the-es6-template-engine","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Using the ES6 template engine","_refWeb-service":{},"web-service":["f0d957ef-7cc5-4c71-bbb6-978c1cb43951"],"_createdBy":"api-client","_id":"60ac08ae-06c9-4f78-a0a0-0ff7fa407cef","meta":{"revision":0,"created":1530494372989,"version":0},"$loki":9},{"_apiVersion":"1.0","_createdAt":1530494374002,"_lastModifiedAt":1526417435378,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["5a7cde69-353b-4718-88d4-29a4cd2608a5"],"body":"In this video, I'll show you how to use delivery recipes.\n\nA delivery recipe is a predefined set of image manipulation parameters stored in a JSON file and applied to images at the time of request.\n\nYou can find more about recipes in our [documentation](https://docs.dadi.tech/#cdn/delivery-recipes).\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/4wYq8fmyYhA\" frameborder=\"0\" allowfullscreen></iframe>","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"A delivery recipe is a predefined set of image manipulation parameters stored in a JSON file and applied to images at the time of request.","publishedAt":1508333400000,"slug":"using-delivery-recipes","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Using delivery recipes","_refWeb-service":{},"web-service":["4693150d-f75c-4563-bb2d-934214b9e9bc"],"_createdBy":"api-client","_id":"96599a31-f445-4569-a32b-dd34f519c0a1","meta":{"revision":0,"created":1530494374003,"version":0},"$loki":10},{"_apiVersion":"1.0","_createdAt":1530494375020,"_lastModifiedAt":1526417487903,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["5a7cde69-353b-4718-88d4-29a4cd2608a5"],"body":"Here's a video on how to install and configure CDN using DADI CLI.\n\nFeel free to post a reply with any questions or issues!\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/qmMFZiILM6g\" frameborder=\"0\" allowfullscreen></iframe>","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"Here's a video on how to install and configure CDN using DADI CLI.","published":true,"publishedAt":1508334600000,"slug":"installing-and-configuring-dadi-cdn","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Installing and configuring DADI CDN","_refWeb-service":{},"web-service":["4693150d-f75c-4563-bb2d-934214b9e9bc"],"_createdBy":"api-client","_id":"c5de610e-d276-47cb-9307-cb18d6817232","meta":{"revision":0,"created":1530494375021,"version":0},"$loki":11},{"_apiVersion":"1.0","_createdAt":1530494376040,"_lastModifiedAt":1526417359484,"_lastModifiedBy":"cloud-client","_refCategory":{},"_version":1,"_refAuthor":{},"author":["6d08fa42-f098-4f94-b6a5-24d04ea09500"],"body":"Over the last couple of years we've spent a lot of time debugging runtime issues with Events and the data they have access to. \n\nAfter a short introduction to Web's Event system, we cover two built-in helpers that are part of the data context for each page request in DADI Web. \n\n## Events\n\nThe Event system in DADI Web provides developers with a way to perform tasks related to the current request, end the current request or extend the data context that is passed to the rendering engine.\n\nAn Event is a JavaScript file stored in your application's workspace folder and attached to a page.json file using the `\"events\"` array:\n\n```js\n\"events\": [\n  \"my-event\"\n]\n```\n\nIt is declared in the following way, receiving the original HTTP request, the response, the data context and a callback function to return control back to the controller that called it:\n\n```js\nconst Event = function (req, res, data, callback) {\n\n}\n```\n\n## The data context \n\nThe `data` argument that an Event receives is JSON which  is eventually passed to the template rendering engine once all the Datasources and Events have finished running. `data` may contain some or all of the following:\n\n* metadata about the current page\n* request parameters\n* data loaded by all datasources that have been run before the Events started executing\n* data added by previous Events\n* global configuration settings\n\n\n```json\n{\n  \"query\": {},\n  \"params\": {\n    \"category\": \"beauty\",\n    \"subCategory\": \"makeup\"\n  },\n  \"pathname\": \"/beauty-hair/makeup/article-one\",\n  \"host\": \"127.0.0.1:8000\",\n  \"debug\": true,\n  \"page\": {\n    \"name\": \"article\", \n    \"description\": \"Article page\",\n    \"language\": \"en\"\n  },\n  \"global\": {\n    \"baseUrl\": \"http://localhost:8000\",\n    \"environment\": \"development\"\n  }\n  \"article\": {\n    \"results\": [\n      {\n        \"slug\": \"article-one\",\n        \"title\": \"Article One\",\n        \"category\": \"beauty\"\n      } \n    ],\n    \"metadata\": {\n      \"limit\": 1,\n      \"page\": 1,\n      \"totalCount\": 19,\n      \"totalPages\": 19,\n      \"nextPage\": 2\n    }\n  }\n}\n```\n\n\n## Data from datasources \n\nAll datasources that retrieve data from a DADI API return data in the following format:\n\n```json\n{\n  \"results\": [\n    {\n      \"slug\": \"article-one\",\n      \"title\": \"Article One\",\n      \"category\": \"beauty\"\n    } \n  ],\n  \"metadata\": {\n    \"limit\": 1,\n    \"page\": 1,\n    \"totalCount\": 19,\n    \"totalPages\": 19,\n    \"nextPage\": 2\n  }\n}\n```\n\nThe results are added to the data context using the datasource's `key` property. Running a datasource configured with `\"key\": \"article\"` would result in the following in the data context:\n\n```json\n\"article\": {\n  \"results\": [\n    {\n      ...\n    } \n  ],\n  \"metadata\": {\n    ...\n  }\n}\n```\n\n## Common Event errors\n\nWhile helping people to implement applications using DADI Web, we've encountered many issues when trying to access data within an Event that simply doesn't exist in the data context.\n\n### No data loaded\n\nFor example, making an assumption that the datasource \"people\" has returned results and the data context has been populated:\n\n```js\nlet myPerson = data.people.results[0]\n```\n\nIf the context doesn't contain the `\"people\"` property, perhaps because the datasource wasn't attached to the page, then the Event fails with:\n\n```\nTypeError: Cannot read property 'results' of undefined\n```\n\n### No `\"results\"` property\n\nAnother way to cause an Event to fail is to reference an nonexistent `\"results\"` property. Perhaps the data we're expecting came from an API other than a DADI API (such as Twitter or Instagram) and the required data has a different structure.\n\n```js\nlet mostRecentInstagramPhoto = data.instagram.results[0]\n```\n\nIf there isn't a `\"results\"` array at `data.instagram`, the Event will fail with:\n\n```\nTypeError: Cannot read property '0' of undefined\n```\n\n## Introducing built-in helper functions\n\nWith these two common errors in mind, we decided it was time to introduce some helper functions into the mix: `has()` and `hasResults()`.\n\nThese helpers are added to the data context before it is passed to the Event system:\n\n```json\n{\n  has: [Function],\n  hasResults: [Function],\n  \"query\": {},\n  \"params\": {\n    \"category\": \"beauty\",\n    \"subCategory\": \"makeup\"\n  },\n  ...\n}\n```\n\n### `has()`\n\nThe `has()` function can be used to check that a property exists in the data context:\n\n```js\nconst Event = function (req, res, data, callback) {\n  if (data.has('instagram')) {\n    // do something with data.instagram\n  } else {\n    // no instagram property\n    res.statusCode = 404\n    res.end('Not found')\n  }\n  \n  callback()\n}\n```\n\n### `hasResults()`\n\nThe `hasResults()` function can be used to check that the results of a DADI API datasource exist, and that there is at least one value in the `\"results\"` array:\n\n```js\nconst Event = function (req, res, data, callback) {\n  if (data.hasResults('people')) {\n    // do something with people\n    let mostRecentPerson = data.people.results[0]\n  } else {\n    // no people results\n    res.statusCode = 404\n    res.end('Not found')\n  }\n  \n  callback()\n}\n```\n\n## Conclusion\n\nThe two helper functions added to the data context in DADI Web help to make your application more robust and keep the code cleaner. Don't worry, we got tired of writing the same statements over and over as well:\n\n```js\nif (data.people &&\n  data.people.results && \n  data.people.results[0]) {\n  // finally do something\n}\n```\n\nWe hope you find these as useful as we have, and welcome any suggestions for additional helpers that might save you time and energy!\n\nFor more information about Events in DADI Web, [see the documentation](https://docs.dadi.tech/web/#events-1).","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"While helping people to implement applications using DADI Web, we've encountered many issues when trying to access data within an Event that simply doesn't exist in the data context.","published":true,"publishedAt":1508408700000,"slug":"dadi-web's-built-in-data-helpers","title":"DADI Web's Built-in Data Helpers","_refWeb-service":{},"web-service":["f0d957ef-7cc5-4c71-bbb6-978c1cb43951"],"_createdBy":"api-client","_id":"8e0f2e70-4beb-4d2a-aeae-104714198b4d","meta":{"revision":0,"created":1530494376041,"version":0},"$loki":12},{"_apiVersion":"1.0","_createdAt":1530494377057,"_lastModifiedAt":1526417523318,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["5a7cde69-353b-4718-88d4-29a4cd2608a5"],"body":"Routes allow you to let CDN choose the appropriate recipe based on device, network, location or language.\n\nWith routes, CDN can make a decision about which delivery recipe to use for the current request, based on a set of configurable conditions.\n\nConditions can include the type of device being used, the network type, user location and language.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9iP1ycfaCZw\" frameborder=\"0\" allowfullscreen></iframe>","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"With routes, CDN can make a decision about which delivery recipe to use for the current request, based on a set of configurable conditions.","published":true,"publishedAt":1508421060000,"slug":"using-routes-to-deliver-targeted-content","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Using routes to deliver targeted content","_refWeb-service":{},"web-service":["4693150d-f75c-4563-bb2d-934214b9e9bc"],"_createdBy":"api-client","_id":"675eddd2-f8b9-4225-986b-1a571a97252c","meta":{"revision":0,"created":1530494377058,"version":0},"$loki":13},{"_apiVersion":"1.0","_createdAt":1530494378075,"_lastModifiedAt":1530187353493,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["5a7cde69-353b-4718-88d4-29a4cd2608a5"],"body":"Images are amongst the most common culprits of poor performance in websites, as they often require sending several megabytes of data across the wire for a single asset. When the network conditions are less than ideal, this can have a huge impact on load times and potentially make the website unusable.\n\nTo get around this, developers typically resort to *lazy loading*, a technique that comprises of sending a tiny temporary image with the initial payload (usually a blank pixel), which is then replaced when the real image, fetched asynchronously, has been downloaded and is ready to be used. In practice, this means that users don't have to wait for a large asset to be downloaded before they start seeing content on the screen, which is an improvement in perceived performance.\n\n## Using a placeholder\n\nAs an alternative to a blank pixel, we can use a placeholder generated from the original asset, as long as it's still small enough to have a marginal impact on the size of the initial payload. This creates a more pleasant visual effect, as users start seeing something that resembles the image they're expecting instead of just a blank slot.\n\nFor example, we can take the original image and reduce its dimensions and compression quality, drastically bringing down the file size. To avoid getting a terribly pixelated image, we can add a blur effect to mask the imperfections.\n\nHere's how it looks like:\n\n**Original image** (203 KB)\n![0_1508708466395_leaf-3.jpg](/assets/uploads/files/1508708469538-leaf-3-resized.jpg) \n\n**100px wide placeholder** (1 KB)\n![0_1508708371357_leaf.jpg](/assets/uploads/files/1508708374028-leaf.jpg)\n\n**Placeholder stretched to 600px**\n<img width=\"600\" src=\"/assets/uploads/files/1508708374028-leaf.jpg\">\n\n## Using CDN recipes\n\nWe could use CDN to generate a placeholder from any image using a set of URL parameters.\n\nhttps://cdn.somedomain.tech/samples/tree.jpg?width=120&quality=75&resizeStyle=aspectfit&blur=4\n\nA more convenient way is to use recipes (click [here](https://www.youtube.com/watch?v=4wYq8fmyYhA) for a video where I talk about them). Just copy the following recipe file to your `workspace` directory.\n\n```json\n{\n  \"recipe\": \"placeholder\",\n  \"settings\": {\n    \"format\": \"jpg\",\n    \"quality\": \"75\",\n    \"width\": \"120\",\n    \"resizeStyle\": \"aspectfit\",\n    \"blur\": 4\n  }\n}\n```\n\nAnd then you can generate a placeholder for any file by prepending its path with `/placeholder` , like https://cdn.somedomain.tech/placeholder/samples/leaf.jpg.\n\nDepending on how large your image slots are, you might want to adapt the recipe settings to better fit your needs. In particular, you can experiment with the `width`, `quality` and `blur` settings.\n\n## Demos\n\n- Lazy loading a large image using a placeholder: https://codepen.io/eduardoboucas/pen/RLOVGm?editors=1100\n\n<p data-height=\"550\" data-theme-id=\"0\" data-slug-hash=\"RLOVGm\" data-default-tab=\"result\" data-user=\"eduardoboucas\" data-embed-version=\"2\" data-pen-title=\"DADI / CDN: Lazy loading a large image with a placeholder\" data-preview=\"true\" class=\"codepen\">See the Pen <a href=\"https://codepen.io/eduardoboucas/pen/RLOVGm/\">DADI / CDN: Lazy loading a large image with a placeholder</a> by Eduardo Bouças (<a href=\"https://codepen.io/eduardoboucas\">@eduardoboucas</a>) on <a href=\"https://codepen.io\">CodePen</a>.</p>\n<script async src=\"https://production-assets.codepen.io/assets/embed/ei.js\"></script>\n\n- Lazy loading gallery images using placeholders: https://codepen.io/eduardoboucas/pen/OxGVqb\n\n<p data-height=\"344\" data-theme-id=\"0\" data-slug-hash=\"OxGVqb\" data-default-tab=\"result\" data-user=\"eduardoboucas\" data-embed-version=\"2\" data-pen-title=\"DADI / CDN: Lazy loading images with placeholders\" data-preview=\"true\" class=\"codepen\">See the Pen <a href=\"https://codepen.io/eduardoboucas/pen/OxGVqb/\">DADI / CDN: Lazy loading images with placeholders</a> by Eduardo Bouças (<a href=\"https://codepen.io/eduardoboucas\">@eduardoboucas</a>) on <a href=\"https://codepen.io\">CodePen</a>.</p>\n<script async src=\"https://production-assets.codepen.io/assets/embed/ei.js\"></script>","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"Images are amongst the most common culprits of poor performance in websites, as they often require sending several megabytes of data across the wire for a single asset.","published":true,"publishedAt":1508679180000,"slug":"lazy-loading-images-with-cdn-recipes","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Lazy loading images with CDN recipes","_refWeb-service":{},"web-service":["4693150d-f75c-4563-bb2d-934214b9e9bc"],"_createdBy":"api-client","_id":"312d1bb6-2f9c-4da7-92bc-c74e4a1787fe","meta":{"revision":0,"created":1530494378076,"version":0},"$loki":14},{"_apiVersion":"1.0","_createdAt":1530494379093,"_lastModifiedAt":1526417912074,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["2e0fd8d3-97e6-4398-b55a-3c8b41dd8214"],"body":"Okay, so I'm product director at DADI and perhaps a little biased. But years of (often bad) experience in procuring and configuring content management systems for digital products have helped define DADI technology – and here's why we think you'll like it:\n\n**It's Open Source**\n[Dive right in](https://github.com/dadi/) and use DADI technology straight away: it delivers enterprise muscle without the fat – or the greedy licence fees\n\n**It's faster to market**\nProducts built with DADI technology get to market in days and weeks, not weeks and months. Just ask [someone who's done it](https://dadi.tech/en/support/empire/)\n\n![0_1509569822713_Screen Shot 2017-11-01 at 20.08.30.png](/uploads/1509569823559screen-shot-2017-11-01-at-20.08.30.png) \n\n**It's API-first and it COPEs**\nDADI technology [connects your content to everything](https://dadi.tech/en/concepts/api-first-and-cope/): you can drive multiple products from a single editorial workflow\n\n**It brings data-driven experiences**\nDADI technology will enable [customized experiences](https://dadi.tech/en/concepts/data-driven-experiences/) for individuals – no more underwhelming one-size-fits-all products\n\n**It's big on microservices**\nDADI technology is built with [small, independent processes](https://dadi.tech/en/concepts/microservices/) enabling flexibility and performance you won't find in traditional systems\n\n**It's cheaper to run**\nUsing DADI technology can save a hefty chunk in infrastructure costs – [DADI CDN](https://dadi.tech/en/cdn/) alone can save 20-30% in related bandwidth on average\n\n**It makes editors happy**\n[DADI Publish](https://dadi.tech/en/publish/) moulds to your editorial workflow, meaning less time for admin and more time for writing – or editing images\n\n**It's world famous**\nWell, okay, so maybe not quite world famous. But it is used by over 200 websites and 400 editors on [major global publications](https://dadi.tech/en/support/) including Empire and Monocle magazine.\n\nConvinced? Find everything you need to get started with DADI on [Github](https://github.com/dadi/). Need a hand getting started? Go [here](https://dadi.tech/en/support/).","category":["6d898244-6df5-40ed-adaf-7378f1fd0a55"],"excerpt":"Okay, so I'm product director at DADI and perhaps a little biased…","published":true,"publishedAt":1509550560000,"slug":"reasons-to-love-dadi-technology","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Reasons to love DADI technology","_createdBy":"api-client","_id":"f3f79ad1-1b78-46f6-94f1-a8afeba64f82","meta":{"revision":0,"created":1530494379094,"version":0},"$loki":15},{"_apiVersion":"1.0","_createdAt":1530494380108,"_lastModifiedAt":1526417409579,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["0c46e2ee-d524-468d-9aff-c0e6439f191e"],"body":"[Web](https://dadi.tech/web) is the front-end component within the DADI web services stack, useful when you want serve any web document – HTML, CSS, JSON & XML etc.\n\nThere are three ways to get started with _Web_, each for slightly different situations.\n\n> N.B. Most DADI web services require [Node.js](https://nodejs.org/) and [NPM](npmjs.com) – and I’ll assume you have a basic knowledge of these.\n\n## 1. DADI CLI - Least hassle\n\n<div style=\"max-width:650px;margin:0 auto\">\n<img src=\"/uploads/1510918641547screen-shot-2017-11-17-at-11.37.01.png\"> \n</div>\n\n[DADI CLI](https://github.com/dadi/cli) (Command line interface) is by far the simplest way. Once you’ve installed it, CLI will help install, update and customise all DADI Web Services.\n\n[Eduardo Bouças](/team/eduardo-boucas) has covered this in a [previous article](/knowledge/tutorials/installing-web-using-dadi-cli), but we'll do a quick overview again here.\n\nAssuming you haven’t already installed the _CLI_. Run the following command in your terminal:\n\n```\nnpm install @dadi/cli -g\n```\n\nThis gives you a world of possibilities across our services. Run `dadi` in your terminal to see the full list.\n\nOk, now we can install _Web_. Firstly, create and/or `cd` into your project directory and run:\n\n```\ndadi web new\n```\n\n_CLI_ will ask you some questions to automate the installation, including which template engine you would like to run.\n\nWe recommend [Dust.js](http://www.dustjs.com/) if this is your first time getting into Javascript-based template engines. Those familiar with ES6 will find [this engine](https://www.npmjs.com/package/web-es6-templates) the simplest to get started.\n\n> You can learn more about template engines [in our documentation](https://docs.dadi.tech/#web/views).\n\n## 2. NPM - Slightly more hassle, more up-front customisation\n\nIf you’re already up and running using NPM to manage dependencies you just need to install the _Web_ package in your project:\n\n```\nnpm i @dadi/web\n```\n\n> N.B. As of NPM 5.0, `--save` is [on by default](http://blog.npmjs.org/post/161081169345/v500) so the package will be added to your package.json file automatically. \n\nSadly, you don’t quite have enough here to start the app just yet. For that you’ll need to grab the latest workspace from [our registry](https://github.com/dadi/registry/tree/master/web/boilerplate). Or create the following files:\n\n`server.js`\n```\nrequire('@dadi/web')({engines:[require('@dadi/web-dustjs')]})\n```\n\n> _Web_ comes with our [Dust.js](https://www.npmjs.com/package/@dadi/web-dustjs) engine by default.\n\n`/config/config.development.json`\n```\n{\n  \"cluster\": false,\n  \"allowJsonView\": true,\n  \"debug\": true\n}\n```\n\n> You can check out the [full configuration options](https://docs.dadi.tech/#web/advanced-configuration) in the docs.\n\n`/workspace/pages/index.json`\n```\n{\n  \"page\": {\n    \"key\": \"index\"\n  }\n}\n```\n\n`/workspace/pages/index.dust`\n```\nThis page is powered by DADI Web {version}\n```\n\nYou can now run the start command, and all being well you should have a site viewable at http://localhost:8080/\n\n```\nnpm start\n```\n\n## 3. Download - Hackers and contributors\n\nUsing NPM keeps the nuts and bolts hidden away in your `node_modules` folder, but for some smaller self-contained projects, or if you want to contribute to the development of _Web_ you’ll need to grab the source directly from our [Github repository](https://github.com/dadi/web).\n\nThe default branch across all DADI services is `develop` where we’re busy merging in new features for the next release, but if you need stability, check out the [releases page](https://github.com/dadi/web/releases). You can also see a detailed notes for each release here.\n\n> P.S. We’ve written a complete guide to [contributing to _Web_](https://github.com/dadi/web/blob/develop/CONTRIBUTING.md) and we’re happy to receive new issues or pull requests :)\n\nAssuming you’ve download a zip of the develop branch ([web-develop.zip](https://github.com/dadi/web/archive/develop.zip)) you’ll first want to install _Web_’s dependencies:\n\n```\ncd ./web-develop\nnpm i\n```\n\nGood luck! Any questions? [Find us on Discord](https://discord.gg/3sEvuYJ).","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"Web is the front-end component within the DADI web services stack, useful when you want serve any web document – HTML, CSS, JSON & XML etc.","published":true,"publishedAt":1509978480000,"slug":"dadi-web-first-install","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"DADI Web, first install","_refWeb-service":{},"web-service":["f0d957ef-7cc5-4c71-bbb6-978c1cb43951"],"_createdBy":"api-client","_id":"7816f7fa-e673-408a-8bf8-847344638666","meta":{"revision":0,"created":1530494380109,"version":0},"$loki":16},{"_apiVersion":"1.0","_createdAt":1530494381127,"_lastModifiedAt":1526477433607,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["5a7cde69-353b-4718-88d4-29a4cd2608a5"],"body":"Have you ever thought about what happens when you type an address in your web browser? Like, what actually happens behind the scenes so that a page with the content you’re looking for lands on your screen? Where is that content coming from?\n\nAs you might know, the Internet is just a massive group of interconnected computers, each of which is identified by a unique address (much like your home address). When you type the address to a website in your browser, you’re sending a message to the computer that lives on that address, asking it to send you the files with the content you’re after.<!--more--> Once the files are transferred to your computer, your browser can render them on the screen.\n\nI wanted to mention this because I want to bust any perception that there’s something magical about the place where websites are stored – there’s not. It’s just computers, not too dissimilar from the one you’re using to read this. In fact, you can easily host a website on your computer in a matter of minutes, which then anyone that knows the address will be able to access. Hold onto this thought, we’ll come back to it later.\n\nHaving looked at the basics, let’s think a bit bigger: how do large corporations host their websites and web applications? We’re talking about sites with massive volumes of traffic and of critical availability, so surely they can’t be running from a bedroom, on the same laptops you and me use to read the news.\n\n## Owned infrastructure: the past\n\nCompanies solved that problem in perhaps the most obvious way possible: they got bigger and faster computers (commonly known as servers) to deliver their websites to large numbers of users, as well as super-fast Internet connections to do it in a timely manner and with high availability. So if you wanted to run a large website, your company would invest in powerful and expensive hardware, like processors, hard drives and network cards, that would physically live within your premises.\n\nThis comes with its own set of challenges. For one, you need an in-house team of people to run the day-to-day maintenance routines: replacing a hard drive, installing new equipment, updating software, etc. This is expensive.\n\nSecondly, if you recall the process of how a web page is transferred from one computer to another, you’ll see that we have a geographical problem at hand. If the server hosting the content you want to access is located on the other side of the world, the transfer will take significantly longer than if it were just a few blocks away from your house. This means that the quality of service that someone will experience will be dependent on their geographical location, which is far from ideal. \n\nFinally, there’s a problem of scalability. Sure, if your site gets more traffic, you can just buy more hardware and increase connectivity – but is that sustainable in the long term? And how long does it take you to scale? Can you do it quickly enough to give your organisation the agility to adapt to traffic spikes? And what happens when traffic goes down again? You can’t easily sell the hardware to buy it back again the next time you need more capacity.\n\n## Cloud computing: the present\n\nCloud computing is a radically different paradigm that sets to tackle the challenges created by owned infrastructure. It abstracts the physical aspect of servers by keeping the hardware off-premises, in large datacenters owned and operated by third parties. Right away, this eliminates the need for costly in-house maintenance personnel.\n\nWith the rise of on-demand cloud computing services like Amazon Web Services, Microsoft Azure and Google Cloud, it became quick, simple and inexpensive to spin up servers in multiple locations around the world, ensuring that all users can communicate with a server that is reasonably close to them geographically, optimising load times and network latency.\n\nThe fact that these cloud computing services are effectively a virtually infinite pool of computing resources means that website owners can scale their infrastructure up and down as needed, paying just for what they use. If you get a traffic spike, you rent more computational power to ensure the demand is met, which you can then discard when no longer necessary. This paradigm is much more scalable than the owned infrastructure.\n\n## Fog computing: the future?\n\nIn less than a decade, cloud computing went from being a novelty concept to becoming the norm, as nowadays companies that still choose to own their infrastructure are certainly an exception. The flexibility, resilience and especially the reduced running costs are very attractive to many organisations, but it does raise some fundamental questions about the founding principles of the Internet itself.\n\nBy design, the Internet is naturally decentralised – a global mesh of interconnected computers where any individual can make content available to anyone, and equally consume content from anyone, in a democratic way. But with the dominance of cloud computing services, that balance was somewhat compromised, as we have a handful of large organisations holding the vast majority of the content in the network.\n\nRemember how we established in the opening paragraphs that you can easily host your website on your own computer? What if you also hosted your friend’s site, and the site for a company that is halfway across the world? And what if that company managed to have people in every capital city in the world doing the same, so that anyone trying to access the content could get it from an optimal location?\n\nThis is the core principle behind fog computing, whereby the consumers of the network can also be the providers, lending their spare bandwidth and computational resources to the delivery of someone else’s content. This reestablishes the balance of a decentralised system and democratically redistributes the power by the members of the network.\n\n![0_1510328186982_theFog.png](/uploads/1510328187684thefog.png) \n\nAs a result of the shorter distance between the resources and their consumers, data takes less time to be passed around, making room for real-time applications in the true sense of the term – as opposed to the questionable use of the terminology to describe applications which, depending on the conditions of the long network hops involved, may take a considerable amount to respond. \n\nThe reduced bandwidth requirements also mean a dramatic reduction of costs when compared to traditional cloud services. And because the members of the network are also providing the content, it’s viable to establish a model whereby people like you and me can get paid for their spare computational resources. In plain words, this means you could sit at home browsing a site, whilst at the same time getting paid to deliver some other site to the rest of the network. \n\n## Are we there yet?\n\nI’m working on a fog computing platform at [DADI](https://dadi.cloud), expected to launch in Q3 2018. It allows anyone to rent their computational resources, becoming what we call *miners*. It’s based on concepts like blockchain and cryptocurrencies, which I’ll cover in a future article. ","category":["6d898244-6df5-40ed-adaf-7378f1fd0a55"],"excerpt":"Have you ever thought about what happens when you type an address in your web browser? Like, what actually happens behind the scenes so that a page with the content you’re looking for lands on your screen? ","published":true,"publishedAt":1510314780000,"slug":"explain-like-i'm-five:-fog-computing","sub-category":["fab17704-fb0b-4a22-aa07-c011e4e6b1d4"],"title":"Explain like I'm five: Fog computing","_createdBy":"api-client","_id":"34794cfe-dd24-49da-be3a-a4e6952f7847","meta":{"revision":0,"created":1530494381128,"version":0},"$loki":17},{"_apiVersion":"1.0","_createdAt":1530494382143,"_lastModifiedAt":1526417577921,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["cdeeab35-b8e7-4aba-ae10-54bffead2ef1"],"body":"DADI API uses the client credentials flow of [OAuth 2.0](https://tools.ietf.org/html/rfc6749) to allow clients to access protected resources by obtaining an access token. First we'll setup an instance of API, then we'll go through creating some credentials, and using them to obtain a [Bearer token](https://tools.ietf.org/html/rfc6750), before we query a protected resource with our new token.\n\nAPI 2.X requires a MongoDB instance to work. If you don't have one of those setup already, you can easily start one with Docker to test with:\n\n```\n$ docker run -d -p 27017:27017 mongo:3.2\n```\n\nLet's create a new API app using the DADI CLI (`npm install @dadi/cli -g`):\n\n```\n$ dadi api new simple-api\n✔ Checking the available versions of DADI API\n✔ Downloading boilerplate (100%)\n✔ Installing DADI API (2.2.x)\n\nAll done! Run the following command to launch your new instance of DADI API:\n\ncd simple-api && npm start\n```\n\nCool, now let's create some credentials:\n\n```\n$ cd simple-api\n$ dadi api clients:add\n? What is the client ID? testUser\n? What is the secret? testPassword\n? What type of access does the user require? user\n✔ Created client with ID testUser and type user\n```\n\nSo now we have API setup, and a new user created, but where is this information stored? If you have a look in the `config/config.development.json` file, you'll find the `auth` block:\n\n```\n\"auth\": {\n  \"tokenUrl\": \"/token\",\n  \"tokenTtl\": 1800,\n  \"clientCollection\": \"clientStore\",\n  \"tokenCollection\": \"tokenStore\",\n  \"database\": {\n    \"hosts\": [\n      {\n        \"host\": \"127.0.0.1\",\n        \"port\": 27017\n      }\n    ],\n    \"username\": \"\",\n    \"password\": \"\",\n    \"database\": \"dadi-api\"\n  }\n}\n```\n\nThe `tokenUrl` defines where we query for a `Bearer token`. The `tokenTtl` is how long it will live for. The `clientCollection` is the Mongo collection which stores client credentials such as the one we created above, and the `tokenCollection` will store any `Bearer token`s that we generate.\n\nThe `database` block contains an array of `hosts` (allowing multiple in the case that you are using replica sets), a `username`, `password`, and finally, a `database`.\n\nLet's look in that database, assuming you have the `mongo` CLI tool installed:\n\n```\n$ mongo dadi-api\n> show collections\nclientStore\ntokenStore\n```\n\nNow query the `clientStore` collection to see if our user from earlier is in there:\n\n```\n> db.clientStore.find()\n{ \"_id\" : ObjectId(\"5a1403634e822a1250e45c56\"), \"clientId\" : \"testUser\", \"secret\" : \"testPassword\", \"type\" : \"user\" }\n```\n\nPerfect. Now let's start the API and generate some tokens.\n\nCancel out of the mongo prompt with `ctrl`+`c` if you haven't already, then start API.\n\n```\n$ npm start\n...\n----------------------------\nDADI API\nStarted 'DADI API'\n----------------------------\nServer:      127.0.0.1:3000\nVersion:     2.2.5\nNode.JS:     8.9\nEnvironment: development\n----------------------------\n```\n\nCool! Now let's try and access a resource without a token:\n\n```\n$ curl http://127.0.0.1:3000/1.0/library/books\n\n{\"statusCode\":401}\n```\n\nNow let's get a token. We'll need to send a `POST` request to `/token` with some JSON containing our `clientId` and our `secret`:\n\n```\ncurl \\\n  --request POST \\\n  --header \"Accept: application/json\" \\\n  --header \"Content-Type: application/json\" \\\n  --data \"{\\\"clientId\\\":\\\"testUser\\\",\\\"secret\\\":\\\"testPassword\\\"}\" \\\n  http://127.0.0.1:3000/token\n\n{\"accessToken\":\"36afb59e-e26a-4a3b-bb03-c6226efdc4f0\",\"tokenType\":\"Bearer\",\"expiresIn\":1800}\n```\n\nWe now have our `Bearer token`. We can use that to query the books collection again, but first, let's go back to mongo now and take a look in the `tokenStore` collection:\n\n```\n$ mongo dadi-api\n\n> db.tokenStore.find()\n{ \"_id\" : ObjectId(\"5a1405ac61977e126ced4fae\"), \"token\" : \"36afb59e-e26a-4a3b-bb03-c6226efdc4f0\", \"tokenExpire\" : 1511263412358, \"created\" : ISODate(\"2017-11-21T10:53:32.358Z\"), \"value\" : { \"_id\" : ObjectId(\"5a1403634e822a1250e45c56\"), \"clientId\" : \"testUser\", \"secret\" : \"testPassword\", \"type\" : \"user\" } }\n```\n\nAs you can see, our token is sat in the `tokenStore` collection now.\n\nNow let's query the books collection, this time, with our `Bearer token` in the `Authorization` header:\n\n```\ncurl \\\n  --header \"Accept: application/json\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"Authorization: Bearer 36afb59e-e26a-4a3b-bb03-c6226efdc4f0\" \\\n  http://127.0.0.1:3000/1.0/library/books\n  \n{\"results\":[],\"metadata\":{\"limit\":40,\"page\":1,\"fields\":{},\"sort\":{\"name\":1},\"offset\":0,\"totalCount\":0,\"totalPages\":0}}\n```\n\nBingo. We're in.\n\nYou don't have to use cURL though, you can use anything that implements HTTP, so long as it follows the pattern above. If you use DADI Web, you don't even need to worry about the authorization flow as it's all built in.\n\nSee our docs to more about [Authentication in DADI API](https://docs.dadi.tech/#api/authentication).","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"DADI API uses the client credentials flow of OAuth 2.0 to allow clients to access protected resources by obtaining an access token. ","published":true,"publishedAt":1511275980000,"slug":"authentication-in-dadi-api","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Authentication in DADI API","_refWeb-service":{},"web-service":["27d1f07c-a77c-4fea-af32-78680674c1f1"],"_createdBy":"api-client","_id":"18de8182-f463-48d4-a0dc-dead37c4ebe1","meta":{"revision":0,"created":1530494382144,"version":0},"$loki":18},{"_apiVersion":"1.0","_createdAt":1530494383156,"_lastModifiedAt":1526417705840,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["2e0fd8d3-97e6-4398-b55a-3c8b41dd8214"],"body":"A big challenge for product managers is finding the signal in the noise of stakeholder requirements to keep your product roadmap pointing in the right direction.\n\nStrong characters with gut feelings can derail projects, there’s nothing new in that. Google Analytics, Chartbeat, Domo, Looker and myriad usual analytics suspects can be deployed to stand in the way of these hunches, but what if product development strategy is baked into your platform?\n\nThis is a key point of focus for three new [DADI web services](https://dadi.tech/) planned for next year ([read our full roadmap here](https://dadi.tech/en/roadmap/)). **DADI Track** is a real-time, streaming data layer providing accurate metrics down to each individual using your product – it watches every move of anonymous and known users to paint a picture far more accurate than the averages and assumption gleaned from GA.\n\n![0_1511426528738_Screen Shot 2017-11-22 at 17.42.15.png](/uploads/1511426529684screen-shot-2017-11-22-at-17.42.15.png) \n\nPaired with **DADI Identity** – a CRM layer that guarantees the uniqueness of individuals – it will also help power highly targeted personalisation of both content and commercial opportunity (more on that in an upcoming post). But with **DADI Visualize**, our data visualization interfaces, Track can be used to floodlight your product roadmap with insight to support development decisions.\n\nYou’ll be able to tune it to measure a full range of metrics relevant to the product management process, from audience retention and conversion to user journeys by demographic or user segment. Plus, the same functionality DADI is developing to drive segmentation of audience for personalisation will also be able to deliver real-time A/B testing to further enrich your armoury.\n\nFinally – and possibly most satisfyingly – DADI Track will kill a whole bunch of third-party integrations you currently need to run on your product to achieve the level of insight you need to keep your stakeholders informed. And Visualize will mean just one interface from which to find it all. \n\nIf you’re interested in hearing more about our plans for analytics in DADI web services in 2018, [drop me a line](mailto:pr@dadi.co) or find me on [Discord](https://discordapp.com/invite/3sEvuYJ). More details on DADI technology [here](https://dadi.tech/).","category":["6d898244-6df5-40ed-adaf-7378f1fd0a55"],"excerpt":"A big challenge for product managers is finding the signal in the noise of stakeholder requirements to keep your product roadmap pointing in the right direction.","published":true,"publishedAt":1511449200000,"slug":"dadi-and-data-driven-product-development","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"DADI and data-driven product development","_createdBy":"api-client","_id":"4394305c-87e9-455e-bb54-9ca9d8d7e3ff","meta":{"revision":0,"created":1530494383157,"version":0},"$loki":19},{"_apiVersion":"1.0","_createdAt":1530494384174,"_lastModifiedAt":1526417724576,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["0c46e2ee-d524-468d-9aff-c0e6439f191e"],"body":"When building the original MVP of DADI Web we settled on using [Dust.js](http://www.dustjs.com/) as the template language. This was because it was the best ‘all rounder’ and in active-development with backing by a large corporation ([LinkedIn](http://www.dustjs.com/), surprisingly).\n\nSince then Node.js-powered front-end has matured considerably and we now see not only the number of template languages evolving, but also those wishing to bring their own template language to the DADI platform.\n\nTo this end, we abstracted the template engine in [Web 3.0](https://github.com/dadi/web/releases/tag/v3.0.0), opening the doors for people to hook in any engine they wish.\n\n> At time of writing we currently support [Dust.js](https://www.npmjs.com/package/@dadi/web-dustjs), [Mustache.js](https://www.npmjs.com/package/dadi-web-mustachejs), [Handlebars.js](https://www.npmjs.com/package/@dadi/web-handlebars), [Pug](https://www.npmjs.com/package/@dadi/web-pugjs) (formerly ‘Jade’) and [ES6 template literals](https://www.npmjs.com/package/web-es6-templates). You can see any more that get added by checking on the NPM keyword ‘[dadi-web-engine](https://www.npmjs.com/browse/keyword/dadi-web-engine)’.\n\n## Getting started\n\nThe best way to get stuck in is to check our existing codebases, listed above. We have also created a [sample engine](https://github.com/dadi/web-sample-engine) that you can use as a starting point.\n\n**If you have an existing template engine in mind, think of this as building a wrapper interface between Web and that engine.**\n\n## Suggested workflow\n\n1. Download & `npm install` Web from [the repo](https://forum.dadi.tech/topic/31/dadi-web-first-install).\n\n2. We include and define _Dust.js_ by default, as seen in [index.js](https://github.com/dadi/web/blob/develop/start.js). You can change this to the location of your `web-sample-engine` (or use [npm link](https://docs.npmjs.com/cli/link)) e.g.,\n\n\t```javascript\n\trequire('./main')({\n\t  engines: [require(‘../web-sample-engine’)]\n\t})\n\t```\n\t\n3. Start by defining the default config-block for your engine, which is overridden by your main [config file](https://docs.dadi.tech/#web/configuration). \n\n\t**N.B. this needs to follow [Mozilla’s Convict](https://github.com/mozilla/node-convict) format.**\n\n\tFor example, see the very simple [ES6 template literals](https://github.com/abovedave/web-es6-templates/blob/master/index.js#L3-L15) config:\n\t\n\t```javascript\n\tconst ENGINE = {\n\t  config: {\n\t   paths: {\n\t      doc: 'Paths required by ES6 templates',\n\t      format: Object,\n\t      default: {\n\t        helpers: 'workspace/utils/helpers'\n\t      }\n\t    }\n\t  },\n\t  extensions: ['.js'],\n\t  handle: 'es6'\n\t}\n\t```\n\t\n\tYour engine might not require any configuration. Although at the minimum you want to set the files `extensions` for the template files in your app.\n\n5. Start to hook in the methods `.initialise()`, `.register()` and `.render()` into the engine you want to use.\n\n6. You can either use your template engine locally, or if you’re proud of it enough to publish it to NPM, let us know so we can include it in the list of supported engines (don’t forget to write some tests!).\n\n## Conclusion\n\nWe are always looking at other parts of the Web experience we can abstract into a plugin-style setup.\n\nWe have [postProcessors](https://github.com/dadi/web/pull/253) coming up in 5.0 – let us know if there is anything you would like to see?","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"We abstracted the template engine in Web 3.0, opening the doors for people to hook in any engine they wish.\n","published":true,"publishedAt":1511535720000,"slug":"build-a-template-engine-for-dadi-web","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Build a template engine for DADI Web","_refWeb-service":{},"web-service":["f0d957ef-7cc5-4c71-bbb6-978c1cb43951"],"_createdBy":"api-client","_id":"ec14f623-2b5a-4629-b3d1-fe4fe05fdd00","meta":{"revision":0,"created":1530494384176,"version":0},"$loki":20},{"_apiVersion":"1.0","_createdAt":1530494385195,"_lastModifiedAt":1528292127165,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["5a7cde69-353b-4718-88d4-29a4cd2608a5"],"body":"The main method for interacting with data in [DADI API](https://github.com/dadi/api) is a set of RESTful endpoints, created automatically for each collection, that allow you to query, create, update or delete documents based on the HTTP verb sent in the request.\n\n```\n# Get documents\nGET /1.0/my-database/my-collection\n\n# Create documents\nPOST /1.0/my-database/my-collection\n\n# Update documents\nPUT /1.0/my-database/my-collection\n\n# Delete documents\nDELETE /1.0/my-database/my-collection\n```\n\nThis a hassle-free, zero-setup-involved way of interacting with your data in the simplest way possible: when you ask for a document, API will give you the document exactly as it is stored in the database; when you create a document, API will store it exactly as you supply it.\n\nBut sometimes you need a bit more flexibility, like adding to a created document a compound field that is automatically generated from the content of other fields. Or massage the way data is presented in a response without actually changing the way it's stored internally. For this, we have [hooks](https://docs.dadi.tech/#api/hooks).\n\n## Introducing hooks\n\nHooks are chainable blocks of arbitrary logic that run at specific points in the lifecycle of a request, having the power to modify the request and response, or to abort an operation completely. There are 8 types of hooks:\n\n1. `beforeGet`: Executed on `GET` requests, before the query is processed. Has the ability to change the parameters supplied in the request, so effectively modify the query before it's processed. Can abort the operation by throwing an error or returning a rejected Promise.\n\n1. `afterGet`: Executed on `GET` requests, after the query has been processed and before the response is sent to the user. Has the ability to modify the response contents and, unlike other `before*` hooks, also has the power to abort the operation.\n\n1. `beforeCreate`: Executed on `POST` requests, before the query is processed. Has the ability to change the documents about to be inserted. Can abort the operation by throwing an error or returning a rejected Promise.\n\n1. `afterCreate`: Executed on `POST` requests, after the query is processed. Does not have the ability to do any further changes to the documents inserted or to change the contents of the response. It's designed to perform asynchronous, fire-and-forget operations as a result of the document creation (e.g. notify an external service).\n\n1. `beforeUpdate`: Executed on `PUT` requests, before the query is processed. Has the ability to change the query that defines which documents are about to be updated, as well as the contents of the update itself. Can abort the operation by throwing an error or returning a rejected Promise.\n\n1. `afterUpdate`: Executed on `PUT` requests, after the query is processed. Similar to `afterCreate`, it does not have the ability to do any further changes to the updated documents or to change the contents of the response. It's designed to perform asynchronous, fire-and-forget operations as a result of the document update.\n\n1. `beforeDelete`: Executed on `DELETE` requests, before the query is processed. Has the ability to change the query that defines which documents are about to be deleted. Can abort the operation by throwing an error or returning a rejected Promise.\n\n1. `afterDelete`: Executed on `DELETE` requests, after the query is processed. Similar to `afterCreate` and `afterUpdate`, it does not have the ability to do any further changes to the deleted documents (like undo the operation) or change the contents of the response. It's designed to perform asynchronous, fire-and-forget operations as a result of the document deletion.\n\n## Example\n\nImagine an `articles` collection with the following fields:\n\n- `title`: The article title (e.g. `Decentralized Web Services with DADI`)\n- `slug`: A URL-friendly version of the title (e.g. `decentralized-web-services-with-dadi`)\n- `body`: The article body\n\nTo create an article, you could simply put the fields above in the body of a `POST` request and sent it to API, like so:\n\n```\nPOST https://api.somedomain.tech/1.0/your-database/articles\n\n{\n    \"title\": \"DADI: Decentralized Architecture for a Democratic Internet\",\n    \"slug\": \"dadi-decentralized-architecture-for-a-democratic-internet\",\n    \"body\": \"A new era of cloud computing services, powered by blockchain technology.\"\n}\n```\n\nWhilst this works, it puts on consumers the responsibility of generating the slug from the title, which leaves room for inconsistencies due to different implementations. This feels like a task that should be performed automatically by API at the point of insertion. This is where hooks come in.\n\nWe start by creating the hook file. We'll place it at `workspace/hooks/slugify.js`.\n\n```js\n// Creates a URL-friendly version (slug) of any given string\nfunction slugify(text) {\n  return text.toString().toLowerCase()\n    .replace(/\\s+/g, '-')\n    .replace(/[^\\w\\-]+/g, '')\n    .replace(/\\-\\-+/g, '-')\n    .replace(/^-+/, '')\n    .replace(/-+$/, '')\n}\n\nmodule.exports = function (obj, type, data) {\n  // `obj` is the document being inserted;\n  // `type` is a numeric code for the type of operation being carried out:\n  //    - 0: create\n  //    - 1: update\n  //    - 2: delete\n  // `data` is an object containing additional data, such as the collection\n  // schema and other properties that vary with the type of hook being used.\n  \n  // In this case, we're simply creating a `slug` property in the document\n  // with the result of slugifying the content of the `title` property.\n  // For now, we're hardcoding the names of the `slug` and `title` fields\n  // in the hook, more on this shortly.\n  obj.slug = slugify(obj.title)\n  return obj\n}\n```\n\nThe last thing we need to do is to activate this hook in the collection schema, specifying where in the lifecycle of the request it will get executed.\n\n```js\n\"settings\": {\n  \"hooks\": {\n    \"beforeCreate\": [\"slugify\"],\n    \"beforeUpdate\": [\"slugify\"]\n  }\n}\n```\n\nThe above means that the hook will get executed just before a document is created or updated, ensuring that the `slug` field is always populated with the URL-friendly version of the `title` field.\n\nAlternatively to defining an array of strings where each element is the name of a hook, we can supply an array of objects, allowing us to provide extra options.\n\n```js\n\"settings\": {\n  \"hooks\": {\n    \"beforeCreate\": [\n      {\n        \"hook\": \"slugify\",\n        \"options\": {\n          \"from\": \"title\",\n          \"to\": \"slug\"\n        }\n      }\n    ]\n  }\n}\n```\n\nIn the example above, we're specifying an `options` object, which gets passed to the hook file. This allows us to make the hook a bit more generic, as we can remove any hardcoded field names from its logic and instead supply them via the options object. To reflect this, we must also change the hook file:\n\n```js\n// Creates a URL-friendly version (slug) of any given string\nfunction slugify(text) {\n  return text.toString().toLowerCase()\n    .replace(/\\s+/g, '-')\n    .replace(/[^\\w\\-]+/g, '')\n    .replace(/\\-\\-+/g, '-')\n    .replace(/^-+/, '')\n    .replace(/-+$/, '')\n}\n\nmodule.exports = function (obj, type, data) {\n  // We get the names of the source (`from`) and target (`to`) fields\n  // from the options object we defined in the collection schema, available\n  // at `data.options`.\n  obj[data.options.to] = slugify(obj[data.options.to])\n  return obj\n}\n```\n\nWith the hook in place, we can achieve the same result as before without having to specify the contents of the `slug` field in the payload:\n\n```\nPOST https://api.somedomain.tech/1.0/your-database/articles\n\n{\n    \"title\": \"DADI: Decentralized Architecture for a Democratic Internet\",\n    \"body\": \"A new era of cloud computing services, powered by blockchain technology.\"\n}\n```\n\n## Taking it further\n\nThe example above is just the simplest of possible use cases for hooks, but the possibilities are endless. These are just a few things I can think off the top of my head where you could use them:\n\n- A `beforeCreate` hook that takes a URL from a field, fires an HTTP request to an external API and injects the response in another field — for example, get a URL for a tweet, grab its content and add it to the document\n\n- A bespoke authentication/authorisation layer that uses a set of `beforeGet`, `beforeCreate`, `beforeUpdate` and `beforeDelete` hooks to grant or block access to resources based on the information sent in the request\n\n- An `afterCreate` hook that indexes documents on a search system after they're created\n\nWith great flexibility comes great... awesomeness? I'm looking forward to know what you'll build with API hooks – [hit me up](https://twitter.com/eduardoboucas)!","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"All the Hooks are chainable blocks of arbitrary logic that run at specific points in the lifecycle of a request, having the power to modify the request and response, or to abort an operation completely.","published":true,"publishedAt":1511778960000,"slug":"using-api-hooks","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Using API hooks","_refWeb-service":{},"web-service":["27d1f07c-a77c-4fea-af32-78680674c1f1"],"_createdBy":"api-client","_id":"76a94159-3099-4a72-8df2-e354ebe18dbf","meta":{"revision":0,"created":1530494385196,"version":0},"$loki":21},{"_apiVersion":"1.0","_createdAt":1530494386214,"_lastModifiedAt":1526417740673,"_lastModifiedBy":"cloud-client","_refCategory":{},"_version":1,"_refAuthor":{},"author":["cdeeab35-b8e7-4aba-ae10-54bffead2ef1"],"body":"In this article we're going to look at how you can construct a photo gallery using DADI Web Services, specifically [Web](https://dadi.tech/en/web/), [API](https://dadi.tech/en/api/), and [CDN](https://dadi.tech/en/cdn/). First we'll create each of the DADI services using the command line interface tool. After that we'll grab some sample images, store them in the CDN, add them to our database via API, and then create a frontend in Web.\n\nFor this project I'm going to be working in a single directory called \"gallery\" with a subdirectory for each of the services, which the DADI CLI tool will create for us.\n\nIf you haven't installed the CLI, let's do that first:\n\n```bash\n$ npm install @dadi/cli -g\n```\n\nNow we'll create the \"gallery\" directory which will contain each of the services:\n\n```bash\n$ mkdir ~/projects/gallery\n$ cd ~/projects/gallery\n```\n\n### CDN\n\nLet's start with the CDN.\n\nFirst, we'll create a new CDN service using the DADI CLI command `dadi cdn new <name>`:\n\n```bash\n$ dadi cdn new cdn\n```\n\nThis will download and install DADI CDN, then take you through an interactive wizard. We're just going to run a simple HTTP, file-based CDN service:\n\n```json\nLet's start by configuring the web server that DADI CDN will run on. (0% complete)\n\n? What is the name of this CDN instance? DADI (CDN)\n? What protocol should this CDN instance run on? HTTP (insecure)\n? What is the IP address the application should run on? 0.0.0.0\n? What is the port number the application should run on? 8002\n\nTime to configure the sources that CDN will use to retrieve images. (18% complete)\n\n? Would you like to load images from the local filesystem? Yes\n? What is the path to the images directory? ./images\n? Would you like to load images from Amazon S3? No\n? Would you like to load images from a remote URL? No\n\nGreat! Let's now define how CDN handles other assets (e.g. CSS, JS or fonts) (44% complete)\n\n? Would you like to load assets from the local filesystem? No\n? Would you like to load assets from Amazon S3? No\n? Would you like to load assets from a remote URL? No\n\nLet's now look at caching, which is crucial to ensure that CDN delivers images and assets in a performant way. (67% complete)\n\n? What is the time-to-live (TTL), in seconds, of cached items? 3600\n? Would you like to cache items on the local filesystem? Yes\n? What is the path to the cache directory? ./cache/\n? Would you like to cache items on a Redis server? No\n\nSuper. You also need to configure the credentials for authenticated consumers to use via oAuth. (85% complete)\n\n? What ID should authenticated clients use? photogalleryUser\n? What secret should authenticated clients use? s3cr3tsqu1rr3l\n? What is the time-to-live (TTL), in seconds, for oAuth tokens? 1800\n\nAlmost there! A couple more questions about your CDN installation. (92% complete)\n\n? Would you like DADI CDN to run in cluster mode, starting a worker for each CPU core? No\n? Which environment does this config apply to? Development\n\nAll done!\n```\n\nGreat, now let's prepare our images. Create a directory within the `cdn` directory and call it `images`. Next we'll grab some sample images for our gallery. One great place to get these is Fujifilm which provides sample images from camera models. Download the following images into the `images` directory: [sample-1.jpg](http://www.fujifilm.com/products/digital_cameras/x/fujifilm_x100f/sample_images/img/index/ff_x100f_001.JPG), [sample-2.jpg](http://www.fujifilm.com/products/digital_cameras/x/fujifilm_x100f/sample_images/img/index/ff_x100f_002.JPG), [sample-3.jpg](http://www.fujifilm.com/products/digital_cameras/x/fujifilm_x100f/sample_images/img/index/ff_x100f_003.JPG), [sample-4.jpg](http://www.fujifilm.com/products/digital_cameras/x/fujifilm_x100f/sample_images/img/index/ff_x100f_004.JPG), [sample-5.jpg](http://www.fujifilm.com/products/digital_cameras/x/fujifilm_x100f/sample_images/img/index/ff_x100f_006.JPG).\n\nFrom the `cdn` directory, run `npm start`, then check the images are working by navigating to `http://localhost:8002/sample-1.jpg`. \n\nWe're now ready to move onto API.\n\n### API\n\nLeave the CDN directory, going back up a level to the \"gallery\" directory. We're going to create a new API service using the DADI CLI command `dadi api new <name>`:\n\n```bash\n$ cd ..\n$ dadi api new api\n```\n\nAt the time of writing, this will install DADI API 2.x so there _will be no interactive wizard_. Soon though, with DADI API 3.0, you'll be walked through a setup wizard and will be able to [select from a number of data connectors](https://forum.dadi.tech/topic/21/dadi-roadmap-update-01st-november-2017).\n\n> Note: API 2.x requires a MongoDB instance to work. If you don't have one of those setup already, you can easily start one with Docker to test with:\n\n```\n$ docker run -d -p 27017:27017 mongo:3.2\n```\n\nThe first thing we'll do is edit the configuration file, `config/config.development.json`, and modify the server settings.\n\nChange the port under `server` and `publicUrl` to `8001`. Let's also change the `database` value from `dadi-api` to `photo-gallery` under the `database` and `auth` sections. Here is a quick summary of changes:\n\n```json\n  \"server\": {\n    \"port\": 8001\n  },\n  \"publicUrl\": {\n    \"port\": 8001\n  },\n  \"database\": {\n    \"database\": \"photo-gallery\",\n  },\n  \"auth\": {\n    \"database\": {\n      \"database\": \"photo-gallery\"\n    }\n  },\n```\n\nAs we saw in my recent post on [Authentication in DADI API](https://forum.dadi.tech/topic/33/authentication-in-dadi-api), we need to create some credentials for our Web service to authenticate with our API service. We can use the DADI CLI to do that, it's really easy. Change into the new `api` directory and run the following:\n\n```bash\n$ dadi api clients:add\n? What is the client ID? photogallery\n? What is the secret? s3cr3tsqu1rr3l\n? What type of access does the user require? user\n✔ Created client with ID photogallery and type user\n```\n\nNext we're going to create some collections in API.\n\nIn the `api/workspace/collections/1.0/` directory, delete the sample `library` directory and create a new directory called `gallery`, and inside that, create a file called `collection.albums.json` with the following contents:\n\n```json\n{\n  \"fields\": {\n    \"title\": {\n      \"type\": \"String\",\n      \"label\": \"Album title\",\n      \"required\": true\n    },\n    \"publicationDate\": {\n      \"type\": \"DateTime\",\n      \"label\": \"Publication date\",\n      \"required\": true\n    }\n  },\n  \"settings\": {\n    \"cache\": true,\n    \"authenticate\": true,\n    \"count\": 10,\n    \"sort\": \"publicationDate\",\n    \"sortOrder\": 1,\n    \"storeRevisions\": false,\n    \"index\": [\n      {\n        \"keys\": {\n          \"publicationDate\": 1\n        }\n      }\n    ]\n  }\n}\n```\n\nNow create a `collection.photos.json` file with the following content:\n\n```json\n{\n  \"fields\": {\n    \"album\": {\n      \"type\": \"String\",\n      \"label\": \"Photo album ID\",\n      \"required\": true\n    },\n    \"title\": {\n      \"type\": \"String\",\n      \"label\": \"Photo title\",\n      \"required\": true\n    },\n    \"filename\": {\n      \"type\": \"String\",\n      \"label\": \"Resource filename\",\n      \"required\": true\n    },\n    \"publicationDate\": {\n      \"type\": \"DateTime\",\n      \"label\": \"Publication date\",\n      \"required\": true\n    }\n  },\n  \"settings\": {\n    \"cache\": true,\n    \"authenticate\": true,\n    \"count\": 10,\n    \"sort\": \"publicationDate\",\n    \"sortOrder\": 1,\n    \"storeRevisions\": false,\n    \"index\": [\n      {\n        \"keys\": {\n          \"publicationDate\": 1\n        }\n      }\n    ]  \n  }\n}\n```\n\nGreat. We now have two collections, `albums` with the `title` and `publicationDate` fields, and `photos` with the `album`, `title`, `description`, `filename`, and `publicationDate` fields. Now it's time to create some entries. From the `api` directory, run `npm start`.\n\n#### Populating the database\n\nFor brevity, we're just going to use cURL to perform API calls. Using cURL, let's first grab ourselves an authentication token:\n\n```bash\n$ curl \\\n  --request POST \\\n  --header \"Accept: application/json\" \\\n  --header \"Content-Type: application/json\" \\\n  --data \"{\\\"clientId\\\":\\\"photogallery\\\",\\\"secret\\\":\\\"s3cr3tsqu1rr3l\\\"}\" \\\n  http://127.0.0.1:8001/token\n  \n{\"accessToken\":\"aa9c8513-1d10-4e62-9457-08891dda6136\",\"tokenType\":\"Bearer\",\"expiresIn\":1800}\n```\n\nUsing the new access token, let's create some albums. We'll create three: `Autumn leaves`, `Music`, and `Travel`.\n\n> Ensure you change the access token used in the Authorization header in the following cURL command!\n\n```bash\n$ curl \\\n  --request POST \\\n  --header \"Accept: application/json\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"Authorization: Bearer aa9c8513-1d10-4e62-9457-08891dda6136\" \\\n  --data \"[{\\\"title\\\":\\\"Travel\\\",\\\"publicationDate\\\":\\\"2017-05-18T10:00:00Z\\\"},\n           {\\\"title\\\":\\\"Music\\\",\\\"publicationDate\\\":\\\"2017-06-20T12:00:00Z\\\"},\n           {\\\"title\\\":\\\"Autumn leaves\\\",\\\"publicationDate\\\":\\\"2017-10-01T14:00:00Z\\\"}]\" \\\n  http://127.0.0.1:8001/1.0/gallery/albums\n  \n{\"results\":[{\"title\":\"Travel\",\"publicationDate\":\"2017-05-18T10:00:00.000Z\",\"apiVersion\":\"1.0\",\"createdAt\":1512056399962,\"createdBy\":\"photogallery\",\"v\":1,\"_id\":\"5a20264f94c0a42e53d0823f\"},{\"title\":\"Music\",\"publicationDate\":\"2017-06-20T12:00:00.000Z\",\"apiVersion\":\"1.0\",\"createdAt\":1512056399962,\"createdBy\":\"photogallery\",\"v\":1,\"_id\":\"5a20264f94c0a42e53d08240\"},{\"title\":\"Autumn leaves\",\"publicationDate\":\"2017-10-01T14:00:00.000Z\",\"apiVersion\":\"1.0\",\"createdAt\":1512056399962,\"createdBy\":\"photogallery\",\"v\":1,\"_id\":\"5a20264f94c0a42e53d08241\"}]}\n```\n\nLet's extract the album IDs from the results. We'll need them for adding photos to each album: `5a20264f94c0a42e53d0823f` for `Travel`, `5a20264f94c0a42e53d08240` for `Music`, and `5a20264f94c0a42e53d08241` for `Autumn leaves`.\n\nNow let's create some photos. We'll add `sample-1.jpg` and `sample-2.jpg` to the first album, `sample-3.jpg` and `sample-4.jpg` to the second album, and `sample-5.jpg` to the final album.\n\n> Don't forget to change the access token used in the Authorization header in the following cURL command, AND the album ID values!\n\n```bash\n$ curl \\\n  --request POST \\\n  --header \"Accept: application/json\" \\\n  --header \"Content-Type: application/json\" \\\n  --header \"Authorization: Bearer aa9c8513-1d10-4e62-9457-08891dda6136\" \\\n  --data \"[{\\\"album\\\":\\\"5a20264f94c0a42e53d0823f\\\",\\\"title\\\":\\\"Travel 1\\\",\\\"filename\\\":\\\"sample-5.jpg\\\",\\\"publicationDate\\\":\\\"2017-05-18T10:00:00Z\\\"},\n           {\\\"album\\\":\\\"5a20264f94c0a42e53d08240\\\",\\\"title\\\":\\\"Music 1\\\",\\\"filename\\\":\\\"sample-3.jpg\\\",\\\"publicationDate\\\":\\\"2017-06-20T12:00:00Z\\\"},\n           {\\\"album\\\":\\\"5a20264f94c0a42e53d08240\\\",\\\"title\\\":\\\"Music 2\\\",\\\"filename\\\":\\\"sample-4.jpg\\\",\\\"publicationDate\\\":\\\"2017-06-20T12:00:00Z\\\"},\n           {\\\"album\\\":\\\"5a20264f94c0a42e53d08241\\\",\\\"title\\\":\\\"Leaves 1\\\",\\\"filename\\\":\\\"sample-1.jpg\\\",\\\"publicationDate\\\":\\\"2017-10-01T14:00:00Z\\\"},\n           {\\\"album\\\":\\\"5a20264f94c0a42e53d08241\\\",\\\"title\\\":\\\"Leaves 2\\\",\\\"filename\\\":\\\"sample-2.jpg\\\",\\\"publicationDate\\\":\\\"2017-10-01T14:00:00Z\\\"}]\" \\\n  http://127.0.0.1:8001/1.0/gallery/photos\n\n{\"results\":[{\"album\":\"5a20264f94c0a42e53d0823f\",\"title\":\"Travel 1\",\"filename\":\"sample-5.jpg\",\"publicationDate\":\"2017-05-18T10:00:00.000Z\",\"apiVersion\":\"1.0\",\"createdAt\":1512056536558,\"createdBy\":\"photogallery\",\"v\":1,\"_id\":\"5a2026d894c0a42e53d08242\"},{\"album\":\"5a20264f94c0a42e53d08240\",\"title\":\"Music 1\",\"filename\":\"sample-3.jpg\",\"publicationDate\":\"2017-06-20T12:00:00.000Z\",\"apiVersion\":\"1.0\",\"createdAt\":1512056536558,\"createdBy\":\"photogallery\",\"v\":1,\"_id\":\"5a2026d894c0a42e53d08243\"},{\"album\":\"5a20264f94c0a42e53d08240\",\"title\":\"Music 2\",\"filename\":\"sample-4.jpg\",\"publicationDate\":\"2017-06-20T12:00:00.000Z\",\"apiVersion\":\"1.0\",\"createdAt\":1512056536558,\"createdBy\":\"photogallery\",\"v\":1,\"_id\":\"5a2026d894c0a42e53d08244\"},{\"album\":\"5a20264f94c0a42e53d08241\",\"title\":\"Leaves 1\",\"filename\":\"sample-1.jpg\",\"publicationDate\":\"2017-10-01T14:00:00.000Z\",\"apiVersion\":\"1.0\",\"createdAt\":1512056536558,\"createdBy\":\"photogallery\",\"v\":1,\"_id\":\"5a2026d894c0a42e53d08245\"},{\"album\":\"5a20264f94c0a42e53d08241\",\"title\":\"Leaves 2\",\"filename\":\"sample-2.jpg\",\"publicationDate\":\"2017-10-01T14:00:00.000Z\",\"apiVersion\":\"1.0\",\"createdAt\":1512056536558,\"createdBy\":\"photogallery\",\"v\":1,\"_id\":\"5a2026d894c0a42e53d08246\"}]}\n```\n\nOkay, so now we have three albums and five photos. We are now ready to move over to our Web service and create our gallery frontend.\n\n### Web\n\nLeave the API directory, going back up a level to the \"gallery\" directory. We're gojng to create a new Web service using the DADI CLI command `dadi web new <name>`:\n\n```bash\n$ cd ..\n$ dadi web new web\n```\n\nYou'll be asked which template engine to use, we're just going to go with Dust.js using the module `@dadi/web-dustjs`, as it's easy.\n\nThe first thing we will do is update our `config/config.development.json` file. Open that, and set the contents to:\n\n```json\n{\n  \"global\": {\n    \"cdn\": \"http://127.0.0.1:8002\"\n  },\n  \"server\": {\n    \"host\": \"127.0.0.1\",\n    \"port\": 8000\n  },\n  \"api\": {\n    \"host\": \"127.0.0.1\",\n    \"port\": 8001\n  },\n  \"auth\": {\n    \"tokenUrl\":\"/token\",\n    \"clientId\":\"photogallery\",\n    \"secret\":\"s3cr3tsqu1rr3l\"\n  },\n  \"cluster\": false,\n  \"allowJsonView\": true,\n  \"debug\": true\n}\n```\n\nIn the `global` section you can see we have a single variable, which is the URL of our CDN instance. In the `server` you can see we're listening on port `8000`. If you recall, the API is listening on port `8001`, and the CDN on port `8002`. We then configure the location of the API instance, and provide some authentication credentials (which we setup earlier.)\n\n#### Preparing our datasources\n\nNext we're going to prepare our datasources. These files will tell Web how to fetch data from our API service. Create a file within the `workspace/datasources` directory called `albums.json` with the following:\n\n```json\n{\n  \"datasource\": {\n    \"key\": \"albums\",\n    \"name\": \"Photo albums\",\n    \"source\": {\n      \"type\": \"dadiapi\",\n      \"endpoint\": \"1.0/gallery/albums\"\n    },\n    \"paginate\": false,\n    \"count\": 8,\n    \"sort\": {\n      \"publicationDate\": -1\n    },\n    \"requestParams\": [\n      { \n        \"field\": \"_id\", \n        \"param\": \"albumId\"\n      }\n    ],\n    \"filter\": {},\n    \"fields\": {}\n  }\n}\n```\n\nNow create a file called `photos.json` with the following:\n\n```json\n{\n  \"datasource\": {\n    \"key\": \"photos\",\n    \"name\": \"Photos\",\n    \"source\": {\n      \"type\": \"dadiapi\",\n      \"endpoint\": \"1.0/gallery/photos\"\n    },\n    \"paginate\": false,\n    \"count\": 8,\n    \"sort\": {\n      \"publicationDate\": -1\n    },\n    \"requestParams\": [\n      { \n        \"field\": \"_id\", \n        \"param\": \"photoId\"\n      }\n    ],\n    \"filter\": {},\n    \"fields\": {}\n  }\n}\n```\n\nThese datasources are used by our pages to fetch data from the API. In particular, the `source` field provides the location of the data provider, and the `requestParams` field will be used later on to filter the data with specific IDs (for when we're loading individual photos, etc.)\n\n#### Preparing our web pages\n\nNow we'll work on the web pages. The DADI CLI will have autogenerated some files for us within the `workspace/pages` directory. Let's quickly simplify & tidy those up.\n\nReplace `workspace/pages/partials/header.dust` with this:\n\n```html\n<!DOCTYPE html>\n<html>\n  <head>\n    <title>PHOTO GALLERY</title>\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"/styles.css\">\n    <link href=\"https://fonts.googleapis.com/css?family=Scope+One|Vollkorn+SC\" rel=\"stylesheet\">\n  </head>\n  <body>\n\n  <header>\n    <a href=\"/\">\n      <h1>PHOTO GALLERY</h1>\n    </a>\n  </header>\n```\n\nReplace `workspace/pages/partials/footer.dust` with this:\n\n```html\n</body>\n</html>\n```\n\nDelete these files, as we're not going to be using them:\n * `workspace/pages/partials/pagination.dust`\n * `workspace/pages/partials/post.dust`\n * `workspace/pages/post.dust`\n * `workspace/pages/post.json` \n\n#### Create the gallery pages\n\nNow we're ready to start configuring our index page to fetch our photo gallery data. Open up the `workspace/pages/index.json` file and replace with this:\n\n```json\n{\n  \"page\": {\n    \"name\": \"index\",\n    \"description\": \"Photo gallery index\"\n  },\n  \"routes\": [\n    { \"path\": \"/\" }\n  ],\n  \"datasources\": [\n    \"albums\",\n    \"photos\"\n  ]\n}\n```\n\nHere we're defining the page path as `/` and hooking up our two datasources: `albums` and `photos`. Next, open up `workspace/pages/index.dust` and insert the following:\n\n```html\n{>\"partials/header\" /}\n\n{#albums.results}\n  <div class=\"album\">\n    <a href=\"/album/{_id}\">\n      <h2>{.title}</h2>\n    </a>\n    {#photos.results albumId=._id}\n      {@eq key=albumId value=.album}\n        <div class=\"photo\">\n          <a href=\"/photo/{._id}\">\n            <h3>{title}</h3>\n          </a>\n          <p class=\"date\">\n            <time datetime=\"{publicationDate}\">\n              {@formatDate\n                data=publicationDate\n                parseFormat=\"YYYY-MM-DD hh:mm:ss\"\n                format=\"MMMM Do, YYYY\"\n              /}\n            </time>\n          </p>\n          <a href=\"/photo/{._id}\">\n            <img \n              src=\"{global.cdn}/{filename}?width=640&amp;resize=aspectfit&amp;devicePixelRatio=2\" \n              alt=\"{title}\">\n          </a>\n        </div>\n      {/eq}\n    {/photos.results}\n  </div>\n{/albums.results}\n\n{>\"partials/footer\" /}\n```\n\nHere we're looping over all the albums, and then we're checking for photos which belong to that album, and then displaying some information including a small photo. We're utilizing DADI CDN's [image resizing](https://docs.dadi.tech/#cdn/resizing-images) here.\n\nNext, create a file called `workspace/pages/photo.json` file with the following:\n\n```json\n{\n  \"page\": {\n    \"name\": \"photo\",\n    \"description\": \"Photo gallery photo view\"\n  },\n  \"routes\": [\n    {\n      \"path\": \"/photo/:photoId\",\n      \"params\": [\n        {\n          \"param\": \"photoId\",\n          \"fetch\": \"photos\"\n        }\n      ]\n    }\n  ],\n  \"datasources\": [\n    \"photos\"\n  ]\n}\n```\n\nHere we're specifying that the path takes a parameter named `photoId`, and that this should be passed to the `photos` datasource, which will filter our results as we touched on earlier.\n\nNext create a file called `workspace/pages/photo.dust` file and insert the following:\n\n```html\n{>\"partials/header\" /}\n\n{#photos.results}\n<div class=\"album\">\n  <div class=\"photo full\">\n    <a href=\"/{._id}\">\n      <h3>{title}</h3>\n    </a>\n    <p class=\"date\">\n      <time datetime=\"{publicationDate}\">\n        {@formatDate\n          data=publicationDate\n          parseFormat=\"YYYY-MM-DD hh:mm:ss\"\n          format=\"MMMM Do, YYYY\"\n        /}\n      </time>\n    </p>\n    <a href=\"/{._id}\">\n      <img src=\"{global.cdn}/{filename}?width=1920&amp;resize=aspectfit&amp;devicePixelRatio=2\" alt=\"{title}\" class=\"full\">\n    </a>\n  </div>\n</div>\n{/photos.results}\n\n{>\"partials/footer\" /}\n```\n\nHere we're going to display a single photo, twice the size this time, along with some relevant information.\n\nNext, create `workspace/pages/album.json`:\n\n```json\n{\n  \"page\": {\n    \"name\": \"album\",\n    \"description\": \"Photo gallery album view\"\n  },\n  \"routes\": [\n    {\n      \"path\": \"/album/:albumId\",\n      \"params\": [\n        {\n          \"param\": \"albumId\",\n          \"fetch\": \"albums\"\n        }\n      ]\n    }\n  ],\n  \"datasources\": [\n    \"albums\",\n    \"photos\"\n  ]\n}\n```\n\nThis looks a lot like the other file, but with a different path and different parameters. Create `workspace/pages/album.dust` with the following:\n\n```html\n{>\"partials/header\" /}\n\n{#albums.results}\n  <div class=\"album\">\n    <a href=\"/album/{_id}\">\n      <h2>{.title}</h2>\n    </a>\n    {#photos.results albumId=._id}\n      {@eq key=albumId value=.album}\n        <div class=\"photo\">\n          <a href=\"/photo/{._id}\">\n            <h3>{title}</h3>\n          </a>\n          <p class=\"date\">\n            <time datetime=\"{publicationDate}\">\n              {@formatDate\n                data=publicationDate\n                parseFormat=\"YYYY-MM-DD hh:mm:ss\"\n                format=\"MMMM Do, YYYY\"\n              /}\n            </time>\n          </p>\n          <a href=\"/photo/{._id}\">\n            <img src=\"{global.cdn}/{filename}?width=640&amp;resize=aspectfit&amp;devicePixelRatio=2\" alt=\"{title}\">\n          </a>\n        </div>\n      {/eq}\n    {/photos.results}\n  </div>\n{/albums.results}\n\n{>\"partials/footer\" /}\n```\n\nThis doesn't look much different from the index page, except we'll be filtering the albums we return via the requestParams. We could (and certainly would) use partials to reduce repetitive code, but to keep things simple we'll skip that here.\n\nFinally, in the `workspace/public` directory, edit `style.css` and replace the contents with this:\n\n```css\n*, body {\n  box-sizing: border-box;\n  margin: 0;\n  padding: 0;\n  color: white;\n}\n\nbody {\n  background: #111;\n}\n\nh1 {\n  font-family: 'Vollkorn SC', serif;\n  font-size: 240%;\n}\n\nh2, h3, h4 {\n  font-family: 'Scope One', serif;\n}\n\np {\n  font-family: 'Scope One', sans-serif;\n  font-size: 80%;\n}\n\na {\n  text-decoration: none;\n}\na:hover {\n  text-decoration: underline;\n  color: #ccc;\n}\n\nheader {\n  width: 1280px;\n  margin: 50px auto;\n  text-align: center;\n}\n\ndiv.album {\n  width: 1280px;\n  margin: 15px auto;\n  padding: 30px 15px;\n  text-align: center;\n  background: #151515;\n}\ndiv.album:after {\n  content: '';\n  display: table;\n  clear: both;\n}\ndiv.album h2 {\n  margin-bottom: 30px;\n}\n\ndiv.photo {\n  float: left;\n  width: 615px;\n  margin: 0 5px;\n}\ndiv.photo.full {\n  width: 1230px;\n}\ndiv.photo p.date {\n  margin: 2px 0 6px 0;\n}\ndiv.photo img {\n  width: 600px;\n}\ndiv.photo img.full {\n  width: 1200px;\n}\n```\n\n### Preview the site\n\nNow you're ready to run the Web service. Ensure the API and CDN services are both running first, then from the `web` directory, run `npm start` and navigate to `http://localhost:8000`. You should be greeted by the index page:\n\n![1](https://i.imgur.com/Q8ZC7ua.jpg)\n\nIf you click into an album, you should be greeted by the album page:\n\n![2](https://i.imgur.com/2GPIHv5.jpg)\n\nIf you click into a photo, you should be greeted by the photo page:\n\n![3](https://i.imgur.com/sfWSvMT.jpg)\n\nThere is more we could do, such as add pagination, add a better user interface, maybe hookup [DADI Publish](https://dadi.tech/en/publish/) for administration, but here we have the basic flow of image storage and manipulation with CDN, data storage and access with API, as well as effortless presentation with Web.\n\nYou can find the source code for this sample photo gallery at [dadi/sample-photo-gallery](https://github.com/dadi/sample-photo-gallery).","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"In this article we're going to look at how you can construct a photo gallery using DADI Web Services, specifically Web, API and CDN.","published":true,"publishedAt":1512140700000,"slug":"create-a-photo-gallery-website-with-dadi","title":"Create a photo gallery website with DADI","_refWeb-service":{},"web-service":["f0d957ef-7cc5-4c71-bbb6-978c1cb43951","27d1f07c-a77c-4fea-af32-78680674c1f1","4693150d-f75c-4563-bb2d-934214b9e9bc"],"_createdBy":"api-client","_id":"c9cf25bd-4092-45b7-bbc2-16a10381ae77","meta":{"revision":0,"created":1530494386215,"version":0},"$loki":22},{"_apiVersion":"1.0","_createdAt":1530494387231,"_lastModifiedAt":1526062430096,"_lastModifiedBy":"cloud-client","_refCategory":{},"_version":1,"_refAuthor":{},"author":["6d08fa42-f098-4f94-b6a5-24d04ea09500"],"body":"DADI's web services have been in development for over four years. It all started with DADI API, the RESTful backbone of the _API-first_, _Create Once Publish Everywhere_ approach that was the founding thought behind the platform.\n\nWe realized very early on in the development process that DADI API would be playing a big part in many of our internal and external projects. To that end we had to ensure that any software we used in-house or published to NPM was well-tested and easy to maintain.\n\nThis post describes some of the ways we achieve this across our (now significantly larger!) development team.\n\n### Coding style\n\nDuring the first iteration of API development, when the team was super small, we had no real need to implement a coding style for the project. With only two developers, one generally followed the style of the other and the style in the codebase was kept largely the same. As more developers began to contribute to the functionality of API, it was obvious we needed to make some decisions about style.\n\nWe ultimately chose to use [JavaScript Standard Style](https://standardjs.com/) throughout the platform, ensuring all developers adhere to a set of rules that includes both those for visual style (such as indentation), and code structure (such as always handling the `err` argument in a Node.js callback).\n\nWith no editor configuration files required, it's incredibly easy to implement across a distributed team.\n\nIn the beginning I think we all had issues with a style being imposed on us, as every engineer has their own personal style which they've developed over many years of coding. Not having to type a semicolon at the end of a statement was the hardest part for most of us!\n\nUsing a StandardJS linter in our editing software - and adding a precommit step to double-check code that was being committed to the repository - has helped us develop faster and with fewer bugs. Being able to open any file for editing in the platform and read the code without having to adjust to a previous developer's style is an enormous timesaver.\n\n### Unit and acceptance testing\n\nAutomated testing of the code written for the platform has been integral to our development process. The first developers on DADI API added a test suite and a testing process right at the beginning of its development. That test suite has now grown to over 560 individual unit and acceptance tests and the process has been replicated across each of DADI's web services.\n\nUtilizing proven testing software such as [Mocha](https://mochajs.org/), [Nock](https://github.com/node-nock/nock), [Sinon.JS](http://sinonjs.org/) and [SuperTest](https://github.com/visionmedia/supertest), there is no better way for us to feel confident when making changes to the codebase.\n\n### Code coverage \n\nIt's all very well having a set of unit and acceptance tests, but how do you know whether you're testing the right things? To obtain statistics on how much of our code is covered by tests, we use [Istanbul](https://istanbul.js.org/).\n\nIstanbul produces a coverage report at the end of every test run which tells us the overall percentage of code that is covered by tests and even goes so far as highlighting - in a lovely shade of red - code that is not touched during a test run. We display the overall coverage percentage in a badge on the individual web service repositories - and whilst we're not at 100% coverage, we're working to improve the coverage every week.\n\nIn some cases if the coverage percentage shows a  decrease when running the tests, we consider the build a failure. This helps to remind developers that any new or modified code must be covered by at least one test.\n\n### Continuous integration \n\nIn addition to running the test suite on a local development machine, DADI's web services are tested throughly by [Travis CI](https://travis-ci.org/dadi). For every pull request submitted to a repository, Travis builds the application, runs the test suite and reports on the success of the build. With Slack integration and tools such as [CC Menu](http://ccmenu.org/), everyone on the team knows how a particular change has affected the codebase.\n\n### Contributing to the codebase\n\nThe DADI engineering team has a number of standards for committing to the codebase, including branch naming conventions and commit message guidelines. Having conventions for these two common tasks makes code and pull-request reviews easier and allows us to automate some of the processes.\n\nCommit messages follow the same [guidelines set by the Angular](https://github.com/angular/angular/blob/master/CONTRIBUTING.md#commit) project. Prefixing commit messages with the type of change makes our git history more readable and for some web services, in conjunction with [Semantic Release](https://github.com/semantic-release/semantic-release), allows for new versions to be published to NPM with automatically generated release notes.\n\n### Keeping an eye on dependencies\n\nIn any project ensuring that dependencies are stable and up-to-date can be a daunting task. At DADI we utilize the services of Snyk and Greenkeeper to help us manage updates to dependencies.\n\n[Snyk](https://snyk.io/) continually monitors our products for issues, checking each dependency against a vast database of  vulnerabilities. For every vulnerability discovered, Snyk suggests actions that can be taken to protect against it, even going so far as submitting a pull request to the repository.\n \n[Greenkeeper](https://greenkeeper.io/) provides a service that informs the development team of new versions of dependencies. New versions of our dependencies are often published to NPM in response to vulnerabilities discovered by Snyk in _their_ dependencies - Greenkeeper makes managing this often-painful process a breeze, and like Snyk it will submit pull requests to the appropriate repository for review.\n\n### It's all about confidence\n\nWhilst we haven't always got it right the first time - there have been a couple of patch releases to NPM mere minutes after a minor version release, for example - the above development processes have evolved over time to where they are today. They are unobtrusive, reliable and an absolute necessity to give the engineering team and the many users of our products confidence that the software works as expected.\n\nFor more detailed information about any of these processes in our development lifecycle, don't hesitate to get in touch!","category":["6d898244-6df5-40ed-adaf-7378f1fd0a55"],"excerpt":"We realized very early on in the development process that DADI API would be playing a big part in many of our internal and external projects. To that end we had to ensure that any software we used in-house or published to NPM was well-tested and easy to maintain.","published":true,"publishedAt":1512300480000,"slug":"maintaining-code-quality-at-dadi","title":"Maintaining code quality at DADI","_createdBy":"api-client","_id":"14c37187-149d-473c-8b0f-95350c01a42c","meta":{"revision":0,"created":1530494387231,"version":0},"$loki":23},{"_apiVersion":"1.0","_createdAt":1530494388245,"_lastModifiedAt":1527666489966,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["2647a07e-0b1f-45c0-8e90-0b55475d838b"],"body":"Publish is the content management component of the DADI web services stack, a set of flexible interfaces to easily manage content stored in API. This post gives an overview of setting it up and connecting to a DADI API instance.\n\n> We should note that many of the core web services in the DADI platform can be installed very simply using the [DADI CLI](https://docs.dadi.tech/#cli) tool. Publish is still in a public beta phase and doesn't yet benefit from CLI's simplified install process. \n\n## Setup and build\n\n1. We need to start by adding a directory for the Publish files. If you already have one, change into that directory first.\n\n    ```shell\n    $ mkdir ~/projects/publish-cms\n    $ cd ~/projects/publish-cms\n    ```\n\n2. Add a [package.json](https://docs.npmjs.com/getting-started/using-a-package.json) file to this directory, to list our dependencies. Add a new file to your directory with the following contents:\n\n    **~/projects/publish-cms/package.json**\n    ```json\n    {\n      \"name\": \"publish-cms\",\n      \"version\": \"1.0.0\",\n      \"main\": \"index.js\",\n      \"dependencies\": {\n        \"@dadi/publish\": \"1.0.3-beta\"\n      }\n    }\n    ```\n\n3. Add a file called `server.js`, which we'll use for launching the application:\n\n    **~/projects/publish-cms/server.js**\n\n    ```js\n    const app = require('@dadi/publish')\n    app.run()\n    ```\n\n4. Install the Publish dependency. Our package.json contains a reference to the `@dadi/publish` dependency, so let's now pull that into the project:\n\n    ```shell\n    $ npm install\n    ```\nDuring the installation process you'll see some deprecation warnings - these are nothing to worry about at this stage.\n\nWhen installation completes, you're ready to start the application.\n\n\n5. You can now run `npm start` to launch the application. Publish runs on port 3001 by default - you should be able to navigate to [http://localhost:3001](http://0.0.0.0:3001) in your browser where you'll see Publish running. The first thing you'll notice is the warning \"API failure\" - we haven't configured the connection with API yet, so this is expected.\n\n## Connecting to API\n\nNow that the application is installed, we're going to configure it to connect to an instance of DADI API.\n\nIf you have your own instance of API running, use that in the below configuration of Publish. If you don't have an API running yet, head to the [API documentation](https://docs.dadi.tech/#api/creating-an-api) for more information, or check out the [Adam K Dean's article](https://forum.dadi.tech/topic/41/how-to-create-a-photo-gallery-website-with-dadi) on setting up a photo gallery website where he briefly explains API installation. In a future article, we'll be covering \"Installing API with CLI and setting up collections\".\n\n1. Create a `config` directory in your Publish directory to contain the configuration files for the application:\n\n2. Inside the new `config` directory create a configuration file for the development environment called `config.development.json`. Add the following:\n\n    **~/projects/publish-cms/config/config.development.json**\n    ```json\n    {\n      \"app\": {\n        \"name\": \"DADI Publish\",\n        \"publisher\": \"My Publishing Company\"\n      },\n      \"server\": {\n        \"host\": \"0.0.0.0\",\n        \"port\": 3001,\n      },\n      \"apis\": [\n        {\n          \"name\": \"Test API\",\n          \"host\": \"http://localhost\",\n          \"port\": 3002,\n          \"database\": \"api\",\n          \"version\": \"1.0\",\n          \"credentials\": {\n            \"clientId\" : \"your-client-id\",\n            \"secret\" : \"your-secret\"\n          }\n        }\n      ],\n      \"auth\": {\n        \"enabled\": false\n      }\n    }\n    ```\n\n3. You can now run `npm start` again. You should see a number of collections listed in the menu, which should look something like this:\n\n![](/media/2018/05/30/1512488142734screen-shot-on-2017-12-04-at-11-3a05-3a41.png)\n\n## Overriding Publish defaults\n\nBy default all collections and collection fields from your API are visible and editable within Publish. To hide collections and/or fields within collections, see below.\n\n### Extending API fields\n\nAPI collection schemas allow custom properties that can be read by Publish to modify the editing interface.\n\nAdd a `publish` configuration block to each field in your API collection schemas to control how they appear in Publish:\n\n```cson\n{\n  \"fields\": {\n    \"field1\": {\n      \"type\": \"String\",\n      \"required\": true,\n      \"label\": \"Title\",\n      \"publish\": {\n        \"display\": {\n          \"edit\": true, // field will appear in the edit view\n          \"list\": false // field will not appear in the index view\n        }\n      }\n    }\n  }\n}\n```\n\n### Hiding an entire collection\n\nHide a collection by adding `hidden` to the collection settings block:\n\n```json\n{\n  \"fields\": {\n    \"field1\": {\n      \n    },\n    \"field2\": {\n      \n    },\n  },\n  \"settings\": {\n    \"publish\": {\n      \"hidden\": true\n    }\n  }\n}\n```\n\n## Next steps\n\nThis post has only just scratched the surface of what is possible with DADI Publish. In a future article, we'll go into more detail about setting up authentication for Publish so you can control access to your content management system.\n\nIf you want to know more about how Publish can elevate the potential of your editorial team, ask me here, on [Discord](https://discordapp.com/invite/3sEvuYJ) or tweet [@dadi](https://twitter.com/dadi?lang=en).","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"Publish is the content management component of the DADI web services stack, a set of flexible interfaces to easily manage content stored in API.","published":true,"publishedAt":1512486420000,"slug":"publish:-step-by-step-first-install","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Publish: Step-by-step first install","_refWeb-service":{},"web-service":["20ccf255-452b-48fd-9a89-4afccfe4d8d7"],"_createdBy":"api-client","_id":"d3bf54fd-a942-4735-b875-efbde0a37d41","meta":{"revision":0,"created":1530494388247,"version":0},"$loki":24},{"_apiVersion":"1.0","_createdAt":1530494389264,"_lastModifiedAt":1526417771563,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["6d08fa42-f098-4f94-b6a5-24d04ea09500"],"body":"API 3.0 comes with various performance and flexibility enhancements, some of which introduce breaking changes. This document is an overview of the changes that are required to make your application ready for the upgrade. \n\n## Configuring a database and data connector\n\nWhilst API 2.0 requires a MongoDB database to run, version 3.0 is capable of working with virtually any database engine, as long as there is a [data connector module](https://www.npmjs.com/search?q=dadi-api-connector&page=1&ranking=optimal) for it.\n\nWhen migrating from 2.0, we need to explicitly specify MongoDB as our database engine by adding `@dadi/api-mongodb` as a project dependency:\n    \n```bash\n$ npm install @dadi/api-mongodb --save\n```\n\nAPI requires each data connector to have its own configuration file located in the same directory as API's main configuration files. Just like API, you'll need one for each environment you run the application in.\n\nFor example, if you currently have a `config.development.json` and `config.production.json` configuration files, you'll need to place `mongodb.development.json` and `mongodb.production.json` in the same directory.\n\n```bash\napi-app/\n  config/              # contains environment-specific configuration files\n    config.development.json\n    config.production.json\n    mongodb.development.json\n    mongodb.production.json\n  package.json\n  workspace/\n    collections/       \n    endpoints/         \n```\n\n### Automatic migration script\n\nWe've added a migration script which can backup your existing API 2.0 configuration files and generate new API 3.0-compatible files automatically.\n\nTo use it, run the following command from your existing API directory:\n\n```bash\n$ curl https://raw.githubusercontent.com/dadi/registry/master/api/migration-scripts/v2-v3.js | node\n```\n\n### Manual configuration\n\nIf you're configuring this manually, follow these steps:\n\n1. Remove the contents of the `database` property from each of your API configuration files, and paste it into the corresponding MongoDB configuration file, so that it looks similar to the following:\n\n    ```json\n    {\n      \"hosts\": [\n        {\n          \"host\": \"123.456.78.9\",\n          \"port\": 27017\n        }\n      ],\n      \"username\": \"\",\n      \"password\": \"\",\n      \"testdb\": {\n        \"hosts\": [\n          {\n            \"host\": \"111.222.33.4\",\n            \"port\": 27017\n          }\n        ]\n      }\n    }\n    ```\n\n2. Each block of database overrides should now be namespaced under a `databases` block. Using the above as our example, it should now be similar to the following. Notice how we've moved the `\"testdb\"` database configuration _inside_ the new `\"databases\"` block:\n      \n\n    ```json\n    {\n      \"hosts\": [\n        {\n          \"host\": \"123.456.78.9\",\n          \"port\": 27017\n        }\n      ],\n      \"username\": \"\",\n      \"password\": \"\",\n      \"databases\": {\n        \"testdb\": {\n          \"hosts\": [\n            {\n              \"host\": \"111.222.33.4\",\n              \"port\": 27017\n            }\n          ]\n        }\n      }\n    }\n    ```\n\n3. In the API configuration files, add a new property `\"datastore\"` where the `\"database\"` property was. It should have the value `\"@dadi/api-mongodb\"`:\n\n\n    ```json\n    {\n      \"server\": {\n        \"host\": \"127.0.0.1\",\n        \"port\": 8000\n      },\n      \"datastore\": \"@dadi/api-mongodb\",\n      \"caching\": {\n        \n      }\n    }\n    ```\n\n4. Your API configuration files should have an `\"auth\"` containing a `\"database\"` block. Change this to simply the name of the database you want to use for authentication, and add a `\"datastore\"` property with the value `\"@dadi/api-mongodb\"`.\n\n    *Before (`config.development.json`)*\n    \n    ```json\n    {\n      \"auth\": {\n        \"tokenUrl\": \"/token\",\n        \"tokenTtl\": 1800,\n        \"clientCollection\": \"clientStore\",\n        \"tokenCollection\": \"tokenStore\",\n        \"database\": {\n          \"hosts\": [\n            {\n              \"host\": \"127.0.0.1\",\n              \"port\": 27017\n            }\n          ],\n          \"username\": \"\",\n          \"password\": \"\",\n          \"database\": \"dadiapiauth\"\n        }\n      }\n    }\n    ```\n    *After (`config.development.json`)*\n    ```json\n    {\n      \"auth\": {\n        \"tokenUrl\": \"/token\",\n        \"tokenTtl\": 1800,\n        \"clientCollection\": \"clientStore\",\n        \"tokenCollection\": \"tokenStore\",\n        \"datastore\": \"@dadi/api-mongodb\",\n        \"database\": \"dadiapiauth\"\n      },\n    }\n    ```\n\n5. If your chosen authentication database (e.g. `\"dadiapiauth\"`) has different hosts to the default you must  ensure an entry exists for it in the `\"databases\"` block in `mongodb.development.json`:\n\n    *`mongodb.development.json`*\n    ```json\n    {\n      \"databases\": {\n        \"dadiapiauth\": {\n          \"hosts\": [\n            {\n              \"host\": \"222.333.44.5\",\n              \"port\": 27017\n            }\n          ]\n        }\n      }  \n    }\n    ```\n\n## What's next?\n\nWhile the above configuration changes should be enough to get the application started, there are several more changes you should know about. They can be found in detail in the release notes for [API Vesion 3.0](https://github.com/dadi/api/releases/tag/v3.0.0).","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"API 3.0 comes with various performance and flexibility enhancements, some of which introduce breaking changes.","published":true,"publishedAt":1512660060000,"slug":"migrating-from-dadi-api-2.0-to-3.0","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Migrating from DADI API 2.0 to 3.0","_refWeb-service":{},"web-service":["27d1f07c-a77c-4fea-af32-78680674c1f1"],"_createdBy":"api-client","_id":"9c6e8351-cde5-4a13-ace7-f66e0a61518e","meta":{"revision":0,"created":1530494389266,"version":0},"$loki":25},{"_apiVersion":"1.0","_createdAt":1530494390283,"_lastModifiedAt":1526417790131,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["0c46e2ee-d524-468d-9aff-c0e6439f191e"],"body":"Most of the DADI platform is built on Node.js, which means access to a plethora of ways to manage front-end workflow within your app: [Bower](https://bower.io/), [Grunt](https://gruntjs.com/), [Gulp](https://gulpjs.com/), [Webpack](https://webpack.js.org/) are just a few.\n\nAt DADI we find the majority of our front-end needs can be met with simple chaining of NPM scripts - eliminating another level of abstraction and one less dependancy.\n\n## Folder structure\n\nAssuming you have [installed DADI Web](https://forum.dadi.tech/topic/31/dadi-web-first-install) you can begin to setup your folder structure. A typical folder structure for us looks like the following:\n\n```\nworkspace/\n├── datasources/\n├── events/\n├── frontend/\n|   ├── lib/\n|   ├── sass/\n|   └── src/\n├── middleware/\n├── pages/\n├── public/\n|   └── assets/\n|      ├── img/\n|      ├── css/\n|      └── js/\n├── routes/\n└── utils/\n```\n\n`frontend/lib`\n\nAny libraries that we have written ourselves, or cannot be found on [NPM](https://npmjs.com).\n\n`frontend/sass`\n\nSass gives us an improved CSS syntax so we can write less code which is more modular and easy to follow.\n\n`frontend/src`\n\nHere we store the source files for each Javascript module we write.\n\n`public/assets`\n\nThese three folders, `img`, `css` and `js` the publicly accessible endpoint for assets to be served through DADI Web.\n\n## NPM scripts\n\nTypically you will use `npm start` to start DADI Web, but we can also use these commands to initiate a build script for us. We are going to assign ours to `npm run build`.\n\nFirst off, here’s what the final `package.json` looks like. It might seem messy, but if you follow along from to to bottom it is in a logical order.\n\n```json\n{\n  \"scripts\": {\n    \"prebuild\": \"mkdir -p workspace/public/assets/css workspace/public/assets/js\",\n    \"build:css\": \"node-sass workspace/frontend/sass/main.scss workspace/public/assets/css/main.css --source-map-embed true --source-map-contents true\",\n    \"postbuild:css\": \"postcss workspace/public/assets/css/main.css --use postcss-import --use autoprefixer --autoprefixer.browsers '> 1%' --use postcss-clean -o workspace/public/assets/css/main.min.css --replace --map\",\n    \"prebuild:js\": \"jshint workspace/frontend/src/**.js\",\n    \"build:js\": \"browserify workspace/frontend/src/**.js --debug | exorcist workspace/public/assets/js/main.js.map > workspace/public/assets/js/main.js\",\n    \"postbuild:js\": \"uglifyjs workspace/public/assets/js/main.js -o workspace/public/assets/js/main.min.js\"\n  },\n  \"devDependencies\": {\n    \"autoprefixer\": \"^7.1.4\",\n    \"browserify\": \"^14.5.0\",\n    \"exorcist\": \"^1.0.0\",\n    \"jshint\": \"^2.9.5\",\n    \"node-sass\": \"^4.7.2\",\n    \"postcss-clean\": \"^1.1.0\",\n    \"postcss-cli\": \"^4.1.1\",\n    \"postcss-import\": \"^11.0.0\",\n    \"uglify-js\": \"^3.2.0\"\n  }\n}\n```\n\n1. `prebuild`  \nThis is a simple command to create the folders we discussed previously (incase they don’t exist for whatever reason).\n2. `build`  \nAlthough we haven’t expressively said it, when we `npm run build` it will run the child scrips delimitated by the `:`, therefore the next two will be run...\n3. `build:css`  \nUse [node-sass](https://github.com/sass/node-sass) to compile our CSS from Sass - for smaller projects you may not even need this extra toolset, but we find it helps larger projects stay organized. \n4. `postbuild:css`  \nUse [PostCSS](https://github.com/postcss/postcss) to [import other CSS files ‘require’ style](https://github.com/postcss/postcss-import), add [vendor prefixes](https://github.com/postcss/autoprefixer/) and then [minify the output](https://github.com/leodido/postcss-clean)...or any other PostCSS plugin you want.\n5. `prebuild:js`  \nUse [JSHint](http://jshint.com/) to check for any code issues (this will stop the process and flag any errors in your terminal).\n6. `build:js`  \nUse [Browserify](http://browserify.org/) to compile the `src/` folder into a single JS file. We then use [exorcist](https://www.npmjs.com/package/exorcist) to create a sourcemap.\n7. `postbuild:js`  \nUse [UglifyJS](https://www.npmjs.com/package/uglify-js) to minify the output.\n\n## Sourcemaps only for debugging\n\nWe now have a minified and no-minified version of our JS and CSS. For an extra touch you can load the non-minified version, including sourcemaps, when you’re in debug mode. For example using [ES6 template](https://www.npmjs.com/package/web-es6-templates) syntax:\n\n```\n<link rel=\"stylesheet\" href=\"/assets/css/main${debug ? '.min' : ''}.css\">\n```\n\n```\n<script src=\"/assets/js/main${debug ? '.min' : ''}.js\"></script>  \n```\n\n## Conclusion\n\nHopefully that's covered an average setup. Once you get familiar with all the CLI tools out there you can begin to add more customisation – such as live reloading and [transpiling](https://babeljs.io/) your JS. Questions? Find us on [Discord](https://discordapp.com/invite/3sEvuYJ).","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"Most of the DADI platform is built on Node.js, which means access to a plethora of ways to manage front-end workflow within your app.","published":true,"publishedAt":1512660180000,"slug":"dadi-web:-handling-front-end-assets","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"DADI Web: Handling front-end assets","_refWeb-service":{},"web-service":["f0d957ef-7cc5-4c71-bbb6-978c1cb43951"],"_createdBy":"api-client","_id":"334dcc54-318b-4327-ab36-f8ca6ca94890","meta":{"revision":0,"created":1530494390285,"version":0},"$loki":26},{"_apiVersion":"1.0","_createdAt":1530494391301,"_lastModifiedAt":1526417821590,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["0c46e2ee-d524-468d-9aff-c0e6439f191e"],"body":"In the next release of DADI Web, version 5.0, we’ll be adding a small but powerful new feature we’re calling ‘post processors’ (borrowed from a little known [Wordpress feature](https://codex.wordpress.org/How_WordPress_Processes_Post_Content)).\n\nFor example you may want to enforce a style-guide amongst several editors, such as enforcing [typographers quotes](https://practicaltypography.com/straight-and-curly-quotes.html) or converting units of measurement automatically for different audiences.\n\nOne use we’ve been finding it particularly useful for is minifying the HTML output of a website.\n\n## Example\n\nThis process is very straightforward thanks to a NPM package called [html-minifier](https://www.npmjs.com/package/html-minifier), so let’s go ahead and install that in our project:\n\n```bash\nnpm install html-minifier\n```\n\nNext we will create a file event in the new workspaces folder `processors`.\n\n**workspace/processors/minify-html.js**\n\n```javascript\nconst minify = require('html-minifier').minify\n\nmodule.exports = (data, output) => {\n  return minify(output, {\n    collapseWhitespace: true,\n    minifyCSS: true,\n    minifyJS: true,\n    removeRedundantAttributes: true,\n    useShortDoctype: true,\n    removeComments: true\n  })\n}\n```\n\nWe’re using the options here provided by the html-minifier package we installed. Of course your needs may differ so customize for your requirements.\n\nNext we need to update Web's configiration file (for example `config/config.development.json`) to load the new processor for every request:\n\n```json\n\"globalPostProcessors\": [\n   \"minify-html\"\n]\n```\n\nAnd that’s it! You should now see minified HTML sent to your browser.\n\nIf you need to disable (or enable) on a page-by-page basis you can customize the options in your `page.json` files:\n\nTo disable all global post processors:\n\n```json\n{\n  \"settings\": {\n     \"postProcessors\": false\n  }\n}\n```\n\nTo enable only a specific one:\n\n```json\n{\n  \"settings\": {\n    \"postProcessors\": [\"remove-swear-words\"]\n  }\n}\n```\n\n## Conclusion\n\nYou can read more of the technical details in the [pull request](https://github.com/dadi/web/pull/253) for this feature. \n\nWe will be talking more about Web 5.0 in the coming weeks so please stay tuned or let us know if there are any features you would like to see. And of course, we’ll let you know when it’s released via [Twitter](https://twitter.com/dadi).","category":["0eda8d2c-e76b-484e-a95b-6ea4b3b1eaa3"],"excerpt":"In the next release of DADI Web, version 5.0, we’ll be adding a small but powerful new feature we’re calling ‘post processors’.","published":true,"publishedAt":1512833040000,"slug":"dadi-web-feature:-post-processors","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"DADI Web feature: post processors","_refWeb-service":{},"web-service":["f0d957ef-7cc5-4c71-bbb6-978c1cb43951"],"_createdBy":"api-client","_id":"3e093dfa-fec5-4133-b7c7-2ec87ace9323","meta":{"revision":0,"created":1530494391302,"version":0},"$loki":27},{"_apiVersion":"1.0","_createdAt":1530494392317,"_lastModifiedAt":1526417839317,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["cdeeab35-b8e7-4aba-ae10-54bffead2ef1"],"body":"## Part I: Running DADI Web in a container\n\nThis is the first article in a series of articles which will take you through the containerization of three core DADI web services: Web, API, and CDN.\n\nThe first service we'll containerize is [DADI Web](https://dadi.tech/en/web/), our schemaless templating layer. First we'll create a standard Web project, and then we'll containerize it. We'll go through some of the basics of Docker, and how we can use them alongside DADI Web.\n\n## Docker\n\nDocker is an open container platform providing developers and sysadmins with a lightweight method of process & resource isolation, the ability to package an application and all it's dependencies into a single image, allowing greater levels of automation and portability.\n\nThe DADI suite runs on Node.js and works perfectly fine without containers, but by utilising containers, we can gain greater isolation, scalability, and enhance our automation ability as well.\n\nIf you haven't used Docker before, then I'd recommend reading through the [Docker getting started](https://docs.docker.com/get-started/) guide.\n\n## Requirements\n\nFor this article I'll be working with `Docker 17.06.2-ce-mac27` and `macOS Sierra` . If you don't have Docker installed, you can get it from the [Docker Community Edition](https://www.docker.com/community-edition) website.\n\nI'll also be installing DADI applications with the DADI command line interface, to install that run `npm install @dadi/cli -g`.\n\n## Getting started\n\nWe'll begin by creating a directory for our project and installing DADI Web using the DADI command-line interface tool. When asked to pick a template engine, select `@dadi/web-dustjs`.\n\n```bash\n$ mkdir web-project\n$ cd web-project\n$ dadi web new\n```\n\nOnce Web is installed, you should have a number of files & directories. By default, no configuration is necessary. Read more about [DADI Web configuration](https://docs.dadi.tech/web/configration).\n\nLet's start the application and preview the default site.\n\n```bash\n$ npm start\n```\n\nNow open [http://127.0.0.1:3001](http://127.0.0.1:3001/).\n\nYou should be greeted by the default site. Cool, now let's containerize it.\n\n## Configuring Web\n\nThe first thing we need to do is configure DADI Web to work inside a container. Containers don't by default have a local loopback so we can't bind our port to `127.0.0.1`, instead we need to bind to `0.0.0.0`. Open up the `config/config.development.json` file in your favourite text editor and change the host to `0.0.0.0`, and while we're here, the port to `80` as well.\n\n```bash\n$ vi config/config.development.json\n```\n\n```json\n{\n  \"global\": {\n    \"site\": \"Your site name\",\n    \"description\": \"An exciting beginning.\"\n  },\n  \"globalEvents\": [\n    \"global\"\n  ],\n  \"server\": {\n    \"host\": \"0.0.0.0\",\n    \"port\": 80\n  },\n  \"cluster\": false,\n  \"allowJsonView\": true,\n  \"debug\": true\n}\n```\n\nDADI Web should now be ready to run in a container.\n\n## Creating a Dockerfile\n\nDockerfiles are like recipes for container images. Traditionally they are kept in the root of the project. Create a file named `Dockerfile` and open it up in your text editor.\n\n```bash\n$ vi Dockerfile\n```\n\n```\nFROM node:6.11\n\nRUN mkdir /var/web\nADD . /var/web\nWORKDIR /var/web\n\nRUN npm install -q\n\nCMD [\"npm\", \"start\"]\n```\n\nFirst we specify a parent container image that we want to build on top of, in this case, `node:6.11`, the official Node.js image. Then we create a directory for the service and add in our project files. We set the working directory, run `npm install` and then, finally, specify the command to run on execution (`npm start`).\n\n## Building a Docker image\n\nNow that we have our `Dockerfile` setup, we need to build our container image. Docker has images, which are binaries generally built from Dockerfiles, and containers, which are running instances of those images.\n\nBefore we do this, let's create a `.dockerignore` file. This works a lot like a `.gitignore` file, and lets us exclude files and directories from being sent to the Docker daemon. This'll make our build process quicker and more efficient.\n\n```bash\n$ vi .dockerignore\n```\n\n```\nnode_modules\nlog\n```\n\nNow let's build our image. With Docker installed, we simply need to tell the Docker CLI to build the image from the current directory. We'll give it a tag with the `-t` flag as well so we can refer to it easier later on.\n\n```bash\n$ docker build -t web-project .\n```\n\nOnce the build has finished, we should be able to see the image:\n\n```bash\n$ docker images | grep web-project\n\nweb-project latest 2398fc7376be About a minute ago 770MB\n```\n\nGreat. Now we're ready to run the container.\n\n## Running the Docker container\n\nThere are two methods that we can use to run the container: in the foreground with an attached tty, or in the background detached. First, let's test everything's running okay by running it in the foreground. The `-ti` flags tell Docker that we want to attach an interactive tty, and the `--rm` flag will remove the container once we exit.\n\n```bash\n$ docker run -ti --rm web-project\n\n> @dadi/web-boilerplate@ start /var/web\n> node server.js\n\n ----------------------------\n DADI Web (Repo Default)\n Started 'DADI Web'\n ----------------------------\n Server: http://0.0.0.0:80\n Version: 4.0.2\n Node.JS: 6.11\n Environment: development\n Engine: dust\n API: Disabled\n ----------------------------\n```\n\nYou'll see that the application starts up and is listening, by default, on `http://0.0.0.0:80`, but we can no longer access the Web service from our browser. We need to setup some port mapping. Let's exit out of the container with `ctrl-c` .\n\nEach container connects by default to the `docker0` bridge network. This is fine for us but we need to map a port on our host to port `80` on the container. The easiest way to do this is with a direct port mapping using the `-p` or `--publish` flag.\n\nLet's map port `8000` on our host to port `80` on our container.\n\n```bash\n$ docker run -ti -p 8000:80 --rm web-project\n```\n\nIf you open up `http://127.0.0.1:8000`, you should now be greeted by the default website. Great, it works! Let's exit out of the container with `ctrl-c` again and now we'll start the container in the background, using the `-d` or `--detach` flag. We no longer want the container to remove-on-exit so we'll also remove the `--rm` flag. We'll also give the container a name so that we can refer to it later on. You can also refer to containers via their long or short UUID identifiers, but names are much nicer, aren't they?\n\n```bash\n$ docker run -d -p 8000:80 --name web-project web-project\n\n403d5e10f5416d7f7a445fb88a851019e6dfd1335a59c35e0d8e2f83c9524c64\n```\n\nWe can now see that the container `web-project` is running.\n\n```bash\n$ docker ps\n\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\n403d5e10f541 web-project \"npm start\" 22 seconds ago Up 21 seconds 0.0.0.0:8000->80/tcp web-project\n```\n\nWe can stop and start this container with the `docker stop <id/name>` and `docker start <id/name>` commands, and if we want to create a fresh container with the name `web-project` then we can remove it with `docker rm <id/name>` .\n\nThat's all there is to it. You now have a website running in DADI Web.\n\nNext up, Part II: [Running DADI API in a container](https://forum.dadi.tech/topic/56/running-dadi-api-in-a-container).","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"This is the first article in a series of articles which will take you through the containerization of three core DADI web services: Web, API, and CDN.","published":true,"publishedAt":1513178700000,"slug":"running-dadi-web-in-a-container","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Running DADI Web in a container","_refWeb-service":{},"web-service":["f0d957ef-7cc5-4c71-bbb6-978c1cb43951"],"_createdBy":"api-client","_id":"39c7113b-69be-4121-b815-a423b766844a","meta":{"revision":0,"created":1530494392318,"version":0},"$loki":28},{"_apiVersion":"1.0","_createdAt":1530494393338,"_lastModifiedAt":1526417856411,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["cdeeab35-b8e7-4aba-ae10-54bffead2ef1"],"body":"## Part II: Running DADI API in a container\n\nPreviously, in the [Running DADI Web in a container](https://forum.dadi.tech/topic/55/running-dadi-web-in-a-container) article, we looked at how we could utilize Docker containers to run DADI Web.\n\nIn this article, the next service we'll containerize is [DADI API](https://dadi.tech/en/api/), our high-performance RESTful API layer. As with web, first we'll create a standard API project, and then we'll containerize it. Before we get started with API though, there are a few things we need to setup. We'll need to setup a data store to hold our data, and we'll need to configure a Docker network so that services can talk to each other.\n\n## Docker\n\nDocker is an open container platform providing developers and sysadmins with a lightweight method of process & resource isolation, the ability to package an application and all it's dependencies into a single image, allowing greater levels of automation and portability.\n\nThe DADI suite runs on Node.js and works perfectly fine without containers, but by utilising containers, we can gain greater isolation, scalability, and enhance our automation ability as well.\n\nIf you haven't used Docker before, then I'd recommend reading through the [Docker getting started](https://docs.docker.com/get-started/) guide.\n\n## Requirements\n\nFor this article I'll be working with `Docker 17.06.2-ce-mac27` and `macOS Sierra`. If you don't have Docker installed, you can get it from the [Docker Community Edition](https://www.docker.com/community-edition) website.\n\nI'll also be installing DADI applications with the DADI command line interface, to install that run `npm install @dadi/cli -g`.\n\n## Getting started\n\nWe're going to be setting up two containers today, our DADI API instance, as well as a data store as well. DADI API currently supports three data connectors: `MongoDB`, `CouchDB`, and `FileStore`. For this article, we'll be using `MongoDB`.\n\n## User Defined Network\n\nPreviously, in the [Running DADI Web in a container](https://forum.dadi.tech/topic/55/running-dadi-web-in-a-container) article, we used the default `docker0` bridge network. But in order for our services to talk to each other, we're going to setup a user defined network. By using a user defined network, we can utilize the embedded DNS server to resolve containers by their names. Read more about [Docker container networking](https://docs.docker.com/engine/userguide/networking/).\n\nLet's create a new bridge network called `dadi-service-network`.\n\n```\n$ docker network create --driver bridge dadi-service-network\n\n35cd5f842594fe553b0369757d70b8356397eb9ad7a40255211d3b55eaaab800\n```\n\nNow when we create containers, we can attach them to that network using the `--network` flag.\n\n## Creating a MongoDB container\n\nNext, we're going to setup the MongoDB server to hold our data. The first thing we're going to need to create is a Docker volume. Volumes are a method for persisting data generated by and used by Docker containers. As a general rule, containers should be transient, and should contain no data themselves that isn't dispensible. Read more about [Docker volumes](https://docs.docker.com/engine/admin/volumes/volumes/).\n\nLet's create a volume for our MongoDB data with the name `api-project-mongo-data`.\n\n```\n$ docker volume create --name=api-project-mongo-data\n\napi-project-mongo-data\n```\n\nNow we can create a MongoDB container and mount our volume to Mongo's default data directory, that way all data that is written to the directory is persisted. We aren't going to publish a port here, we'll connect to it via the user defined network instead.\n\n```\n$ docker run \\\n  --name mongo-server \\\n  --volume api-project-mongo-data:/data/db \\\n  --network dadi-service-network \\\n  --detach \\\n  mongo:3.2\n\n28d98b0d98fb2743a2c90d29d238064ea97e9efc58c2b0140491cdf4e07f4406\n```\n\nGreat. Let's check that that's running okay. I've trimmed the logs for brevity.\n\n```\n$ docker logs mongo-server\n\n15:39:37.615+0000 I CONTROL [initandlisten] MongoDB starting : pid=1 port=27017 dbpath=/data/db 64-bit host=ede6ddeecafa\n**SNIP**\n15:39:37.877+0000 I NETWORK [initandlisten] waiting for connections on port 27017\n```\n\nNow that we have our MongoDB container running, we're ready to setup DADI API. Notice how we didn't create a Dockerfile for the MongoDB container? That's because we're using the prebuilt `mongo` image with the `3.2` version tag. It's prebuilt so we just run it.\n\n## Setting up DADI API\n\nNow it's time to create a DADI API project using the DADI CLI. Previously when setting up DADI Web, we created the directory ourselves. This time we'll use the shorthand `dadi <product> new <project-name>`. This will create the directory for us and take us through the interactive wizard.\n\n```\n$ dadi api new api-project\n```\n\n```json\n✔ Checking the available versions of DADI API\n✔ Pulling the list of available database connectors from NPM\n? Which database engine would you like to install? @dadi/api-mongodb\n✔ Downloading boilerplate (100%)\n✔ Installing DADI API (3.x)\n✔ Installing the '@dadi/api-mongodb' database connector\n\n  ▓▓▓▓▓  ▓▓▓▓▓▓▓\n              ▓▓▓▓\n     ▓▓▓▓▓▓▓    ▓▓▓▓\n              ▓▓▓▓\n          ▓▓▓▓▓▓▓\n  \n\n    DADI API setup\n\nLet's start by configuring the web server that API runs. (0% complete)\n\n? What is the name of this DADI API instance? api-project\n? What is the IP address the application should run on? 0.0.0.0\n? What is the port number? 80\n? What protocol would you like to use? HTTP (insecure)\n\nWe'll now define how your API instance can be accessed from the outside world. (16% complete)\n\n? What is the hostname or domain where your API can be accessed at? api-project\n? What is the port? 80\n\nLooking great! Time to configure your databases. (22% complete)\n\n? What is the name of the database? dadiapi\n? What is the database username? \n? What is the database password? \n? What is the database server host? mongo-server\n? And what is the database server port? 27017\n\nYou'll need an oAuth2 client to interact with API. It consists of an ID + secret pair, which you'll send to API in exchange for a bearer token. This token is then sent alongside each request in order to authenticate you with the system. (44% complete)\n\n? Would you like to create a client? No\n\nLet's now look at caching, which is crucial to ensure that API delivers data in a performant way. (56% complete)\n\n? Would you like to cache items on the local filesystem? Yes\n? What is the path to the cache directory? ./cache/web\n? Would you like to cache items on a Redis server? No\n\nAlmost there! Time to define how API handles media uploads (e.g. images). (75% complete)\n\n? Where should API store uploaded assets? Nowhere, I don't want API to handle media\n\nYou made it! We're wrapping up. (94% complete)\n\n? Which environment does this config apply to? Development\n\n✔ API configuration file written to /projects/api-project/config/config.development.json.\n✔ Database configuration file written to/projects/api-project/config/mongodb.development.json.\n\nAll done!\n```\n\nIf you remember, we named our MongoDB container `mongo-server` and attached it to our user defined network. This means we can use the container name like a hostname, and Docker's embedded DNS server will resolve it for us.\n\nNext, let's create a collection to test later. Create a new directory within `workspace/collections/1.0` called `test` and inside that, a new file called `collection.messages.json`.\n\n```\n$ cd api-project\n$ mkdir workspace/collections/1.0/test\n$ vi workspace/collections/1.0/test/collection.messages.json\n```\n\n```json\n{\n  \"fields\": {\n    \"name\": {\n      \"type\": \"String\",\n      \"label\": \"name\",\n      \"example\": \"Joe Blogs\",\n      \"comments\": \"This is the name of the author\",\n      \"required\": true\n    },\n    \"message\": {\n      \"type\": \"String\",\n      \"label\": \"message\",\n      \"example\": \"Hello world!\",\n      \"comments\": \"This is the author's message\",\n      \"required\": true\n    }\n  },\n  \"settings\": {\n    \"cache\": true,\n    \"authenticate\": true,\n    \"count\": 10,\n    \"sort\": \"name\",\n    \"sortOrder\": 1\n  }\n}\n```\n\nWe'll use this simple collection later to test that everything is working. Read more about [API Collections](https://docs.dadi.tech/api#collections).\n\nDADI API should now be ready to run in a container.\n\n## Creating a Dockerfile\n\nDockerfiles are like recipes for container images. Traditionally they are kept in the root of the project. Create a file named `Dockerfile` and open it up in your text editor.\n\n```\n$ vi Dockerfile\n```\n\n```\nFROM node:6.11\n\nRUN npm install @dadi/cli -qg\n\nRUN mkdir /var/api\nADD . /var/api\nWORKDIR /var/api\n\nRUN npm install -q\n\nCMD [\"npm\", \"start\"]\n```\n\nFirst we specify a parent container image that we want to build on top of, in this case, `node:6.11`, the official Node.js image. Next we install the DADI CLI globally. We'll need that later. Then we create a directory for the service, add in our project files, and set that to the working directory. Next we run `npm install` and then, finally, specify the command to run on execution – `npm start`.\n\n## Building a Docker image\n\nNow that we have our `Dockerfile` setup, we need to build our container image. Docker has images, which are binaries generally built from Dockerfiles, and containers, which are running instances of those images.\n\nBefore we do this, let's create a `.dockerignore` file. This works a lot like a `.gitignore` file, and lets us exclude files and directories from being sent to the docker daemon. This will make our build process quicker and more efficient.\n\n```\n$ vi .dockerignore\n\nnode_modules\nlog\n```\n\nNow let's build our image. With Docker installed, we simply need to tell the Docker CLI to build the image from the current directory. We'll give it a tag with the `-t` flag as well so we can refer to it easier later on.\n\n```\n$ docker build -t api-project .\n```\n\nOnce the build has finished, we should be able to see the image:\n\n```\n$ docker images | grep api-project\n\napi-project latest 925df4ec046d About a minute ago 737MB\n```\n\nGreat. Now we're ready to run the container.\n\n## Running the Docker container\n\nIn the [Running DADI Web in a container](https://forum.dadi.tech/topic/55/running-dadi-web-in-a-container) article, we ran the container in the foreground to check everything worked. This time, we'll run it in the background to begin with, and use the `docker logs` command to peer inside.\n\nBelow we give the container the name `api-project`, attach to the `dadi-service-network` network, map the host port `8001` to the container's port `80`, and specify that we wish to run detached.\n\n```\n$ docker run \\\n  --name api-project \\\n  --network dadi-service-network \\\n  --publish 8001:80 \\\n  --detach \\\n  api-project\n\n152b664e9483f82f0a019e6e2155a11d3bbba7323993d65d69d621f85564090c\n```\n\nNow let's take a look at the logs. We'll use the `-f` flag to \"follow\" the logs. I've trimmed these a bit.\n\n```\n$ docker logs -f api-project\n\n> @dadi/api-boilerplate@ start /var/api\n> node server.js\n\n----------------------------\nDADI API\nStarted 'DADI API'\n----------------------------\nServer: 0.0.0.0:80\nVersion: 2.2.5\nNode.JS: 6.11\nEnvironment: development\n----------------------------\n```\n\nLet's exit out of these logs with `ctrl-c` and see if we can query the API. First, we'll need to add a client to the API so that we can authenticate. For that we'll use the `docker exec` command to attach to the `api-project` container and then use the DADI CLI to add an authentication client to the database. Read more about [DADI API Authentication](https://docs.dadi.tech/api#authentication).\n\n```\n$ docker exec -ti api-project /bin/bash\nroot@152b664e9483:/var/api# dadi api clients:add\n? Enter the client ID someUser\n? Enter a strong secret (press Enter if you want us to generate one for you) \n? What type of access does the user require? Regular user\n✔ Created client with ID someUser and type user. The secret we generated for you is 00nvrno0fv – store it somewhere safe!\n```\n\nNow let's check the Mongo database to see our user. Again we'll use the `docker exec` command.\n\n```\n$ docker exec mongo-server \\\n  mongo dadi-api --eval 'db[\"clientStore\"].find({})'\n\nMongoDB shell version: 3.2.7\nconnecting to: dadi-api\n{ \"_id\" : ObjectId(\"59ce87f21aa0ec006417830e\"), \"clientId\" : \"someUser\", \"secret\" : \"00nvrno0fv\", \"type\" : \"user\" }\n```\n\nNice. We can use this to obtain ourselves a bearer token. I've formatted the JSON result here and below for readability, but if you want this in the terminal you can use the `jq` utility (just pipe the output into `jq` like this: `curl ... | jq` etc.)\n\n```\n$ curl \\\n  --header \"Content-Type: application/json\" \\\n  --data '{\"clientId\":\"someUser\",\"secret\":\"00nvrno0fv\"}' \\\n  http://localhost:8001/token\n\n{\n  \"accessToken\": \"2055ac17-5423-47f1-a31c-a085e302291f\",\n  \"tokenType\": \"Bearer\",\n  \"expiresIn\": 1800\n}\n```\n\nNow we have our access token, let's query the collection we created earlier.\n\n```\n$ curl \\\n  --header \"Authorization: Bearer 2055ac17-5423-47f1-a31c-a085e302291f\" \\\n  --header \"Accept: application/json\" \\\n  http://localhost:8001/1.0/test/messages\n\n{\n  \"results\": [],\n  \"metadata\": {\n    \"limit\": 10,\n    \"page\": 1,\n    \"fields\": {},\n    \"sort\": {\n      \"name\": 1\n    },\n    \"offset\": 0,\n    \"totalCount\": 0,\n    \"totalPages\": 0\n  }\n}\n```\n\nDarn. Empty!\n\nLet's add some messages. If you remember, we need a `name` and a `message` . Let's add two by posting an array of objects to the API.\n\n```\n$ curl \\\n  --header \"Authorization: Bearer 2055ac17-5423-47f1-a31c-a085e302291f\" \\\n  --header \"Accept: application/json\" \\\n  --header \"Content-Type: application/json\" \\\n  --data '[{\"name\": \"Joe Blogs\", \"message\": \"Hello world!\"},{\"name\": \"Steve Jones\", \"message\": \"This is an example message.\"}]' \\\n  http://localhost:8001/1.0/test/messages\n\n{\n  \"results\": [\n    {\n      \"name\": \"Joe Blogs\",\n      \"message\": \"Hello world!\",\n      \"apiVersion\": \"1.0\",\n      \"createdAt\": 1506765822151,\n      \"createdBy\": \"someUser\",\n      \"history\": [],\n      \"v\": 1,\n      \"_id\": \"59cf6bfefb09920011d9fbf1\"\n    },\n    {\n      \"name\": \"Steve Jones\",\n      \"message\": \"This is an example message.\",\n      \"apiVersion\": \"1.0\",\n      \"createdAt\": 1506765822151,\n      \"createdBy\": \"someUser\",\n      \"history\": [],\n      \"v\": 1,\n      \"_id\": \"59cf6bfefb09920011d9fbf2\"\n    }\n  ]\n}\n```\n\nGreat, now let's check again.\n\n```\n$ curl \\\n  --header \"Authorization: Bearer 2055ac17-5423-47f1-a31c-a085e302291f\" \\\n  --header \"Accept: application/json\" \\\n  http://localhost:8001/1.0/test/messages\n\n{\n  \"results\": [\n  {\n    \"_id\": \"59cf6bfefb09920011d9fbf1\",\n    \"name\": \"Joe Blogs\",\n    \"message\": \"Hello world!\",\n    \"apiVersion\": \"1.0\",\n    \"createdAt\": 1506765822151,\n    \"createdBy\": \"someUser\",\n    \"history\": [],\n    \"v\": 1\n  },\n  {\n    \"_id\": \"59cf6bfefb09920011d9fbf2\",\n    \"name\": \"Steve Jones\",\n    \"message\": \"This is an example message.\",\n    \"apiVersion\": \"1.0\",\n    \"createdAt\": 1506765822151,\n    \"createdBy\": \"someUser\",\n    \"history\": [],\n    \"v\": 1\n  }\n],\n\"metadata\": {\n  \"limit\": 10,\n  \"page\": 1,\n  \"fields\": {},\n  \"sort\": {\n    \"name\": 1\n  },\n  \"offset\": 0,\n  \"totalCount\": 2,\n  \"totalPages\": 1\n  }\n}\n```\n\nFantastic. There we go, a fully working, containerized API with data persistence and resource isolation. Let's have a quick recap.\n\n1. We setup a Docker network for our containers to communicate.\n2. Then we created a volume to hold our MongoDB data.\n3. Next we created a MongoDB server and attached the volume & network.\n4. We then created a new DADI API project using the DADI CLI.\n5. Once that was done, we configured API to bind to `0.0.0.0:80` and connect to `mongo-server`, and added a collection to test later on.\n6. Then we wrote our `Dockerfile`, created a `.dockerignore` file to optimize the build, and built our Docker image.\n7. After that, we ran our container, attached it to our network, and published the API port to our host on port `8001`.\n8. We added a client to our API and checked MongoDB to see that it'd been saved using the `docker exec` command.\n9. Finally, we queried our empty collection, added some documents, and queried it again to show our data.\n\nIn the next article (coming soon), we'll look at containerizing DADI CDN, our just-in-time asset manipulation and delivery layer, where we'll be looking at resource persistence & isolation.","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"In this article, the next service we'll containerize is DADI API, our high-performance RESTful API layer.","published":true,"publishedAt":1513351620000,"slug":"running-dadi-api-in-a-container","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Running DADI API in a container","_refWeb-service":{},"web-service":["27d1f07c-a77c-4fea-af32-78680674c1f1"],"_createdBy":"api-client","_id":"96189892-1b38-4d68-ba12-52ce0cd09b9d","meta":{"revision":0,"created":1530494393339,"version":0},"$loki":29},{"_apiVersion":"1.0","_createdAt":1530494394353,"_lastModifiedAt":1526418131887,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["2e0fd8d3-97e6-4398-b55a-3c8b41dd8214"],"body":"We are excited to announce that [Wirehive](https://www.wirehive.com/) CEO [Robert Belgrave](https://www.linkedin.com/in/robertbelgrave/) is joining the DADI team as an advisor.\n\nRobert offers lengthy industry experience in cloud services — he was part of the founding team at Wirehive and instrumental to its success in the five years since.\n\nHe is also chair of [BIMA South](https://www.bima.co.uk/), co-creator of the [Alexa Stop! podcast](https://itunes.apple.com/gb/podcast/alexa-stop-podcast/id1223829037?mt=2) and founder of [Omnitude](http://omnitude.org/), an ecommerce ecosystem built on the blockchain.\n\n> “Rob brings huge experience and insight into the hosting market, which will be invaluable as we look to disrupt the status quo”\n — Joseph Denne, founder and CEO of DADI\n\nRobert’s unique background in cloud consultancy and blockchain services makes him an ideal fit as an advisor to DADI as its decentralized peer-to-peer hosting network develops.\n\n> “The approach being taken by Joe and his team is incredibly exciting for someone that has spent their entire career delivering hosting services. Blockchain technology is going to change most industries for the better and I’m delighted to be advising them on the journey to delivering that promise in cloud hosting.” — Robert Belgrave, CEO of Wirehive\n\n## More information on Wirehive\n\nTo learn more about Wirehive please visit: [wirehive.com](https://www.wirehive.com/).","category":["0eda8d2c-e76b-484e-a95b-6ea4b3b1eaa3"],"excerpt":"We are excited to announce that Wirehive CEO Robert Belgrave is joining the DADI team as an advisor.","published":true,"publishedAt":1513608300000,"slug":"robert-belgrave-joins-dadi","sub-category":["f74856ec-dbdc-45ff-b096-9b078c4b3348"],"title":"Robert Belgrave joins DADI","_createdBy":"api-client","_id":"1064d912-216d-47ff-a5be-f241274249b9","meta":{"revision":0,"created":1530494394354,"version":0},"$loki":30},{"_apiVersion":"1.0","_createdAt":1530494395368,"_lastModifiedAt":1526417876274,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["5a7cde69-353b-4718-88d4-29a4cd2608a5"],"body":"In this article, I'll show you how to build a simple application with DADI API, covering everything from the installation process to the definition of the data architecture.\n\nWe'll take inspiration from [TodoMVC](http://todomvc.com/), a project that shows how a particular application – a to-do list manager – can be built using different front-end frameworks. Let's build a to-do list application using DADI API.\n\n## Installation\n\nThe quickest way to get API up and running is with [DADI CLI](https://docs.dadi.tech/cli). If you haven't already, start by installing it on your system.\n\n```\nnpm install @dadi/cli -g\n```\n\nWith CLI installed, navigate to the directory where you want to install API and use CLI to start the installation.\n\n```\n# Choose any directory\ncd ~/Sites\n\n# This will install API in a subdirectory called todo-api\ndadi api new todo-api\n```\n\nThe command above will do *a lot*. For starters, it will:\n\n1. Find the latest version of API\n2. Pull a blank project (what we call a *boilerplate*) from the [DADI registry](https://github.com/dadi/registry/tree/master/api/boilerplate)\n3. Install all the dependencies using NPM\n\nAfter that, CLI will take you through an interactive setup process where, by asking you a series of questions, it will gather all the information it needs to generate the configuration files API needs.\n\n### Choosing a database engine\n\nThe first question you'll get is about which database engine you'd like to use.\n\n```\n$ dadi api new todo-api\n✔ Checking the available versions of DADI API\n✔ Pulling the list of available database connectors from NPM\n? Which database engine would you like to install? (Use arrow keys)\n❯ @dadi/api-mongodb — A MongoDB adapter for DADI API \n  @dadi/api-filestore — A JSON datastore adapter for DADI API\n```\n\n\nWhen we first launched API, MongoDB was our database engine of choice. This has been the case until version 3.0, released earlier this month, where we rebuilt the application core such that virtually any database engine can be used, as long as there's an adapter for it (or a *data connector*, as we call them).\n\nWe're working on a few data connectors for engines like CouchDB and RethinkDB, but don't feel discouraged if your engine of choice isn't covered yet – we're publishing an article later this month that will show how you can easily build your own data connector for DADI API.\n\nFor our to-do list app, and to keep things simple, we'll choose `@dadi/api-filestore`, which uses an in-memory file store, which is persisted to disk at regular intervals as JSON files.\n\nAs for the rest of the questions, you can customise your installation as you see fit. Here's what I went with:\n\n- *What is the name of this DADI API instance?* `Todos API`\n- *What is the IP address the application should run on?* `0.0.0.0`\n- *What is the port number?* `8081`\n- *What protocol would you like to use?* `HTTP (insecure)`\n\n- *What is the hostname or domain where your API can be accessed at?* `todos-api.com`\n- *What is the port?* `80`\n\n- *What is the name of the database?* `todosapi`\n- *Where do you want to save your database files?* `workspace/db`\n\n- *Would you like to create a client?* `Yes`\n- *What is the client ID?* `todoClient`\n- And what is the secret? Press <Enter> if you want us to generate one for you.\n- *What level of permissions should this client get?* `Administrator`\n\n- *Would you like to cache items on the local filesystem?* `Yes`\n- *What is the path to the cache directory?* `./cache/api`\n- *Would you like to cache items on a Redis server?* `No`\n\n- *Where should API store uploaded assets?* `Nowhere, I don't want API to handle media`\n\n- *Which environment does this config apply to?* `Development`\n\nIf all goes well, you should see a confirmation message telling you which configuration files were created and where, as well as the commands you must run to spin up your freshly-made API instance.\n\n```\n✔ API configuration file written to /Users/eduardoboucas/Sites/todo-api/config/config.development.json.\n✔ Database configuration file written to /Users/eduardoboucas/Sites/todo-api/config/filestore.development.json.\n✔ Created client with ID todoClient and type admin. The secret we generated for you is h5zfpj5gb7 – store it somewhere safe!\n\nAll done! Run the following command to launch your new instance of DADI API:\n\ncd todo-api && npm start\n```\n\nTo confirm that your API is up and running, fire a `GET` request to http://localhost:8081/hello. You should see a message saying `Welcome to API`.\n\n## Authentication\n\nDADI API uses 2-step oAuth to authenticate requests. In the previous step, we created a client ID/secret pair – in my case, these are `todoClient`/`h5zfpj5gb7`. You'll need to exchange your credentials for a bearer token, valid for a configurable period of time, which is what will identify you as a consumer on every request to API.\n\nTo get a bearer token, load [Postman](https://www.getpostman.com/) (or any other HTTP client) and fire a `POST` request to the `/token` endpoint in your API.\n\n```\nPOST http://localhost:8081/token\n\nAccept: application/json\nContent-Type: application/json\n\n{\n    \"clientId\": \"todoClient\",\n    \"secret: \"h5zfpj5gb7\n}\n```\n\nThe response will be a JSON payload containing the bearer token and the amount of time, in seconds, for which the token will be valid.\n\n```json\n{\n    \"accessToken\": \"2c483e66-74c9-4535-ac7b-26b4ca53ef53\",\n    \"tokenType\": \"Bearer\",\n    \"expiresIn\": 1800\n}\n```\n\nKeep this bearer token handy, we'll need it in a bit. Now, let's start shaping our data.\n\n## Data architecture\n\nData is organized in collections, which are declared as JSON schema files. This is where we'll define things like field types and validation rules.\n\nFor our to-do list app we'll need two collections. The main one is *items*, where we'll store all our to-do items. To take the TodoMVC concept a step further, let's also add a *users* collection, which we'll use to assign items to people.\n\nCollections live in `workspace/collections`. Because API has first-class support for endpoint versioning, you'll find a `1.0` directory created for you. If you were to release a new version of your endpoints, you'd create a new version (say `2.0`) whilst retaining the old, deprecated `1.0` until you decide to decomission it.\n\nUnder each version directory you'll find a subdirectory for each database, in case you decide to split collections into different databases.\n\nLet's create our users collection. Under `workspace/collections/1.0/todos`, create a `collection.users.json` file with the following: \n\n```json\n{\n  \"fields\": {\n    \"name\": {\n      \"type\": \"String\"\n    },\n    \"email\": {\n      \"type\": \"String\",\n      \"validation\": {\n        \"regex\": {\n          \"pattern\": \"^[A-Z0-9._%+-]+@[A-Z0-9.-]+\\\\.[A-Z]{2,}$\",\n          \"flags\": \"gi\"\n        }\n      }\n    }\n  },\n  \"settings\": {\n    \"description\": \"Represents people who have registered with the system\"\n  }\n}\n```\n\nLet's take a closer look at what's happening here. The `fields` block defines the fields and their types and constraints.\n\nWe're defining a `name` and `email` fields, both strings, with the latter having to meet a specific validation rule, using a regular expression that defines what represents a valid email address.\n\nWe can now move to the *items* collection. Create a `collection.items.json` in the same directory and paste the following:\n\n```json\n{\n  \"fields\": {\n    \"name\": {\n      \"type\": \"String\",\n      \"required\": true\n    },\n    \"assignee\": {\n      \"type\": \"Reference\",\n      \"settings\": {\n        \"collection\": \"users\"\n      }\n    },\n    \"completed\": {\n      \"type\": \"Boolean\",\n      \"default\": false\n    }\n  },\n  \"settings\": {\n    \"description\": \"Represents to-do items\"\n  }\n}\n```\n\nIf the collection schema for users made sense to you, this one shouldn't feel too dissimilar. The only particularity we see is a field of type `Reference`, which will link users to to-do items. You can look at this as a 1-N relationship in more traditional database terms.\n\n## Adding data\n\nTime to add some data! Let's start by creating a to-do item by firing a `POST` request to the *items* collection endpoint.\n\n```\nPOST http://localhost:8081/1.0/todos/items\n\nAccept: application/json\nAuthorization: Bearer 2c483e66-74c9-4535-ac7b-26b4ca53ef53\nContent-Type: application/json\n\n{\n  \"name\": \"Build a RESTful API with DADI API\",\n  \"completed\": false\n}\n```\n\nNote the `Authorization` header containing the bearer token we got earlier. The response will be a `results` array containing the documents inserted.\n\nA series of internal fields (prefixed with an underscore) will be added, like a unique document identifier and the creation date.\n\n```json\n{\n  \"results\": [\n    {\n      \"_apiVersion\": \"1.0\",\n      \"_createdAt\": 1513007727220,\n      \"_createdBy\": \"todoClient\",\n      \"_history\": [],\n      \"_id\": \"d353bfe1-7ef2-42ca-baf1-77cff1de13e8\",\n      \"_version\": 1,\n      \"completed\": false,\n      \"name\": \"Build a RESTful API with DADI API\"\n    }\n  ]\n}\n```\n\nYou can get a list of the existing to-do items with `GET http://localhost:8081/1.0/todos/items`, or query a particular item by ID like `GET http://localhost:8081/1.0/todos/items/d353bfe1-7ef2-42ca-baf1-77cff1de13e8`.\n\nLet's now create a user and assign our new to-do item to them. Similarly to the above, we fire a `POST` request to `1.0/todos/users`.\n\n```\nPOST http://localhost:8081/1.0/todos/users\n\nAccept: application/json\nAuthorization: Bearer 2c483e66-74c9-4535-ac7b-26b4ca53ef53\nContent-Type: application/json\n\n{\n  \"name\": \"John Doe\",\n  \"email\": \"john.doe@somedomain.tech\"\n}\n```\n\n```json\n{\n  \"results\": [\n    {\n      \"_apiVersion\": \"1.0\",\n      \"_createdAt\": 1513008476627,\n      \"_createdBy\": \"todoClient\",\n      \"_history\": [],\n      \"_id\": \"2fe27c95-dbff-4945-80a8-52fc188f1038\",\n      \"_version\": 1,\n      \"email\": \"john.doe@somedomain.tech\",\n      \"name\": \"John Doe\"\n    }\n  ]\n}\n```\n\nDocuments are referenced by their unique ID, which means that we must take the ID of the user we've just created and add it to the `assignee` property of the to-do item to be assigned.\n\nTo modify an existing document, we fire a `PUT` request to the collection endpoint including in the body the fields to be updated along with their new values.\n\n```\nPUT http://localhost:8081/1.0/todos/items/d353bfe1-7ef2-42ca-baf1-77cff1de13e8\n\nAccept: application/json\nAuthorization: Bearer 2c483e66-74c9-4535-ac7b-26b4ca53ef53\nContent-Type: application/json\n\n{\n  \"assignee\": \"2fe27c95-dbff-4945-80a8-52fc188f1038\"\n}\n```\n\nWe should get back something like:\n\n```json\n{\n  \"results\": [\n    {\n      \"_apiVersion\": \"1.0\",\n      \"_composed\": {\n          \"assignee\": \"2fe27c95-dbff-4945-80a8-52fc188f1038\"\n      },\n      \"_createdAt\": 1513007727220,\n      \"_createdBy\": \"todoClient\",\n      \"_history\": [\n        \"d353bfe1-7ef2-42ca-baf1-77cff1de13e8\"\n      ],\n      \"_id\": \"d353bfe1-7ef2-42ca-baf1-77cff1de13e8\",\n      \"_lastModifiedAt\": 1513008678684,\n      \"_lastModifiedBy\": \"todoClient\",\n      \"_version\": 2,\n      \"assignee\": {\n        \"_apiVersion\": \"1.0\",\n        \"_createdAt\": 1513008476627,\n        \"_createdBy\": \"todoClient\",\n        \"_history\": [],\n        \"_id\": \"2fe27c95-dbff-4945-80a8-52fc188f1038\",\n        \"_version\": 1,\n        \"email\": \"john.doe@somedomain.tech\",\n        \"name\": \"John Doe\"\n      },\n      \"completed\": false,\n      \"name\": \"Build a RESTful API with DADI API\"\n    }\n  ],\n  \"metadata\": {\n    \"page\": 1,\n    \"offset\": 0,\n    \"totalCount\": 1,\n    \"totalPages\": 1\n  }\n}\n```\n\nIf you now do a `GET http://localhost:8081/1.0/todos/items/d353bfe1-7ef2-42ca-baf1-77cff1de13e8` you'll notice that the `assignee` field will show the ID of the assigned user and not the resolved reference. This is the default behavior that can be changed by adding `?compose=true` to the request URL or by setting the `compose` key to `true` in the `settings` block of the collection schema.\n\n## Listing to-dos\n\nLet's add a few more to-dos. We can do a bulk insert by suppling an array in the `POST` request, instead of a single object.\n\n```\nPOST http://localhost:8081/1.0/todos/items\n\nAccept: application/json\nAuthorization: Bearer 2c483e66-74c9-4535-ac7b-26b4ca53ef53\nContent-Type: application/json\n\n[\n  {\n    \"name\": \"Learn about API authentication\",\n    \"completed\": true\n  },\n  {\n    \"name\": \"Learn about API collections\",\n    \"completed\": true\n  },\n  {\n    \"name\": \"Insert some data\",\n    \"completed\": true\n  }\n]\n```\n\nWhen querying a collection, you can specify a variety of filters to narrow down the result set to be exactly what you want. For example, if you wanted to get just the completed to-do items, you could do:\n\n```\nGET http://localhost:8081/1.0/todos/items?filter={\"completed\":false}\n```\n\nAnd to get all to-do items that contain the term `API` in the name:\n\n```\nGET http://localhost:8081/1.0/todos/items?filter={\"name\":{\"$regex\":\"API\"}}\n```\n\n## Wrapping up\n\nWe saw how to create a RESTful API and start inserting and querying data in just a few steps. Our [documentation pages](http://docs.dadi.tech/api) should be your next stop if you want to dive deeper into the product. If you'd rather skip the installation process and get your hands on the API ready to roll, you can get it from [this repository](https://github.com/eduardoboucas/todo-api).\n\nSo, you should now be ready to take the `Build a RESTful API with DADI API` to-do we created earlier and mark it as complete. I'll leave that one with you, I'm sure you'll nail it.","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"In this article, I'll show you how to build a simple application with DADI API.","published":true,"publishedAt":1513610880000,"slug":"building-a-to-do-list-app-with-dadi-api","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Building a to-do list app with DADI API","_refWeb-service":{},"web-service":["27d1f07c-a77c-4fea-af32-78680674c1f1"],"_createdBy":"api-client","_id":"ba4050a8-805c-473d-941a-e3f6be0864b1","meta":{"revision":0,"created":1530494395369,"version":0},"$loki":31},{"_apiVersion":"1.0","_createdAt":1530494396381,"_lastModifiedAt":1526418110057,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["2e0fd8d3-97e6-4398-b55a-3c8b41dd8214"],"body":"Today we are delighted to reveal an important new partnership with [Netwise](https://www.netwisehosting.co.uk/), a leading provider of server colocation and data centre services.\n\nNetwise offers private facilities in London and throughout Europe, designed and built entirely in-house, and delivers end-user content on a national and international scale. It is also a pioneer in green colocation solutions, offering highly-efficient rack space powered by 100% renewable energy — an issue of equal importance to the DADI team.\n\n> “Netwise has been supporting the DADI network since the early days of prototyping and testing, so it is a logical evolution for them to formally lend their expertise as advisors during our next phase of development.” — Joseph Denne, founder and CEO of DADI\n\nThe collaboration between Netwise and DADI will bring industry insight, consultation and support during the rollout of the DADI network in 2018. It will also provide spare capacity for the network as it grows, ensuring performance at scale.\n\n> “We are very happy to be supporting DADI as they go to market with their innovative new decentralised cloud platform. As avid supporters of bleeding-edge technologies, we are very much looking forward to helping DADI develop this system by supporting their growing core infrastructure requirements.” — Matt Seaton, Senior Manager at Netwise Hosting\n\n## More information on Netwise\n\nTo learn more about Netwise please visit: [netwisehosting.co.uk](https://www.netwisehosting.co.uk/).","category":["0eda8d2c-e76b-484e-a95b-6ea4b3b1eaa3"],"excerpt":"Today we are delighted to reveal an important new partnership with Netwise, a leading provider of server colocation and data centre services.","published":true,"publishedAt":1513867500000,"slug":"netwise-partners-with-dadi","sub-category":["f74856ec-dbdc-45ff-b096-9b078c4b3348"],"title":"Netwise partners with DADI","_createdBy":"api-client","_id":"b6535721-5dfa-4bd8-800a-41bf37bda34d","meta":{"revision":0,"created":1530494396383,"version":0},"$loki":32},{"_apiVersion":"1.0","_createdAt":1530494397401,"_lastModifiedAt":1526314832360,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["2647a07e-0b1f-45c0-8e90-0b55475d838b"],"body":"## Preface\n\nThere's a wealth of payment APIs on the market, commonly with support for the majority of programming languages and frameworks. DADI Web makes it particularly easy to integrate third-party services, and by leveraging the power of DADI Web's [Event System](https://docs.dadi.tech/web#events-1), I'll show you how your _webshop_, _booking system_ or _cat walking business_ can start taking payments today. \n\nFor this tutorial I've chosen to show you how to integrate the popular payment service [Stripe](https://stripe.com), however the logic should be easily transferable to a different service of your choosing. \n\n## Outcome\n\nBy the end of this tutorial, you'll have a fully functioning HTML payment form that accepts card payment details. The form will save a new customer, and their card details, to your Stripe account.\n\n## Requirements\n\nThis article assumes that:\n- You have DADI Web installed. If you don't, or you want a quick refresh, head over to the lovingly crafted [first install guide](https://forum.dadi.tech/topic/31/dadi-web-first-install) penned by our Design Director, David Longworth.\n- You have an active Stripe account. If you don't, [register here](https://dashboard.stripe.com/register).\n- You have your Stripe secret key ready. If you don't, read more about API Keys [here](https://stripe.com/docs/dashboard#api-keys).\n\n## Using in production\n\nWhilst this tutorial covers all of the app requirements for handling payments, there are some additional security requirements to consider when using in a production environment. \n\nYou should always use TLS and make sure you're performing a PCI compliance test annually. More information can be found in [Stripe's security documentation](https://stripe.com/docs/security).\n\n\n## Let's go!\n\nWe'll need a page where our users can pay for their service. It will contain a payment form with a button that, when submitted, sends the payment details to Stripe.\n\n### Creating the page\n\nAdd a file to `workspace/pages` to begin creating the payment page. For full documentation about pages in Web, see https://docs.dadi.tech/web#adding-pages.\n\n**workspace/pages/payment.json**\n\n```json\n{\n  \"page\": {\n    \"name\": \"Payment\",\n    \"description\": \"Honestly, it's for a good cause\",\n    \"language\": \"en\"\n  },\n  \"routes\": [\n    {\n      \"path\": \"/payment\"\n    }\n  ]\n}\n```\n\nThe above page specification tells Web that this page should be loaded for requests to `/payment`. If you like, read more information about [Routing in Web](https://docs.dadi.tech/web#routing-rewrites-and-redirects).\n\nNow that we have a page specification file, we need a template to display the HTML.\n\nI've chosen the Dust.js templating language, but feel free to choose a [different template engine](https://www.npmjs.com/search?q=keywords:dadi-web-engine&page=1&ranking=optimal) (or build your own!).\n\nCreate a new Dust.js file in `workspace/pages` which contains a form with input fields to capture the card details, and a submit button to post the form.\n\n**workspace/pages/payment.dust**\n\n```html\n<form action=\"\" method=\"POST\">\n  <input name=\"email\" type=\"email\" placeholder=\"Email\" required />\n  <input name=\"number\" placeholder=\"Card Number\" type=\"number\" />\n  <input name=\"cvc\" placeholder=\"CVC\" type=\"number\" />\n  <select name=\"exp_month\">\n    <option disabled selected>Expiry Month</option>\n    {! Months !}\n  </select>\n  <select name=\"exp_year\">\n    <option disabled selected>Expiry Year</option>\n    {! Years !}\n  </select>\n  <h3>Amount</h3>\n  <span>£<input name=\"amount\" type=\"number\" placeholder=\"Amount\" required value=\"5.00\" /></span>\n  <input type=\"submit\" value=\"Pay me!\" />\n</form>\n```\n\nYou'll see that the date inputs aren't populated with options. We _could_ do this manually, but this is a good opportunity to use Web's Preload Events to generate data for us.\n\n### Card dates event\n\nThe payment form we're building asks the customer to enter the _valid from_ month and year, and to make each respective dropdown menu readable and dynamic in our page template, we're going to create a Preload Event to generate the next 30 years worth of possible months and years.\n\nAdd a file to `workspace/events` called `cardDates.js`:\n\n**workspace/events/cardDates.js**\n\n```js\n/**\n * Generate an array of years and an array of months for\n * use in the frontend form\n */\nconst Event = function (req, res, data, callback) {\n  const MONTHS_TO_GENERATE = 12\n  const YEARS_TO_GENERATE = 30\n  const currentYear = new Date().getFullYear()\n  let years = []\n  let months = []\n\n  // produces an array similar to the following\n  // [ '2017', '2018', '2019', '2020', '2021', '2022'... ]\n  for (let year = 0; year <= YEARS_TO_GENERATE; year++) {\n    years.push((currentYear + year).toString())\n  }\n\n  // produces the following array \n  // [ '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12' ]\n  for (let month = 1; month <= MONTHS_TO_GENERATE; month++) {\n    months.push(month < 10 ? `0${month}` : month.toString())\n  }\n  \n  data.years = years\n  data.months = months\n\n  callback(null)\n}\n\nmodule.exports = function (req, res, data, callback) {\n  return new Event(req, res, data, callback)\n}\n\n```\n\nTo tell Web about this event, edit the page specification file to add a \"preloadEvents\" section:\n\n```json\n{\n  \"page\": {\n    \"name\": \"Payment\",\n    \"description\": \"Honestly, it's for a good cause\",\n    \"language\": \"en\"\n  },\n  \"routes\": [\n    {\n      \"path\": \"/payment\"\n    }\n  ],\n  \"preloadEvents\": [\n    \"cardDates\"\n  ]\n}\n```\n\nNow when we load the page it will contain an array of months and years in the data context, so we can iterate over these preloaded values in the form:\n\n```html\n<select name=\"exp_month\">\n  <option disabled selected>Expiry Month</option>\n  {#months}\n    <option value=\"{.}\">{.}</option>\n  {/months}\n</select>\n<select name=\"exp_year\">\n  <option disabled selected>Expiry Year</option>\n  {#years}\n    <option value=\"{.}\">{.}</option>\n  {/years}\n</select>\n```\n\n### Adding feedback to the page\n\nFinally, let's add some error output to the page template so we can let the customer know when something's gone wrong:\n\n```html\n{?err}\n  <mark>{err.message}</mark>\n{/err}\n```\n\n### Check your progress\n\nIf you start the app with `npm start` and visit the `/payment` route in your browser, you should the form rendered like this:\n\n![Screenshot of the credit card fields](/media/2018/05/10/1513869076902screen-shot-on-2017-12-20-at-14-3a52-3a54.png)\n\n### Interacting with Stripe\n\nIn order to actually send the payment details to Stripe, we need to have a way to process the form that the customer submits. We'll do this using an event attached the the payment page that is run when the form is submitted.\n\n#### Creating the Event\n\nFirst let's install the Stripe dependency we'll be using in our payment event. Inside your Web application directory, run the following:\n\n```shell\nnpm install stripe --save\n```\n\nEvent files are placed in the directory `workspace/events` (unless you've changed the default configuration). You've already added the `cardDates` event here, so this is nothing new. Create a new file called `payments.js`:\n\n**workspace/events/payments.js**\n```js\n// require the Stripe dependency and pass it our API key\nconst stripe = require('stripe')('sk_test_BQokikJOvBiI2HlWgH4olfQ2')\n\nconst Event = function (req, res, data, callback) {\n  callback()\n}\n\nmodule.exports = function (req, res, data, callback) {\n  return new Event(req, res, data, callback)\n}\n\nmodule.exports.Event = Event\n```\n\nThis is really as basic as events get, but it doesn't do anything yet. Stripe requires that a customer is created with at least one card _before_ any payments can be made (see the [Stripe documentation](https://stripe.com/docs/charges)). In the event we'll create two extra methods to handle this, as well as a method to actually charge the customer's card. Place the following code at the end of the `payments.js` file.\n\n> **Note:** the Stripe package allows you to use either regular callbacks or Promises, in this post we'll be using Promises \n\n**workspace/events/payments.js**\n```js\n/**\n * Create a customer record using the authenticated Stripe account\n * https://stripe.com/docs/api/node#create_customer\n * \n * @param  {String} email - the email to register to the customer record.\n * @return {Promise} Stripe API action\n */\nfunction createCustomer (email) {\n  return stripe.customers.create({\n    email: email\n  })\n}\n\n/**\n * Create a customer card object\n * https://stripe.com/docs/api/node#create_card \n * \n * @param  {String} cust_id - the Stripe customer ID\n * @param  {Number} exp_month - the payment card expiry month\n * @param  {Number} exp_year - the payment card expiry year\n * @param  {Number} number - the 3-4 digit Card Verification Code\n * @param  {String} object - the source creation type\n * @return {Promise} Stripe API action          \n */\nfunction createCustomerCard (cust_id, exp_month, exp_year, number, cvc, object) {\n  return stripe.customers.createSource(cust_id, {\n    source: {\n      object: object || 'card',\n      exp_month: exp_month,\n      exp_year: exp_year,\n      number: number,\n      cvc: cvc\n    }\n  })\n}\n\n/**\n * Charge a customer's card\n * https://stripe.com/docs/api/node#create_charge\n * \n * @param  {String} customer - the Stripe customer ID\n * @param  {String} card - the ID of the customer's card object\n * @param  {String} amount - the amount to charge to the card\n * @param  {String} currency - the currency to charge the card in\n * @return {Promise} Stripe API action          \n */\nfunction chargeCustomerCard (customer, card, amount, currency) {\n  return stripe.charges.create({\n    customer: customer,\n    card: card,\n    amount: Number(amount.replace('.', '')),\n    currency: currency || 'gbp'\n  })\n}\n```\n\n#### Event form action\n\nThe event still doesn't do anything, but now we have the basics we can hook it up to the page so that it is executed when the form is submitted.\n\nAdding the event to the page by specifying it in an \"events\" block in the page specification file:\n\n```json\n\"events\": [\n  \"payments\"\n]\n```\n\nIt should now look like this:\n\n**workspace/pages/payment.json**\n```json\n{\n  \"page\": {\n    \"name\": \"Payment\",\n    \"description\": \"Honestly, it's for a good cause\",\n    \"language\": \"en\"\n  },\n  \"routes\": [\n    {\n      \"path\": \"/payment\"\n    }\n  ],\n  \"preloadEvents\": [\n    \"cardDates\"\n  ],\n  \"events\": [\n    \"payments\"\n  ]\n}\n```\n\nNow that the event is hooked up to the page, it will fire whenever the page is loaded. Because events will fire on both POST and GET requests, we add a check to the beginning of the event to ensure it is a POST operation and then use the contents of the `req.body` property to call the functions we created earlier.\n\nThe full event code should look like this:\n\n**workspace/events/payments.js**\n```js\nconst stripe = require('stripe')('sk_test_BQokikJOvBiI2HlWgH4olfQ2')\n\nconst Event = function (req, res, data, callback) {\n  // not a POST, no need to run the rest of the event\n  if (req.method && req.method.toLowerCase() !== 'post') return callback()\n\n  // get the email, card details and charge amount from the request body\n  const requestData = req.body\n\n  createCustomer(requestData.email).then(customer => {\n    return createCustomerCard(\n      customer.id,\n      requestData.exp_month,\n      requestData.exp_year,\n      requestData.number,\n      requestData.cvc\n    ).then(card => {\n      return chargeCustomerCard(customer.id, card.id, requestData.amount)\n    }).then(charge => {\n      // Success! lets set a value we can display to the user\n      // Remember that `data` is the context that is passed to the page template\n      // for rendering\n      data.success = {\n        message: charge.outcome.seller_message\n      }\n\n      callback(null)\n    }).catch(err => {\n      data.err = err\n      callback(null)\n    })\n  })\n}\n\nmodule.exports = function (req, res, data, callback) {\n    return new Event(req, res, data, callback)\n}\n\n/**\n * Create a customer record using the authenticated Stripe account\n * https://stripe.com/docs/api/node#create_customer\n * \n * @param  {String} email - the email to register to the customer record.\n * @return {Promise} Stripe API action\n */\nfunction createCustomer (email) {\n  return stripe.customers.create({\n    email: email\n  })\n}\n\n/**\n * Create a customer card object\n * https://stripe.com/docs/api/node#create_card \n * \n * @param  {String} cust_id - the Stripe customer ID\n * @param  {Number} exp_month - the payment card expiry month\n * @param  {Number} exp_year - the payment card expiry year\n * @param  {Number} number - the 3-4 digit Card Verification Code\n * @param  {String} object - the source creation type\n * @return {Promise} Stripe API action          \n */\nfunction createCustomerCard (cust_id, exp_month, exp_year, number, cvc, object) {\n  return stripe.customers.createSource(cust_id, {\n    source: {\n      object: object || 'card',\n      exp_month: exp_month,\n      exp_year: exp_year,\n      number: number,\n      cvc: cvc\n    }\n  })\n}\n\n/**\n * Charge a customer's card\n * https://stripe.com/docs/api/node#create_charge\n * \n * @param  {String} customer - the Stripe customer ID\n * @param  {String} card - the ID of the customer's card object\n * @param  {String} amount - the amount to charge to the card\n * @param  {String} currency - the currency to charge the card in\n * @return {Promise} Stripe API action          \n */\nfunction chargeCustomerCard (customer, card, amount, currency) {\n  return stripe.charges.create({\n    customer: customer,\n    card: card,\n    amount: Number(amount.replace('.', '')),\n    currency: currency || 'gbp'\n  })\n}\n\nmodule.exports.Event = Event\n```\n\nA couple of things to mention here. Firstly, I've added a default currency, but this could be changed to your preferred currency, determined by some clever geolocation or selected by the user from the HTML form. \n\nSecondly, Stripe requires 'amount' to be represented as the lowest common denominator of the selected currency, so **£30.00** would be **3000**. You'll notice that we're removing the decimal place from 'amount' for this reason.\n\n### Add payment feedback\n\nWe should really let the customer know how it went. As we're only setting the `success` property in the data context if the charge was successful, we can use this property to decide what to render in the template. Change your template to look like the following. You can see that if the `success` property exists, then we only show the message, otherwise we show an error message and redisplay the form:\n\n**workspace/pages/payment.dust**\n```html\n<main>\n  <section>\n    {?success}\n      <h2>{success.message|s}</h2>\n    {:else}\n      {?err}\n        <mark>{err.message}</mark>\n      {/err}\n\n      <h3>Payment</h3>\n      <form action=\"\" method=\"POST\">\n        <input name=\"email\" type=\"email\" placeholder=\"Email\" required />\n        <input name=\"number\" placeholder=\"Card Number\" type=\"number\" />\n        <input name=\"cvc\" placeholder=\"CVC\" type=\"number\" />\n        <select name=\"exp_month\">\n          <option disabled selected>Expiry Month</option>\n          {#months}\n            <option value=\"{.}\">{.}</option>\n          {/months}\n        </select>\n        <select name=\"exp_year\">\n          <option disabled selected>Expiry Year</option>\n          {#years}\n            <option value=\"{.}\">{.}</option>\n          {/years}\n        </select>\n        <h3>Amount</h3>\n        <span>£<input name=\"amount\" type=\"number\" placeholder=\"Amount\" required value=\"5.00\" /></span>\n        <input type=\"submit\" value=\"Pay me!\" />\n      </form>\n    {/success}\n  </section>\n</main>\n```\n\n\n\n### Test\n\nLet's test it! In order to test, you'll need some test card details:\n\n * Card number: **4242424242424242**\n * CVC: any 3 digit CVC\n * Expiry: any future expiry date\n\n\nIf your payment is successful, you'll see something like this: \n\n![The payment complete view](/media/2018/05/14/1513868998883screen-shot-on-2017-12-20-at-18-3a42-3a59.png)\n\n## And that's a wrap!\n\nIf you want to take your cart to the next steps, let me know and I'll happily write more. Some ideas:\n- Store user data in [session](https://docs.dadi.tech/web#sessions).\n- Retrieve and reuse existing cards.\n- Subscribe users to a payment plan.\n\nYou can ask me here on the forum, on [Discord](https://discordapp.com/invite/3sEvuYJ) or tweet [@dadi](https://twitter.com/dadi?lang=en).","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"There's a wealth of payment APIs on the market, commonly with support for the majority of programming languages and frameworks. ","published":true,"publishedAt":1513886220000,"slug":"integrating-checkout-for-dadi-web","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Integrating checkout for DADI Web","_refWeb-service":{},"web-service":["f0d957ef-7cc5-4c71-bbb6-978c1cb43951"],"_createdBy":"api-client","_id":"99694b74-6600-43d3-ab7e-e5c8d53788a4","meta":{"revision":0,"created":1530494397403,"version":0},"$loki":33},{"_apiVersion":"1.0","_createdAt":1530494398417,"_lastModifiedAt":1526417894206,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["6d08fa42-f098-4f94-b6a5-24d04ea09500"],"body":"## Overview\n\nFor the first two major versions of API, MongoDB was the only supported database engine. With the release of 3.0, we wanted to offer developers the flexibility to use the database engine that most suited their projects, regardless of size and scalability needs.\n\nIn line with the introduction of support for multiple template engines in [Web 3.0](https://github.com/dadi/web/releases/tag/v3.0.0), we moved all interactions with MongoDB from the app core into a plugin system – which we call *data connectors* – which allows API to work with virtually any database engine.\n\nWe've been hard at work creating data connectors for various vendors, like [CouchDB](https://github.com/dadi/api-couchdb), [RethinkDB](https://github.com/dadi/api-rethinkdb) and even a flat file [JSON filestore](https://github.com/dadi/api-filestore). In this tutorial, we're going to show you how to build your own.\n\nBy the end of this tutorial we'll have a partially-complete API Data Connector for MySQL, with a subset of the database operations implemented. Everything else can be finished for homework!\n\n\n## A Data Connector Template\n\nDuring the development of API 3.0, the DADI engineering team put together a sample repository to make it easier to build your own connector. It even contains a test suite which should help you craft a robust connector for your chosen database engine.\n\nThe repository can be found here: https://github.com/dadi/api-connector-template. You can fork this repository and follow along.\n\n## Beginning the Data Connector\n\n1. Fork the [dadi/api-connector-template](https://github.com/dadi/api-connector-template#fork-destination-box) repository into your own profile or organization.\n1. Rename it to something nicer – let's call it `api-mysql`.\n1. Clone the repository to your local machine.\n    ```bash\n    $ git clone https://github.com/johndoe/api-mysql.git\n    Cloning into 'api-mysql'...\n    ```\n1. Change into the repository directory and install the dependencies.\n    ```bash\n    $ cd api-mysql/\n    $ npm install\n    ```\n\n1. Now install any dependencies needed to interact with your chosen database engine. For our MySQL connector, we'll need the [Node.js MySQL driver](https://www.npmjs.com/package/mysql2).\n    ```bash\n    $ npm install mysql2 --save\n    ```\n1. To get a checklist of the things your connector must be able to do, run the test suite.\n    ```bash\n    $ npm test\n    ```\n\n1. Create a MySQL database if one doesn't already exist:\n\n    ```\n    CREATE DATABASE 'my_database';\n    ```\n\n1. Create a MySQL user if one doesn't already exist:\n\n    ```sql\n    CREATE USER 'mysqluser'@'localhost' IDENTIFIED BY 'password';\n    GRANT ALL PRIVILEGES ON *.* TO 'mysqluser'@'localhost';\n    ```\n    \n## Configuration file\n\nEach data connector normally requires its own configuration file containing things such as database host, database name and connection credentials. The configuration files follow the naming convention `<connector>.<environment>.json`. For example the MongoDB data connector uses a file called `mongodb.development.json` when running in development mode. These configuration files are placed in the `config` directory of your connector module and also in the `config` directory of your API application.\n\nFor the MySQL connector we'll need a file called `mysql.development.json`. Because we're starting with the unit tests, let's create the test mode configuration first. \n1. Rename `config/apiConnector.test.json` to `config/mysql.test.json`\n2. Add the following configuration, changing values to match your setup:\n\n    ```json\n    {\n      \"database\": {\n        \"host\": \"localhost\",\n        \"port\": 3306,\n        \"database\": \"my_database\",\n        \"user\": \"mysqluser\",\n        \"password\": \"password\"\n      }\n    }\n    ```\n\n3. Edit `config.js` so the line that loads the configuration file is correct:\n\n    ```javascript\n    // conf.loadFile('./config/apiConnector.' + env + '.json')\n    conf.loadFile('./config/mysql.' + env + '.json')\n    ```\n\n\n## Database operations\n\nThis tutorial will only focus on the bare minimum required for a data connector and as such will only implement three database operations: [connect](https://github.com/dadi/api-connector-template#connectdatabase-collection), [insert](insertdata-collection-options---schema-settings--) and [find](https://github.com/dadi/api-connector-template#find-query-collection-options---schema-settings-).\n\nTo keep it simple, we'll do all our work in the existing file `lib/index.js`.\n\nBefore adding code for the database operations, require the mysql2 dependency at the top of this file:\n\n**lib/index.js**\n```js=\nconst config = require('../config')\nconst debug = require('debug')('api:mysql')\nconst EventEmitter = require('events').EventEmitter\nconst util = require('util')\nconst uuid = require('uuid')\n\n// require MySQL module\nconst mysql = require('mysql2')\n```\n\n### connect\n\nWhen API calls the connector's `connect()` method it expects the connector to create a connection to the database and assign it to the property `this.database`, before resolving the promise. \n\n\nTo connect to MySQL we call `createConnection` and supply the connection options from the configuration file:\n\n```js=\n/**\n * Connect to the database\n *\n * @param {ConnectionOptions} options\n */\nDataStore.prototype.connect = function (options) {\n  debug('connect %o', options)\n\n  return new Promise((resolve, reject) => {\n    // read configuration options from config/mysql.<environment>.json\n    const dbConfig = config.get('database')\n\n    // connect!\n    this.database = mysql.createConnection(dbConfig)\n\n    // everything is ok, emit 'DB_CONNECTED' event\n    this.readyState = STATE_CONNECTED\n    this.emit('DB_CONNECTED', this.database)\n\n    // problem connecting? emit 'DB_ERROR' event\n    // this.emit('DB_ERROR', err)\n    // return reject(err)\n\n    return resolve()\n  })\n}\n\n```\n\n\n### insert\n\nAll API data connectors should support inserting an array of documents. For this simple version of the MySQL connector we're only going to handle inserting single documents. \n\n\nIn the sample connector repository, the `insert()` method returns an empty array:\n\n```js=\nreturn new Promise((resolve, reject) => {\n  let results = []\n  return resolve(results)\n})\n```\n\nReplace the code above with the following:\n\n```javascript=\nreturn new Promise((resolve, reject) => {\n  const insertQuery = `INSERT INTO ${collection} SET ?`\n  const findQuery = `SELECT * FROM ${collection} WHERE _id = ?`\n\n  return this.database.query(insertQuery, data[0], (err, results, fields) => {\n    if (err) {\n      return reject(err)\n    }\n\n    // query the database for the new document(s)\n    return this.database.query(findQuery, data[0]._id, (err, results, fields) => {\n      return resolve(results)\n    })\n  })\n})\n```\n\nTwo things you may notice in our new `insert()` method. The first is that we don't have to build a full INSERT statement to pass to the database. The Node.js MySQL module that we're using handles converting objects passed to the query method into SQL statements, saving us quite a lot of manual transformation. The following snippet is from the [documentation for the `mysql` module](https://github.com/mysqljs/mysql#escaping-query-values), which the `mysql2` module is based off.\n\n```javascript\nconst post  = {id: 1, title: 'Hello MySQL'}\n\nlet query = connection.query('INSERT INTO posts SET ?', post, (error, results, fields) => {\n  if (error) throw error\n  \n})\n\nconsole.log(query.sql) // INSERT INTO posts SET `id` = 1, `title` = 'Hello MySQL'\n```\n\nThe second is that we're making a second call to the database to get the results of the insert. This is because API expects a result set back from `insert()` that contains the inserted documents, and MySQL only returns the number of affected rows.\n\nBefore we can run any tests for the `insert()` method, we have to remove the \"skip\" instruction in the test file. In `test/index.js`, change the following:\n\n```js\n// describe.skip('insert', function () {\ndescribe('insert', function () {\n```\n\nIf you run `npm test` now the first thing you should notice (if the database has connected correctly) is an error saying that a table doesn't exist. \n> Uncaught Error: Table 'my_database.users' doesn't exist\n\n### Automatic table creation\n\nWe really need data connectors to automatically create tables that don't exist, so that new collections can be added to API without having to perform any maintenance on the underlying database.\n\nLet's add a `createTable()` method that can be called from each of the database operations to ensure the requested table exists. This method needs to read the schema for the requested collection and generate a CREATE TABLE query that includes the names of the columns and their data types. See the DADI API documentation for [Collections](https://docs.dadi.tech/api#the-json-file) for detailed information.\n\n```javascript=\n/**\n * Create a new MySQL table if the specified one doesn't exist\n *\n * Queries the MySQL information_schema database, returning a count of\n * tables that exist for the specified database + table_name parameters\n * \n * If the table doesn't exist, build a CREATE TABLE statement to execute against the database\n *\n * @param {String} name - the name of the table to check or create\n * @param {Object} schema - the API collection schema fields\n * @returns {Promise.<undefined, Error>} A promise that returns an Array of results,\n *     or an Error if the operation fails\n */\nDataStore.prototype.createTable = function (table, schema) {\n  return new Promise((resolve, reject) => {\n    // does the table exist?\n    let tableQuery = `SELECT COUNT(*)\n        FROM information_schema.TABLES\n        WHERE TABLE_SCHEMA = '${config.get('database.database')}' AND TABLE_NAME = '${table}'`\n\n    return this.database.query(tableQuery, (err, result) => {\n      if (result[0]['COUNT(*)'] === 0) {\n        let createTableQuery = 'CREATE TABLE ' + table + ' ('\n\n        // add an _id column as a default\n        createTableQuery += '_id VARCHAR(100),'\n\n        // add columns based on the collection schema\n        Object.keys(schema).forEach(key => {\n          createTableQuery += `${key} ${schema[key].type === 'String' ? 'VARCHAR(255)' : 'INT'},`\n        })\n\n        createTableQuery += 'PRIMARY KEY(_id))'\n\n        this.database.query(createTableQuery, (err, result) => {\n          return resolve()\n        })\n      }\n\n      // the table already exists, return\n      return resolve()\n    })\n  })\n}\n```\n\nNow modify the `insert()` method to include a call to `createTable()`:\n\n```javascript=\n  return new Promise((resolve, reject) => {\n    const insertQuery = `INSERT INTO ${collection} SET ?`\n    const findQuery = `SELECT * FROM ${collection} WHERE _id = ?`\n\n    // first check the table exists\n    return this.createTable(collection, schema).then(() => {\n      return this.database.query(insertQuery, data[0], (err, results, fields) => {\n        if (err) {\n          return reject(err)\n        }\n\n        // query the database for the new document(s)\n        return this.database.query(findQuery, data[0]._id, (err, results, fields) => {\n          return resolve(results)\n        })\n      })\n    })\n  })\n```\n\n\n#### Test again!\n\nRunning `npm test` now should see some of our `insert` tests passing:\n\n```bash\ninsert\n  ✓ should insert a single document into the database\n  - should insert an array of documents into the database\n  ✓ should add _id property if one isn't specified\n  ✓ should use specified _id property if one is specified\n```\n\n### find\n\nHaving already implemented a find within the `insert()` method, it should be simple to make our connector's `find()` method work. Add the following to `lib/index.js` in the existing `find()` method:\n\n```javascript=\nconst findQuery = `SELECT * FROM ${collection} WHERE _id = ?`\n\n// query the database\nreturn this.database.query(findQuery, query, (err, results, fields) => {\n  return resolve(results)\n})\n```\n\nBefore we can run any tests for the `find()` method, we have to remove the \"skip\" instruction in the test file. In `test/index.js`, change the following:\n\n```js\n// describe.skip('find', function () {\ndescribe('find', function () {\n```\n\nWhen you run `npm test` the first `find` test should fail because it's expecting the database to contain only a couple of records. We should really be clearing the database before each test run. For this we'll use the convenient `dropDatabase()` method in the sample connector. \n\nFind the `beforeEach()` method in `test/index.js` and modify it as follows:\n\n```javascript=\nbeforeEach(function (done) {\n  done()\n})\n```\n\n```javascript=\nbeforeEach(function (done) {\n  var apiConnector = new ApiConnector()\n\n  // clear the 'users' table\n  apiConnector.connect({ database: 'content', collection: 'users' }).then(() => {\n    apiConnector.dropDatabase('users').then(() => {\n      done()\n    })\n  })\n})\n```\n\nFind the `dropDatabase()` method in `lib/index.js` and modify it as follows:\n\n```javascript=\nDataStore.prototype.dropDatabase = function (collectionName) {\n  if (this.readyState !== STATE_CONNECTED) {\n    return Promise.reject(new Error('DB_DISCONNECTED'))\n  }\n\n  debug('dropDatabase %s', collectionName || '')\n\n  return new Promise((resolve, reject) => {\n    const deleteQuery = `DELETE FROM ${collectionName}`\n\n    // delete all\n    return this.database.query(deleteQuery, (err, results, fields) => {\n      return resolve()\n    })\n  })\n}\n```\n\nIf we run the tests now, we should have one passing test (and six pending):\n\n```bash\n    find\n      ✓ should find a single document in the database\n      - should return the number of records requested when using `limit`\n      - should sort records in ascending order by the `createdAt` property when no query or sort are provided\n      - should sort records in ascending order by the query property when no sort is provided\n      - should sort records in ascending order by the specified property\n      - should sort records in descending order by the specified property\n      - should return only the fields specified by the `fields` property\n```\n\n## Tasks to complete\n\nBefore this connector can be finished and used with API, there are a number of problems to resolve. Obviously we've left the `update()` and `delete()` methods for you to implement, but the following information about \"options\" also needs to be considered.\n\n### Passing options to `find()`\n\nAPI calls the `find()` method with an `options` object which contains values that instruct the connector how to do things such as sorting and limiting the records returned. See the [API documentation](https://docs.dadi.tech/api) and the [JSON Filestore](https://github.com/dadi/api-filestore) connector for ideas on implementing.\n\nWe've left these unit tests pending in the sample connector repository. Simply remove the `.skip` for each of these when you're ready to test your implementation.\n\n### Returning results & metadata\n\nWhile we haven't added this to the sample connector, API actually expects a result from the data connector's `find()` method that contains both the result set _and_ a \"metadata\" block that contains the total count of matching records and the number of pages. You can see [how the MongoDB connector does it](https://github.com/dadi/api-mongodb/blob/master/lib/index.js#L168). This uses the NPM package [@dadi/metadata](https://www.npmjs.com/package/@dadi/metadata) which you can add to your project's dependencies.\n\n\n## Publishing the connector to NPM\n\nEdit the package.json. We need to rename the package (it's still called **@dadi/api-connector-template**). Change the name property to be **api-mysql**:\n\n```json\n{\n  \"name\": \"api-mysql\"\n}\n```\n\nYou should also change the `repository` property to reflect your own GitHub repository:\n\n```json\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"https://github.com/johndoe/api-mysql.git\"\n  }\n```\n\nWhen you're ready to publish the connector, execute the following command:\n\n```bash\n$ npm publish --access public\n```\n\n## Testing with API\n\nSo far we've been testing the connector in isolation. Once all the tests are passing, it's a good idea to use `npm link` to test the connector with your API application. This can be done before you publish to NPM, to ensure everything is working before you make it publicly available. See [this excellent article](https://medium.com/trisfera/the-magic-behind-npm-link-d94dcb3a81af) regarding the use of `npm link`.\n\n\n## Usage with DADI CLI\n\nThe easiest way to install API is with [DADI CLI](https://docs.dadi.tech/cli), a command-line tool that allows developers to spin up an API instance in just a few seconds using a single command.\n\nNow that your data connector is available on NPM, it can be pulled into CLI for new API installations. The following command will install the latest version of API in a directory called `my-new-api` using our fresh MySQL data connector, taking care of installing all the dependencies.\n\n```bash\n$ dadi api new my-new-api --database=api-mysql\n```\n\nWhen users don't supply a `--database` parameter, CLI will present them with a list of data connector modules straight from NPM.\n\n```bash\n$ dadi api new todo-api\n✔ Checking the available versions of DADI API\n✔ Pulling the list of available database connectors from NPM\n? Which database engine would you like to install? (Use arrow keys)\n❯ @dadi/api-mongodb — A MongoDB adapter for DADI API \n  @dadi/api-filestore — A JSON datastore adapter for DADI API\n```\n\nTo ensure an optimal and safe experience, we filter this list with modules that are built or trusted by DADI. If you think your data connector should be on this list, [get in touch](https://dadi.tech/en/contact/) – we'll be delighted to review your work and, if necessary, help you smooth any rough edges before adding it to the list.\n\n\n\n## That's it for today!\n\nThe code for this article, forked from the sample data connector, can be found [here](https://github.com/jimlambie/api-mysql).\n\nFor more information about the API Data Connectors, or help building your own, ask us in the usual channels. You can connect with the engineering team on [Discord](https://discordapp.com/invite/3sEvuYJ), via email (email addresses in our profiles) or send a tweet to [@dadi](https://twitter.com/dadi).","category":["3bf24914-17bd-4116-9412-b7b76fc8f0f6"],"excerpt":"For the first two major versions of API, MongoDB was the only supported database engine. With the release of 3.0, we wanted to offer developers the flexibility to use other database engines.","published":true,"publishedAt":1514302260000,"slug":"building-a-data-connector-for-dadi-api","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Building a Data Connector for DADI API","_refWeb-service":{},"web-service":["27d1f07c-a77c-4fea-af32-78680674c1f1"],"_createdBy":"api-client","_id":"08a88655-e4fa-4b70-b5c6-9f98a62bfda3","meta":{"revision":0,"created":1530494398418,"version":0},"$loki":34},{"_apiVersion":"1.0","_createdAt":1530494399431,"_lastModifiedAt":1526418027394,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["2902224e-daee-4bea-bf64-3ca15f1333c4"],"body":"## Why use a task queue, such as DADI Queue?\n\nChaos, uncertainty and disorder. These are the building blocks of our world, out of which we evolved, got smart and became capable of organizing the complex network of countless variables around us.\n\nAs civilization continued to emerge, we built dens and dwellings and then pyramids and pantheons. Our projects became increasingly complex and we needed other ways of managing the chaos.\n\nSo we planned. We wrote task lists and then delegated those jobs to our minions.\n\nAnd this, if you'll excuse the rhetoric, is an example of the asynchronous task queue in action.\n\n## Okay, I'm sold, lay on the details\n\nSo having established that a task queue is, ahem, the cornerstone of human civilisation, let's talk about DADI Queue and what it can do for your app or service.\n\nDADI Queue is one of the microservices from our open-source box of [web service](https://dadi.tech) tricks. It's currently available from [the repo on Github](https://github.com/dadi/queue) and is soon to be available via our decentralized [cloud services platform](https://dadi.cloud).\n\nOnce installed, following [the instructions](https://github.com/dadi/queue/blob/master/readme.md) from the repo, you'll be the proud admin of a lightweight, high-performance task queue running on Node.js and Redis.\n\nIntegrating DADI Queue into your app allows you to queue up work asynchronously so that it can be done outside the context of the initiating user request.\n\nFor example, let's say your social networking app sends 'new message' emails to all your friends when you post a message. This could be handled in one of these three ways.\n\n1. Synchronously, while-u-wait\n2. Asynchronously, now, using a backgrounded client-process like AJAX\n3. Asynchronously, later, by storing the details and running a cron job\n\nIf you choose the first option your users had better get a brew on ☕️. The other two options are feasible, but neither are inherently scalable or failure tolerant and both of these measures of quality are essential in cultivating positive user sentiment.\n\nThe ideal response to the above scenario is, of course, to use a task queue.\n\n## So how does it all work?\n\nDADI Queue consists of three parts: the queue, the broker and the workers.\n\nIn the scenario above, posting a message would add a task to the queue via [the API](https://github.com/dadi/queue-wrapper). A task is usually just a text string with relevant data, e.g. `email:new-message:12322:432`. In this case the numbers are user and message IDs.\n\nThe broker is like the middleman between queue and workers. It polls the queue for new tasks and then routes them to a matching worker based on the content of the task string (a bit like routing to an endpoint in a REST API).\n\nThe workers are JavaScript modules that receive tasks from the broker. This is where you come in. These modules are the parts you need to code to do work in response to a task.\n\n## And what's the payback?\n\nFair question. Since you ask, here's a summary:\n\n### Fault-tolerant\n\nTasks are 'leased' to a worker and have a deadline by which the worker must finish. If the worker fails (e.g. sending an email and the SMTP service is down) the broker will put the task back on the queue for a specified number of retries, thus providing fault-tolerance. On the last retry the worker can perform a final remedial action.\n\n### Scalable\n\nThe queue part of the system can run on a separate server to the workers, thus providing scalability. Throughput can be monitored and extra worker instances added as necessary. It is also possible to schedule specified tasks to be processed at times when system resources are optimal. Conversely, the processing of tasks can be throttled if required.\n\n### Decoupled\n\nWorkers can be grouped in folders according to the task domain (e.g. image processing or push notifications) and thus each folder forms a neat abstraction of the functions of that domain. Workers can also chain each other or create new tasks, thus removing complexity from the calling code.\n\n### A killer example, please\n\nNaturally. Consider an online shop.\n\nThe checkout process may interact with a number of external APIs when a customer places an order: CRM system, payment gateway, fraud prevention, email provider, etc. \n\nPerforming these interactions synchronously makes the checkout process slow, tightly coupled to the external APIs and error prone, due to the number of failure points.\n\nUsing a task queue, each API interaction can become a worker module. On order confirmation the checkout process simply saves the order data, sends the relevant tasks to the queue, e.g. `create-user:4324`, `create-transaction:14388`, etc., then shows the confirmation page without waiting for those tasks to complete.\n\nThe queue will then asynchronously process the tasks, retrying any on error. In this example, the workers would likely be chained to send a final notification on success/failure.\n\nThe result is a fast user experience, API code decoupled from the checkout, fault-tolerance, plus the ability to scale up on Black Friday. Spot on!\n\n## Wrapping up\n\nHopefully I've shown that using a task queue can significantly increase the resilience and availability of your app or service.\n\nIt's worth examining your use case carefully to avoid over-engineering or prematurely optimising, but in the right place a task queue can bring many benefits, especially when building large interconnected systems.\n\nSimply put, DADI Queue is a great way to manage complexity in your app.\n\nHappy queuing and don't forget to [register for the crowdsale](https://dadi.cloud/en/register) to get involved.\n\nFor more information or help integrating DADI Queue you can get in touch on [Discord](https://discordapp.com/invite/3sEvuYJ), via email (see profile) or send a tweet to [@dadi](https://twitter.com/dadi).","category":["6d898244-6df5-40ed-adaf-7378f1fd0a55"],"excerpt":"DADI Queue is one of the microservices from our open-source box of web service tricks. It will soon to be available via our decentralized cloud services platform.","published":true,"publishedAt":1514888940000,"slug":"forming-an-orderly-queue","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"Forming an orderly queue","_refWeb-service":{},"web-service":["b3fbbacc-dcd6-45c2-b2ff-914aa1502506"],"_createdBy":"api-client","_id":"6fb08725-d10a-4ddf-a89b-1081e7f42cca","meta":{"revision":0,"created":1530494399432,"version":0},"$loki":35},{"_apiVersion":"1.0","_createdAt":1530494400450,"_lastModifiedAt":1526055128930,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["2647a07e-0b1f-45c0-8e90-0b55475d838b"],"body":"We've been looking forward to writing this update. Not only because it spills detail on the status of our testnet (which up until this point we've confirmed only as 'in development'), but also because we'll soon be announcing how you can be part of the testing process and experience the network for yourself. But more on that at the end of this article.\n\nFirst, some context. Following several months of focused R&D, the last two quarters of 2017 saw us rapidly progress development on the network, taking our testnet to live in October and moving it into production testing with a single web service (DADI CDN) in November.\n\nThe testnet is currently small - it's running on our hardware testbed with Netwise in London Bridge, and across a bunch of Raspberry Pi's connected to routers in our employees' homes.\n\n![Some of the DADI testnet in our employees' homes.](https://cdn-images-1.medium.com/max/2000/1*1lUL14DN1FK_YuKYhN5OoQ.png \"Some of the DADI testnet in our employees' homes.\")\n\nThe Raspberry Pi is a low-powered, low-footprint machine, developed with affordability and versatility in mind. These features make it a great low-end benchmark for performance testing the Host component of the network.\n\nBut being small doesn't mean that it lacks punch. The predominantly Pi-powered testnet has handled over 5m requests in the last two months and has maintained 100% uptime.\n\n## Network nodes\n\nThe testnet is running all three major components of the DADI network: Stargates, Gateways and Hosts. Each has a unique role to play in enabling our decentralized infrastructure, and the current version of each that is deployed to the testnet reflects what we consider to be a minimum working product.\n\n![The DADI network.](https://cdn-images-1.medium.com/max/2000/1*wcvjnaYNIQyYxYl-Ygacqw.jpeg \"The DADI network.\")\n\n### Stargates\n\nStargates manage DNS and routing in the network, distributing incoming traffic to the appropriate Gateway, based on their knowledge of which Gateways are handling requests for the corresponding web service.\n\nThey also manage the distribution of application data to all Gateways and Hosts, as well as handling the coordination of network devices, allowing the network to 'talk'. Realtime communication between devices in the network enables the scaling resources on demand.\n\n### Gateways\n\nWhen a request reaches the Gateway, it joins an in-memory queue. Each connected Host plucks a job from the queue, resolves the request and returns the response back via the queue handler to the end user.\n\n### Hosts\n\nEach Host is responsible for a single Consumer application within the DADI network (a single tenancy setup that helps to ensure performance). When you deploy to the network, it replicates across Hosts to meet the unique requirements of your web services setup. As with our testnet, a Host can be anything from a rack in a server farm, to a Raspberry Pi wired into a home router.\n\n## Network footprint\n\nThe testnet currently has a tiny footprint — 20 low powered machines in two countries (the United Kingdom and Portugal).\n\nWe will be expanding this rapidly over the coming months, placing nodes with our connectivity partners, predominantly focused on Europe and the United States in the first instance.\n\nThe testnet will become a shadow network to the backbone of the mainnet: present in all key locations to provide a complete testing setup for future developments.\n\n![Early network footprint.](https://cdn-images-1.medium.com/max/2000/1*ov_Dy9LNytHzd35ollOmaw.png \"Early network footprint.\")\n\n## A peak under the hood\n\n### Firewall... what firewall?\n\nEnsuring that Hosts could live anywhere has been a key focus for the engineering team.\n\nThe obstacles faced by a server running on a home network traditionally make it very difficult to create a reliable entry point into a networked service. For a start, most ISPs use an IP pool, meaning that your home IP address regularly changes without warning. Even if with a fixed IP, you still need to configure port forwarding on the router, and manually assign a single port per networked machine.\n\nRemote Procedure Calls (RPC) is a protocol that allows for the distribution of procedures across a network of machines. We've chosen to make use of GRPC — a universal RPC framework — to distribute traffic from a single Gateway to multiple Hosts within the network. Because GRPC uses standards-based HTTP/2 as transport, it easily traverses proxies and firewalls, enabling it to be setup in a home environment without the need for router/firewall configuration — a key requirement for the Host component in the network.\n\n### Distributed configuration coordination\n\nKeeping an eye on resources is an essential requirement for any distributed infrastructure. To manage hardware, the network needs realtime information on current capacity, granular system load information and application integrity.\n\nApache Zookeeper is an extremely simple, reliable application for handling the distributed synchronisation and coordination of persistent and ephemeral configuration data. Each Stargate in the DADI network Hosts a Zookeeper server, creating persistent configuration directories for Gateways and Hosts to interact.\n\nWhen Gateways or Hosts connect to a Stargate, they create their own unique ephemeral node. And each node replicates it in appropriate directories according to status, for instance if a Gateway needs more Hosts, it replicates itself into 'Awaiting Hosts'. Any new or idle Hosts in the network watch this directory for changes and connect to any new candidates that join the directory.\n\n## Adding apps into the mix\n\nFor the first iteration of the network we decided to deploy apps to DADI Host with Docker. In the next phase we'll be moving the containers onto an encrypted VMM.\n\nWhen a Host launches, the Host system app checks for running Docker processes, identifying those that are running DADI services, such as CDN, Web, API and Publish.\n\nThe Host polls the Gateway request queue for requests pertaining to the application running on the Host machine.\n\n![Pub/sub strcuture.](https://cdn-images-1.medium.com/max/1600/1*bBB69K7a5vROaLAY6uQBvg.jpeg \"Pub/sub strcuture.\")\n\n### App distribution\n\nWe're working on methods to handle distributing apps within the network. Each consumer app will have unique system and bandwidth requirements, which should be configurable to allow for growth/contraction over time. Our approach to this remains work in progress.\n\n## Want to get involved?\n\nWe're really happy with the performance we've achieved so far on the testnet. So happy that we'd like to bring some of the community into the next phase of network testing.\n\nTo this end we will opening up applications to be part of the testnet. We will be looking for individuals from around the world to join the network — successful applicants will be provided a low-powered computer with DADI Host preinstalled and capable of running DADI CDN and DADI Web.\n\nWe'll be providing more details regarding the community involvement project in a future post. Keep an eye on our usual social channels — plus the news feed in your account page at dadi.cloud — to be the first to hear about it.","category":["0eda8d2c-e76b-484e-a95b-6ea4b3b1eaa3"],"excerpt":"Following several months of focused R&D, the last two quarters of 2017 saw us rapidly progress development on the network, taking our testnet to live in October and moving it into production testing.","published":true,"publishedAt":1516286100000,"slug":"introducing-the-dadi-testnet","sub-category":["fab17704-fb0b-4a22-aa07-c011e4e6b1d4"],"title":"Introducing the DADI testnet","_createdBy":"api-client","_id":"229d7491-ff1b-4854-95bf-32987558126a","meta":{"revision":0,"created":1530494400451,"version":0},"$loki":36},{"_apiVersion":"1.0","_createdAt":1530494401467,"_lastModifiedAt":1526477589232,"_lastModifiedBy":"cloud-client","_refCategory":{},"_version":1,"_refAuthor":{},"author":["0e03ec2e-48ca-4acc-9a0e-ed302d38b8e8"],"body":"As communicated (in all of our community channels) within 60 mins of the security breach being reported, DADI was able to ascertain ad confirm that the incident involved email addresses  and names only. No other personal data has been compromised.  The security breach was not of DADI systems but occurred on a third party solution used by DADI to provide marketing communications.  As soon as we were aware of this issue, we immediately disabled the third party solution and removed all data from that service. We also contacted individuals affected by email within 6 hours of the breach event, to inform them of the event itself and of the actions taken.\n\nDADI takes security very seriously and we will be reviewing our security measures and policies with the assistance of a third party security expert to ensure any improvements are identified and made. We are also reporting the “email hack” to the appropriate police authorities in the UK and assisting them with any enquiries regarding the phishing/scam information that was sent via email.\n\nDADI has always stressed that no account information or wallets would be sent via email. DADI has where possible taken actions necessary for any scam / phishing sites or wallets addresses to be removed or an appropriate warning noted. Unfortunately DADI is not always able to prevent phishing / scam activities, many of which are very sophisticated and part of organised crime, and whilst we will undertake measures to mitigate these risks, we cannot be responsible for the illegal actions of these people.  Please remain vigilant whenever you are supplying personal information, and especially so if it is in response to an email.","category":["0eda8d2c-e76b-484e-a95b-6ea4b3b1eaa3"],"excerpt":"You may be aware that there was an incident on 1st February 2018 where unauthorised use was made by a hacker of some email addresses which DADI holds. DADI apologises to everyone affected.","published":true,"publishedAt":1517558400000,"slug":"security-statement","title":"Security Statement","_createdBy":"api-client","_id":"4d885532-d9ef-4324-a68f-22da13e5d766","meta":{"revision":0,"created":1530494401468,"version":0},"$loki":37},{"_apiVersion":"1.0","_createdAt":1530494402483,"_version":1,"_refAuthor":{},"author":["0c46e2ee-d524-468d-9aff-c0e6439f191e"],"body":"You probably already know that we are proponents of open source software at DADI - we develop the majority of our products _in the wild_ on [GitHub](https://github.com/dadi). \n\nWe believe in transparency and the benefit of having an extra set of eyes on our output, welcoming outside suggestions and criticism. Our enterprise clients also enjoy the advantage of not being ‘locked-in’ with proprietary products – most developers can work with our applications due to our committment to [open standards](https://forum.dadi.tech/topic/43/maintaining-code-quality-at-dadi).\n\nAbout a year ago we decided to make the repositories for our [web services site](https://dadi.tech/) and [our developer documentation](https://docs.dadi.tech/) publicly available - and we and haven’t looked back since.\n\nHere are some benefits open-sourcing your site might provide to your technology business.\n\n## Provides a live example for the community \n\n**Especially relevant for us at DADI**; we build all our public-facing sites in our own technology. This gives developers and potential clients real-world, practical examples of our tech in action without having to download and install the product. Doing this shows that you have enough faith in your own technology to use it yourself.\n\n## Crowdsource your copy editing\n\nDealing with spelling and grammar mistakes are a part of the lifecycle of any website - stuff always seems to slip through through. We’ve been surprised a few times when internet strangers send pull requests for reasonably small content errors. This is a benefit for community developers as they get the credit on their Github profile for contributing to another public repository 🏅. It is also a nice feeling knowing people are reading your content so carefully!\n\n## Showcase your team\n\nGitHub is great for showing and discussing code edits in a nice sharable interface. Furthermore, every line of code can be traced back to a specific individual. Just like having your team speak at conferences and write articles (_ahem_) it raises the profile of your team - something which benefits the reputation of your company. It also encourages your team to write beautiful code the ‘right’ way, rather than taking shortcuts.\n\n## Illustrate your commitment to transparency\n\nWe are moving on from an age of closed and selfish business practices – many realise it is better to run a business like a family and show there are fellow humans on the other side. Those worried about what you are doing with their data and whether you're tracking them can now see exactly how the cogs are working when they interact with your service - and the developers in your audience can audit your site for you.\n\n## A note about licensing & sensitive data\n\nIt's worth pointing out you might want to [choose your licensing](https://choosealicense.com/) carefully when you take this step and also be extremely careful about what you commit to a public repository – remember it's not easy to delete or change a git history and we've seen cases of bots trawling public repos for private API keys. \n\n_If you spot any errors in this post, please let me know :) and as always you can find us on [Discord](https://discordapp.com/invite/3sEvuYJ), [Telegram](https://t.me/dadichat) or in the comments._","excerpt":"We believe in transparency and the benefit of having an extra set of eyes on our output, welcoming outside suggestions and criticism.","published":true,"publishedAt":1519588560000,"slug":"why-you-should-open-source-your-website","title":"Why you should open source your website","_createdBy":"api-client","_id":"70251b2c-2321-4a2f-8147-bb943c559871","meta":{"revision":0,"created":1530494402484,"version":0},"$loki":38},{"_apiVersion":"1.0","_createdAt":1530494403496,"_lastModifiedAt":1526417060543,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["cdeeab35-b8e7-4aba-ae10-54bffead2ef1"],"body":"Regular readers will remember our post “[Public sale: by the numbers](https://medium.com/@daditech/public-sale-by-the-numbers-88b086ef17ae)”, where we outlined the tremendous response to the DADI Crowdsale — 42,000+ on Telegram; 4,600+ on Discord; and almost 200,000 users registered at dadi.cloud.\n\nAn audience of this size requires some management, so we thought we’d share some stats on how we are doing, starting with email support. This is the backbone of our process for dealing with questions and issues from the community. We have a team dedicated to this support channel and we’re proud to say that we’re maintaining an average response time of under 24hrs (17 hours and 56 minutes to be precise).\n\n![A graph of the stats](/media/2018/05/15/1_DHjwdx7Vu6XV_MvflZeJqg.png)\n\nTo elaborate further, 18% of tickets were responded to within an hour, 27% of tickets were responded to within 8 hours, and a further 27% of tickets within 24 hours.\n\nIn total, there have been 3,709 tickets, of which 3,469 are solved, which is a whopping 93.5%.\n\n![](https://cdn-images-1.medium.com/max/2000/1*60TIi5Jse2bDlmOcxjP1Ig.png)\n\nWe had a number of contributors requesting changes to their account wallet following the conclusion of the Public sale. Security is of paramount importance, and the process for verifying wallet ownership takes some time to complete. Even still, of over 200 requests we’ve verified over 130, updating their wallets accordingly.\n\nOf course email support isn’t the only place where you can request assistance. Our Telegram and our Discord are channels are for life, not just for crowdsale, and our 10-strong team of community admins can be found in these channels around the clock. They can answer your questions even faster than we can by email.\n\nSo, if there’s something you need to know about DADI or about your account, try one of our social channels first, or DM an admin. Otherwise send us an email and we’ll try to beat 17 hours and 56 minutes.","category":["6d898244-6df5-40ed-adaf-7378f1fd0a55"],"excerpt":"Regular readers will remember our post “Public sale: by the numbers”, where we outlined the tremendous response to the DADI Crowdsale...","published":true,"publishedAt":1519646220000,"slug":"community-support-by-the-numbers","sub-category":["fab17704-fb0b-4a22-aa07-c011e4e6b1d4"],"title":"Community support, by the numbers","_createdBy":"api-client","_id":"bb622392-8ae2-472e-aee8-899cb542da44","meta":{"revision":0,"created":1530494403498,"version":0},"$loki":39},{"_apiVersion":"1.0","_createdAt":1530494404509,"_lastModifiedAt":1526477466108,"_lastModifiedBy":"cloud-client","_refCategory":{},"_refSub-category":{},"_version":1,"_refAuthor":{},"author":["6d08fa42-f098-4f94-b6a5-24d04ea09500"],"body":"DADI Track is a real-time data visualisation layer built around the concept of events.\n\n![](https://cdn-images-1.medium.com/max/2000/1*lk5d4BnOrud6xId8Ct_40g.png)\n\nEvents can be sent to DADI Track from applications within the DADI platform or from any website that includes an appropriate script.\n\nDADI Track is deliberately lightweight. It is not intended to be a long term data storage solution or as a replacement for existing tracking tools such as Google Analytics or the wider DADI platform; rather it is designed to facilitate real-time data dashboarding and piping.\n\n## Architectural overview\n\nDADI Track is an event tracking server that records user activity using either a tracking pixel built into a website, via a client-side JavaScript library or via an API interface.\n\nThe Track server broadcasts received events to any connected clients using Websockets — it is intended that DADI Visualise is one such connected client, and will utilise a collection of widgets to display user activity broadcast by Track.\n\n## Tracking\n\nThere are currently two forms of tracking available — a tracking pixel and a lightweight client-side JavaScript library.\n\n### Tracking pixel\n\nThe tracking pixel solution allows the embedding of a one pixel image into your webpages, using the domain of the Track server. Specific events may be captured using the querystring on the request to the tracking pixel:\n\n```\nhttp://your-domain.com/tracking_pixel.gif?event=cart_add\n```\n\nAdditional arbitrary data can be sent along with the request, simply by extending the querystring:\n\n```\nhttp://your-domain.com/tracking_pixel.gif?event=cart_add&clientId=679101\n```\n\n#### Client-side JavaScript\n\nThe JavaScript library contains a small script to trigger a Track event. Once the file is embedded in your webpage(s) the `track()` function can be called:\n\n```\nDADI.track();\n```\n\nCalled with no arguments, it will send some standard parameters such as the page URL. As with the tracking pixel approach, you can pass arbitrary data with the event:\n\n```\nDADI.track({logged_in: true});\n```\n\n## Event handling via Metrics\n\nThe Track server loads a collection of \"Metrics\", some of which are included in the core, and some which may be custom built and included via a plugin system. Metrics are stored in directories within the Track server and auto-loaded when the server starts.\n\nEach metric contains a handler function that is called every time a new event occurs. Data is stored within the metric and emitted to connected clients at intervals specified by the metric’s own configuration options.\n\nA basic example of a Metric is one for “total page views”. It would look something like this:\n\n```\n// Total views\n// Sends total count per interval of time for every request made\n\nconst Metric = require('../metric').Metric\n\nmodule.exports = function () {\n  let totalViews = new Metric()\n  totalViews.name = ‘view_totals’\n  totalViews.initialData = 0\n  totalViews.interval = 500\n  totalViews.ignoreOnEmpty = false\n  totalViews.increment = function (request) {\n    this.data += 1\n  }\n  return totalViews\n}\n```\n\n## Visualisation\n\nThe alpha build of Track contains a simple dashboard to display events as they are broadcast.\n\nTo maintain the lightweight approach for Track it is intended that DADI Visualise will handle the display of events, meaning that ultimately the current dashboard will be removed from the Track codebase.\n\n## DADI Track — Current state and next steps\n\nDADI Track is at an Alpha release stage and available only for private development and testing. The first public release of DADI Track will be the Beta, scheduled for Q2 2018.","category":["0eda8d2c-e76b-484e-a95b-6ea4b3b1eaa3"],"excerpt":"DADI Track is a real-time data visualisation layer built around the concept of events.","published":true,"publishedAt":1519823280000,"slug":"dadi-track","sub-category":["6de412de-607d-4957-9c49-a48ec3ebace2"],"title":"DADI Track","_refWeb-service":{},"web-service":["8929b56d-4527-48d6-a81d-b24761d4ebd2"],"_createdBy":"api-client","_id":"5351cd1f-31bc-435d-81d5-a17565041a6d","meta":{"revision":0,"created":1530494404510,"version":0},"$loki":40}],"idIndex":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40],"binaryIndices":{"publishedAt":{"name":"publishedAt","dirty":false,"values":[5,4,3,2,1,0,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39]}},"constraints":null,"uniqueNames":[],"transforms":{},"objType":"articles","dirty":false,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":false,"cloneMethod":"parse-stringify","asyncListeners":false,"disableChangesApi":true,"autoupdate":false,"ttl":null,"maxId":40,"DynamicViews":[],"events":{"insert":[null],"update":[null],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[]},{"name":"images","data":[],"idIndex":[],"binaryIndices":{"filename":{"name":"filename","dirty":false,"values":[]}},"constraints":null,"uniqueNames":[],"transforms":{},"objType":"images","dirty":false,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":false,"cloneMethod":"parse-stringify","asyncListeners":false,"disableChangesApi":true,"autoupdate":false,"ttl":null,"maxId":0,"DynamicViews":[],"events":{"insert":[null],"update":[null],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[]},{"name":"milestones","data":[{"_apiVersion":"1.0","_createdAt":1530494325368,"_version":1,"complete":true,"date":1506812400000,"desc":"Version 4.0 milestone release of DADI Web brings several performance enhancements and a simplified codebase for easier maintenance.","title":"DADI Web (v4.0)","_refWeb-service":{},"web-service":["f0d957ef-7cc5-4c71-bbb6-978c1cb43951"],"_createdBy":"api-client","_id":"be956f99-3599-4895-b7e9-f1fb97a4badb","meta":{"revision":0,"created":1530494325368,"version":0},"$loki":1},{"_apiVersion":"1.0","_createdAt":1530494326381,"_lastModifiedAt":1519980961943,"_version":1,"complete":true,"date":1506812400000,"desc":"Version 3.0 milestone release of DADI API will widen the choice of data connectors available and allow engineers to build their own.","title":"DADI API (v3.0)","_refWeb-service":{},"web-service":["20ccf255-452b-48fd-9a89-4afccfe4d8d7"],"_createdBy":"api-client","_id":"d6d8c29b-a775-497e-b4b8-eb67a88bf570","meta":{"revision":0,"created":1530494326382,"version":0},"$loki":2},{"_apiVersion":"1.0","_createdAt":1530494327396,"_lastModifiedAt":1519746526045,"_version":1,"complete":true,"date":1506812400000,"desc":"Custom build server-class hardware installed with Netwise in London Bridge.","title":"Hardware test bed for Stargate, Gateway and Host","_createdBy":"api-client","_id":"685d4ef0-0af1-4b72-a36c-5bd51bd4c39d","meta":{"revision":0,"created":1530494327396,"version":0},"$loki":3},{"_apiVersion":"1.0","_createdAt":1530494328406,"_lastModifiedAt":1526572805886,"_lastModifiedBy":"cloud-client","_refArticles":{},"_version":1,"articles":["5a95747bca5bf96853458485"],"complete":true,"date":1506812400000,"desc":"Full public release of DADI publishing interfaces: production ready build, complete with baseline test coverage.","title":"Beta release: DADI Publish","_refWeb-service":{},"web-service":["20ccf255-452b-48fd-9a89-4afccfe4d8d7"],"_createdBy":"api-client","_id":"8a1ed499-4c53-4e71-a975-7ee302388246","meta":{"revision":0,"created":1530494328406,"version":0},"$loki":4},{"_apiVersion":"1.0","_createdAt":1530494329421,"_version":1,"complete":true,"date":1514764800000,"desc":"Development and release of test network using Raspberry Pi machines located at DADI team members’ homes and offices.","title":"Testnet production testing","_createdBy":"api-client","_id":"1346916d-a061-4e2e-82b2-03863985575a","meta":{"revision":0,"created":1530494329421,"version":0},"$loki":5},{"_apiVersion":"1.0","_createdAt":1530494330435,"_lastModifiedAt":1526572962363,"_lastModifiedBy":"cloud-client","_version":1,"complete":true,"date":1514764800000,"desc":"Development and release of secure Know Your Customer system supported by dedicated team of staff in London.","title":"DADI KYC system","_createdBy":"api-client","_id":"5fb30bec-5bf0-4c43-ae40-b3bedcf08015","meta":{"revision":0,"created":1530494330436,"version":0},"$loki":6},{"_apiVersion":"1.0","_createdAt":1530494331449,"_lastModifiedAt":1526572891427,"_lastModifiedBy":"cloud-client","_version":1,"complete":true,"date":1516579200000,"desc":"Development and release of account pages and supporting system to manage contributions to Presale on January 22nd at 12.00UTC.","title":"DADI Presale","_createdBy":"api-client","_id":"ddc58c6f-78d1-4fbd-bd0b-abfb8b3c214e","meta":{"revision":0,"created":1530494331450,"version":0},"$loki":7},{"_apiVersion":"1.0","_createdAt":1530494332463,"_lastModifiedAt":1526572881662,"_lastModifiedBy":"cloud-client","_version":1,"complete":true,"date":1517184000000,"desc":"Development and release of account pages and supporting system to manage contributions to Public sale on January 29th at 12.00UTC.","title":"DADI Public sale","_createdBy":"api-client","_id":"0439d3c9-1afe-4dcc-8046-84036617f247","meta":{"revision":0,"created":1530494332463,"version":0},"$loki":8},{"_apiVersion":"1.0","_createdAt":1530494333475,"_lastModifiedAt":1526573062757,"_lastModifiedBy":"cloud-client","_version":1,"complete":true,"date":1517443200000,"desc":"Concept interfaces for all-new DADI.cloud website, to replace existing DADI.cloud and DADI.tech properties.","title":"Preview release: DADI.cloud","_createdBy":"api-client","_id":"70b4c34c-299d-44a2-935d-abcd68ab4f6f","meta":{"revision":0,"created":1530494333476,"version":0},"$loki":9},{"_apiVersion":"1.0","_createdAt":1530494334487,"_lastModifiedAt":1526573124624,"_lastModifiedBy":"cloud-client","_refArticles":{},"_version":1,"articles":["5a9e92e1dd6169353344d6bf"],"complete":true,"date":1517443200000,"desc":"First release of DADI’s enterprise-strength web, mobile and event analytics platform. Private Alpha for testing and development.","title":"Alpha release: DADI Track","_refWeb-service":{},"web-service":["8929b56d-4527-48d6-a81d-b24761d4ebd2"],"_createdBy":"api-client","_id":"4cea0c93-b7f4-4c6d-b37f-891222366b65","meta":{"revision":0,"created":1530494334487,"version":0},"$loki":10},{"_apiVersion":"1.0","_createdAt":1530494335502,"_lastModifiedAt":1526572178060,"_lastModifiedBy":"cloud-client","_version":1,"complete":true,"date":1517443200000,"desc":"Mid-term release to include plugin support, simplified URL paths for CSS and JS assets, ES6 compression ad transpiling.","title":"DADI CDN (v2.0)","_refWeb-service":{},"web-service":["4693150d-f75c-4563-bb2d-934214b9e9bc"],"_createdBy":"api-client","_id":"894c3dc0-11d2-4090-a33e-34907741d94c","meta":{"revision":0,"created":1530494335504,"version":0},"$loki":11},{"_apiVersion":"1.0","_createdAt":1530494336517,"_lastModifiedAt":1526571886507,"_lastModifiedBy":"cloud-client","_refArticles":{},"_version":1,"articles":["5a956cdeca5bf9685345847b"],"complete":true,"date":1517443200000,"desc":"Distribution of DADI tokens to all crowdsale participants via the wallet addresses registered at their DADI account page.","title":"DADI token distribution","_createdBy":"api-client","_id":"cad8cd2c-bc33-46a8-8d4f-5f4f6950aefd","meta":{"revision":0,"created":1530494336518,"version":0},"$loki":12},{"_apiVersion":"1.0","_createdAt":1530494337531,"_version":1,"complete":true,"date":1517443200000,"desc":"Account pages will show how many user referrals participated in the DADI crowdsale and how many tokens to be received as a result.","title":"Referral data available on account pages","_createdBy":"api-client","_id":"b0cb224d-2fac-4e1e-b6eb-2327c91a8eb1","meta":{"revision":0,"created":1530494337531,"version":0},"$loki":13},{"_apiVersion":"1.0","_createdAt":1530494338542,"_lastModifiedAt":1526638855964,"_lastModifiedBy":"cloud-client","_refWeb-service":{},"_version":1,"complete":true,"date":1522796400000,"desc":"Milestone release of our API web service, adding multi-collection reference fields and modularised field types.","title":"DADI API (v3.1)","web-service":["27d1f07c-a77c-4fea-af32-78680674c1f1"],"_createdBy":"api-client","_id":"494a4078-03ed-470d-adf6-50babcceb321","meta":{"revision":0,"created":1530494338543,"version":0},"$loki":14},{"_apiVersion":"1.0","_createdAt":1530494339559,"_lastModifiedAt":1526639486549,"_lastModifiedBy":"cloud-client","_version":1,"complete":true,"date":1524697200000,"desc":"Significant feature release for DADI Web, to include multiple API support and debug view.","title":"DADI Web (v6.0)","_refWeb-service":{},"web-service":["f0d957ef-7cc5-4c71-bbb6-978c1cb43951"],"_createdBy":"api-client","_id":"4a8ecb72-e602-4804-99c4-039f6759fe9a","meta":{"revision":0,"created":1530494339560,"version":0},"$loki":15},{"_apiVersion":"1.0","_createdAt":1530494340571,"_lastModifiedAt":1526639666392,"_lastModifiedBy":"cloud-client","_version":1,"complete":true,"date":1526630400000,"desc":"Launch of all-new DADI.cloud website, to replace existing DADI.cloud and DADI.tech properties.","title":"Beta release: new DADI.cloud","_createdBy":"api-client","_id":"afa834a6-c900-4b2a-bd25-7f596b8dd95c","meta":{"revision":0,"created":1530494340572,"version":0},"$loki":16},{"_apiVersion":"1.0","_createdAt":1530494341583,"_createdBy":"api-client","_lastModifiedAt":1527072845236,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":true,"date":1526886000000,"desc":"Network-ready release of DADI CDN – the first web service to be made available for DADI network.","title":"DADI CDN (v3.0)","_id":"faa2669f-5bd1-4b87-b743-85cecc5e138e","meta":{"revision":0,"created":1530494341583,"version":0},"$loki":17},{"_apiVersion":"1.0","_createdAt":1530494342596,"_lastModifiedAt":1530198190962,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refNetwork-service":{},"_version":1,"complete":true,"date":1530054000000,"desc":"First release of DADI Gateway, containing MVP VPC functionality, messaging and a custom Job queue.","network-service":["0063d4e3-491e-44d2-b99c-c7deaf44d13c"],"title":"Beta release: DADI Gateway","_createdBy":"api-client","_id":"5ee7476a-fbb7-48e9-adb2-01e3d8186746","meta":{"revision":0,"created":1530494342596,"version":0},"$loki":18},{"_apiVersion":"1.0","_createdAt":1530494343605,"_lastModifiedAt":1530198210925,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":true,"date":1530061200000,"desc":"First full release of DADI Stargate for testing with trusted partners.","title":"Beta release: DADI Stargate","_createdBy":"api-client","_id":"4863feec-9bef-4af7-bff6-27c9dec17f1f","meta":{"revision":0,"created":1530494343606,"version":0},"$loki":19},{"_apiVersion":"1.0","_createdAt":1530494344614,"_lastModifiedAt":1530198227833,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":true,"date":1530085560000,"desc":"First release of DADI Host, containing MVP Docker layer, Gateway integration and support for DADI CDN.","title":"Beta release: DADI Host","_createdBy":"api-client","_id":"d3adbaa6-e1cb-476e-a322-1bb69f22781c","meta":{"revision":0,"created":1530494344615,"version":0},"$loki":20},{"_apiVersion":"1.0","_createdAt":1530494345624,"_lastModifiedAt":1530208072606,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":true,"date":1530201600000,"desc":"DADI network up and running with “backbone” Stargate, Gateway and Host set in key locations. Network open for testing by early adopters.","title":"Beta release: Mainnet","_refWeb-service":{},"web-service":["4693150d-f75c-4563-bb2d-934214b9e9bc"],"_createdBy":"api-client","_id":"2549d80f-933e-429a-b476-d3f7f93ba41e","meta":{"revision":0,"created":1530494345625,"version":0},"$loki":21},{"_apiVersion":"1.0","_createdAt":1530494346636,"_createdBy":"api-client","_lastModifiedAt":1530208078025,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":true,"date":1530203400000,"desc":"dApp version of DADI CDN ready for use on DADI mainnet","title":"Network-ready CDN","_id":"7e9203d1-417b-4471-a4af-2dc4f525b65d","meta":{"revision":0,"created":1530494346636,"version":0},"$loki":22},{"_apiVersion":"1.0","_createdAt":1530494347646,"_createdBy":"api-client","_version":1,"complete":false,"date":1530723540000,"desc":"Introduces an advanced permissions system, making it easier for end-users or client-side applications to communicate with API directly","title":"DADI API v4.0","_id":"12fdf74c-3b3f-4751-952a-4f825216144a","meta":{"revision":0,"created":1530494347647,"version":0},"$loki":23},{"_apiVersion":"1.0","_createdAt":1530494348655,"_createdBy":"api-client","_lastModifiedAt":1530199761618,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":false,"date":1531236420000,"desc":"First phase of nodes added to DADI network","title":"Node onboarding: phase one","_id":"111c1fe7-c369-4555-a965-1fdc9aa7ceb0","meta":{"revision":0,"created":1530494348655,"version":0},"$loki":24},{"_apiVersion":"1.0","_createdAt":1530494349665,"_createdBy":"api-client","_lastModifiedAt":1530199790993,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":false,"date":1531322820000,"desc":"Second phase of nodes added to DADI network","title":"Node onboarding: phase two","_id":"0c7e46c3-5cd1-4367-b897-d7fb72a69517","meta":{"revision":0,"created":1530494349666,"version":0},"$loki":25},{"_apiVersion":"1.0","_createdAt":1530494350679,"_createdBy":"api-client","_lastModifiedAt":1530199795446,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":false,"date":1531409220000,"desc":"Third phase of nodes added to DADI network","title":"Node onboarding: phase three","_id":"9f78ad1c-29bc-4601-a71e-a963a994cf05","meta":{"revision":0,"created":1530494350680,"version":0},"$loki":26},{"_apiVersion":"1.0","_createdAt":1530494351691,"_createdBy":"api-client","_lastModifiedAt":1530203526077,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":false,"date":1531667520000,"desc":"dApp version of our cloud storage solution ready for use on mainnet","title":"Network-ready Store","_id":"5b795dc0-f9cc-4a0c-a6a9-a760e5ad12b9","meta":{"revision":0,"created":1530494351692,"version":0},"$loki":27},{"_apiVersion":"1.0","_createdAt":1530494352705,"_createdBy":"api-client","_version":1,"complete":false,"date":1539616380000,"desc":"dApp version of DADI CDN ready for use on DADI mainnet","title":"Network-ready DADI API","_id":"bf7e44f4-8f58-47a2-9d80-ff166627718a","meta":{"revision":0,"created":1530494352705,"version":0},"$loki":28},{"_apiVersion":"1.0","_createdAt":1530494353715,"_createdBy":"api-client","_lastModifiedAt":1526640852298,"_lastModifiedBy":"cloud-client","_version":1,"complete":false,"date":1544010720000,"desc":"Full public release of the DADI network, details to be confirmed.","title":"DADI 'Constellation'","_id":"b716abc6-c211-4be8-92b5-26491ad54739","meta":{"revision":0,"created":1530494353715,"version":0},"$loki":29},{"_apiVersion":"1.0","_createdAt":1530494354723,"_createdBy":"api-client","_lastModifiedAt":1530203480769,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":false,"date":1547568780000,"desc":"dApp version of content management interfaces ready for use on mainnet","title":"Network-ready DADI Publish","_id":"094e99da-6331-4457-96ac-6de4170c7c39","meta":{"revision":0,"created":1530494354724,"version":0},"$loki":30},{"_apiVersion":"1.0","_createdAt":1530494355733,"_createdBy":"api-client","_lastModifiedAt":1530203574208,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":false,"date":1555341240000,"desc":"dApp version of our web templating layer ready for use on mainnet","title":"Network-ready DADI Web","_id":"76661efb-9211-4eb5-a6b2-22c5b8688200","meta":{"revision":0,"created":1530494355734,"version":0},"$loki":31},{"_apiVersion":"1.0","_createdAt":1530494356745,"_createdBy":"api-client","_lastModifiedAt":1530203611673,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":false,"date":1555427700000,"desc":"dApp version of user data store ready for use on mainnet","title":"Network-ready DADI identity","_id":"0b832707-b8c4-4125-9fa4-3b03d5d9c6f9","meta":{"revision":0,"created":1530494356745,"version":0},"$loki":32},{"_apiVersion":"1.0","_createdAt":1530494357756,"_createdBy":"api-client","_lastModifiedAt":1530203636993,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":false,"date":1563203700000,"desc":"dApp version of our real-time, streaming data layer ready for use on mainnet","title":"Network-ready DADI Track","_id":"0ef1c6c3-2159-40f4-9fec-b4c2cc660966","meta":{"revision":0,"created":1530494357757,"version":0},"$loki":33},{"_apiVersion":"1.0","_createdAt":1530494358769,"_createdBy":"api-client","_lastModifiedAt":1530203657552,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":false,"date":1563203760000,"desc":"dApp version of our data visualization interface ready for use on mainnet","title":"Network-ready DADI Visualize","_id":"8fad58e6-e738-4a5a-9332-f8c4d1226239","meta":{"revision":0,"created":1530494358769,"version":0},"$loki":34},{"_apiVersion":"1.0","_createdAt":1530494359782,"_createdBy":"api-client","_lastModifiedAt":1530203683555,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":false,"date":1571152560000,"desc":"dApp version of our machine learning layer ready for use on mainnet","title":"Network-ready DADI Predict","_id":"bb62d787-6d4a-46dd-a650-1eb465b83f78","meta":{"revision":0,"created":1530494359782,"version":0},"$loki":35},{"_apiVersion":"1.0","_createdAt":1530494360791,"_createdBy":"api-client","_lastModifiedAt":1530203710606,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":false,"date":1571152620000,"desc":"dApp version of our asynchronous task queue ready for use on mainnet","title":"Network-ready DADI Queue","_id":"e92399d7-7405-40f3-badf-70131c0b927a","meta":{"revision":0,"created":1530494360792,"version":0},"$loki":36},{"_apiVersion":"1.0","_createdAt":1530494361801,"_createdBy":"api-client","_lastModifiedAt":1530203941625,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"complete":false,"date":1571152620000,"desc":"dApp version of our taxonomic framework for automated content classification ready for use on mainnet","title":"Network-ready DADI Match","_id":"e4c63910-a6ba-4c77-aa81-5e1a7febea03","meta":{"revision":0,"created":1530494361801,"version":0},"$loki":37}],"idIndex":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37],"binaryIndices":{"date":{"name":"date","dirty":false,"values":[3,2,1,0,5,4,6,7,12,11,10,9,8,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,36,35]}},"constraints":null,"uniqueNames":[],"transforms":{},"objType":"milestones","dirty":false,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":false,"cloneMethod":"parse-stringify","asyncListeners":false,"disableChangesApi":true,"autoupdate":false,"ttl":null,"maxId":37,"DynamicViews":[],"events":{"insert":[null],"update":[null],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[]},{"name":"network-services","data":[{"_apiVersion":"1.0","_createdAt":1530494320311,"_createdBy":"api-client","_version":1,"name":"Host","overview":"Provide processing power for multiple Consumer app bundles.","published":true,"slug":"host","_id":"b355a880-dcfd-4019-8847-26bbd8f4b593","meta":{"revision":0,"created":1530494320311,"version":0},"$loki":1},{"_apiVersion":"1.0","_createdAt":1530494321324,"_createdBy":"api-client","_version":1,"name":"Gateway","overview":"Act as network aggregation points for Host capacity and negotiate rates with Hosts.","published":true,"slug":"gateway","_id":"0063d4e3-491e-44d2-b99c-c7deaf44d13c","meta":{"revision":0,"created":1530494321324,"version":0},"$loki":2},{"_apiVersion":"1.0","_createdAt":1530494322336,"_createdBy":"api-client","_version":1,"name":"Stargate","overview":"Provide domain routing and negotiate rates with Consumers.","published":true,"slug":"stargate","_id":"d7272edd-3274-4a89-892e-363309002f29","meta":{"revision":0,"created":1530494322337,"version":0},"$loki":3}],"idIndex":[1,2,3],"binaryIndices":{"order":{"name":"order","dirty":false,"values":[2,1,0]}},"constraints":null,"uniqueNames":[],"transforms":{},"objType":"network-services","dirty":false,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":false,"cloneMethod":"parse-stringify","asyncListeners":false,"disableChangesApi":true,"autoupdate":false,"ttl":null,"maxId":3,"DynamicViews":[],"events":{"insert":[null],"update":[null],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[]},{"name":"sub-categories","data":[{"_apiVersion":"1.0","_createdAt":1530494271769,"_lastModifiedAt":1519821212169,"_version":1,"name":"Announcements","order":1,"slug":"announcements","_createdBy":"api-client","_id":"f74856ec-dbdc-45ff-b096-9b078c4b3348","meta":{"revision":0,"created":1530494271770,"version":0},"$loki":1},{"_apiVersion":"1.0","_createdAt":1530494272780,"_lastModifiedAt":1519821415331,"_version":1,"name":"Web Services","order":2,"slug":"web-services","_createdBy":"api-client","_id":"6de412de-607d-4957-9c49-a48ec3ebace2","meta":{"revision":0,"created":1530494272780,"version":0},"$loki":2},{"_apiVersion":"1.0","_createdAt":1530494273791,"_lastModifiedAt":1519821421608,"_version":1,"name":"Network","order":3,"slug":"network","_createdBy":"api-client","_id":"fab17704-fb0b-4a22-aa07-c011e4e6b1d4","meta":{"revision":0,"created":1530494273791,"version":0},"$loki":3},{"_apiVersion":"1.0","_createdAt":1530494274797,"_lastModifiedAt":1519821434587,"_version":1,"name":"Website","order":5,"slug":"website","_createdBy":"api-client","_id":"9138e53b-06e4-49ff-a27b-f82965a52d9b","meta":{"revision":0,"created":1530494274798,"version":0},"$loki":4}],"idIndex":[1,2,3,4],"binaryIndices":{"order":{"name":"order","dirty":false,"values":[0,1,2,3]}},"constraints":null,"uniqueNames":[],"transforms":{},"objType":"sub-categories","dirty":false,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":false,"cloneMethod":"parse-stringify","asyncListeners":false,"disableChangesApi":true,"autoupdate":false,"ttl":null,"maxId":4,"DynamicViews":[],"events":{"insert":[null],"update":[null],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[]},{"name":"team","data":[{"_apiVersion":"1.0","_createdAt":1530494283844,"_lastModifiedAt":1526318931887,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"advisor":true,"body":"DADI is delighted to welcome [Wirehive](https://www.wirehive.com/) CEO Robert Belgrave as an advisor to its cloud computing product.\n\nRobert brings experience in cloud services having been part of the founding team at Wirehive and instrumental in its success in the five years since. He is also chair of BIMA South, co-creator of the [Alexa Stop](https://twitter.com/alexa_stop)! podcast and founder of [Omnitude](http://omnitude.org/) – an ecommerce ecosystem built on the blockchain\n\nRobert's unique background in cloud consultancy and blockchain services makes him an ideal fit as an advisor to DADI as its decentralized peer-to-peer hosting network develops.","name":"Robert Belgrave","personalSite":"https://about.me/robertbelgrave","slug":"robert-belgrave","twitter":"robertbelgrave","_createdBy":"api-client","_id":"294ef49d-1349-4dfd-a91e-850eea486e6f","meta":{"revision":0,"created":1530494283845,"version":0},"$loki":1},{"_apiVersion":"1.0","_createdAt":1530494284855,"_lastModifiedAt":1526318249409,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Joseph is the Founder & CEO of DADI, and the visionary behind DADI’s decentralized architecture and web services.\n\nHe is an expert in multi-agent and blockchain technologies as well as big data and machine learning. He was responsible for Symphony CMS and has 20+ years experience developing data and content platforms.\n\nJoseph was previously Group Technical Director for the Leo Burnett Group, the Founder of Airlock (a multi-award winning technology company), the Technical Director at Chime Communications and a member of the technology board at the BBC. His work is known across the industry and has been recognised with multiple Webby, Lovie, Emmy, Sony and Bafta awards.","linkedIn":"josephdenne","name":"Joseph Denne","order":1,"personalSite":"https://josephdenne.com/","role":"Founder & CEO","slug":"joseph-denne","twitter":"josephdenne","advisor":false,"_createdBy":"api-client","_id":"cdd7a16d-b068-40c7-8b22-6c1e39c35145","meta":{"revision":0,"created":1530494284855,"version":0},"$loki":2},{"_apiVersion":"1.0","_createdAt":1530494285862,"_lastModifiedAt":1526318881712,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Chris is a strategist and an early blockchain enthusiast, having first invested in Bitcoin in 2011.\n\nHe is a founder of DADI and is the key architect behind the decentralized business strategy. Prior to DADI he was a partner at London based technology company Airlock, where he headed the strategy division.\n\nBefore joining Airlock, Chris was the Global Head of Digital Technology for fashion brand Diesel where he was responsible for the development and implementation of the brand’s digital strategy across 33 markets worldwide.","linkedIn":"chrismair","name":"Chris Mair","order":2,"role":"Founding Partner","slug":"chris-mair","twitter":"chrismair","advisor":false,"_createdBy":"api-client","_id":"cec4002b-4b8c-4e01-bea2-716752159dfb","meta":{"revision":0,"created":1530494285863,"version":0},"$loki":3},{"_apiVersion":"1.0","_createdAt":1530494286889,"_lastModifiedAt":1526318892512,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Will has been founding tech companies since 1998, specialising in content and data management. He leads the solutions and support teams at DADI, focusing on partnerships and the strategic implementation of DADI technology\n\nWill was previously Managing Director for technology company Airlock, where he oversaw all client relationships – and sat on the Senior Management Council at Leo Burnett Group. He was also a Non-Executive Director for Symphony CMS.","linkedIn":"willlebens","name":"Will Lebens","order":3,"role":"Founding Partner","slug":"will-lebens","twitter":"WillLebens","advisor":false,"_createdBy":"api-client","_id":"6cee0279-f772-4312-a758-e4047b726a55","meta":{"revision":0,"created":1530494286889,"version":0},"$loki":4},{"_apiVersion":"1.0","_createdAt":1530494287899,"_lastModifiedAt":1526318948044,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Paul is Chief Operating Officer at DADI, bringing COO and CFO experience from portfolio of digital, marketing and media companies and projects.\n\nHe is responsible for operations and financial management at DADI.\n\nPaul also holds a variety of senior and consulting positions across a portfolio of digital and tech organisations including Code and Theory and Bright Analytics.","linkedIn":"paul-kingsley-838bb2","name":"Paul Kingsley","order":4,"role":"Chief Operating Officer","slug":"paul-kingsley","twitter":"PaulDKingsley","advisor":false,"_createdBy":"api-client","_id":"0e03ec2e-48ca-4acc-9a0e-ed302d38b8e8","meta":{"revision":0,"created":1530494287900,"version":0},"$loki":5},{"_apiVersion":"1.0","_createdAt":1530494288910,"_lastModifiedAt":1526318963913,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"James is a Technical Director at DADI and is an experienced software/data architect.\n\nHe has extensive experience in blockchain technologies, load balancing (nginx, nginx modules), containerization (Docker) and multi-agent tech (with a focus on Node.js, Go, C#, ASP.Net and PHP). He is responsible for the development of DADI’s decentralized cloud and web services layer.\n\nJames was previously the Lead Developer for BBC Worldwide, a Lead Developer for Barclays bank (working on deep system integration and distributed data warehousing) and the Senior Developer at Synergy International (working in the networking space for NGOs).","linkedIn":"jameslambie","name":"James Lambie","order":5,"role":"Technical Director","slug":"james-lambie","twitter":"jimlambie","advisor":false,"_createdBy":"api-client","_id":"6d08fa42-f098-4f94-b6a5-24d04ea09500","meta":{"revision":0,"created":1530494288911,"version":0},"$loki":6},{"_apiVersion":"1.0","_createdAt":1530494289920,"_lastModifiedAt":1526318975879,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Francesco is a Technical Director at DADI with over 25 years of engineering experience.\n\nHe is a systems security expert with experience across the stack, including in cloud and distributed networking. He specialises in network architecture design and oversees much of the core DADI product.\n\nFrancesco previously held senior systems and consultancy roles with many organisations and was the technical architect for several multi-channel solutions in various sectors, such as e-commerce, retail (including tesco.com), broadcasting, tourism, education, media and publishing.","linkedIn":"iannuzzelli","name":"Francesco Iannuzzelli","order":6,"role":"Technical Director ","slug":"francesco-iannuzzelli","twitter":"iannuzzelli","advisor":false,"_createdBy":"api-client","_id":"a0df5e07-55cb-4187-87f2-e3ddb6093371","meta":{"revision":0,"created":1530494289921,"version":0},"$loki":7},{"_apiVersion":"1.0","_createdAt":1530494290934,"_lastModifiedAt":1526318986858,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Jo is the Technology Implementation Director at DADI and is responsible for the support of the growing portfolio of enterprise clients using the platform.\n\nShe brings over 15 years product management, operational support and account handling experience to the role and previously led the delivery of Renault TV, a globally distributed platform covering 126 territories, delivering over 200,000 broadcast hours of content. At DADI she has led the delivery of 200+ products for Bauer Media.\n\nJo was DADI’s second employee.","linkedIn":"jobiddulph","name":"Jo Biddulph","order":7,"role":"Tech. Implementation Director","slug":"jo-biddulph","twitter":"jobiddulph","advisor":false,"_createdBy":"api-client","_id":"8693ffbd-67ec-49e8-ba81-e4657e5264cd","meta":{"revision":0,"created":1530494290935,"version":0},"$loki":8},{"_apiVersion":"1.0","_createdAt":1530494291947,"_lastModifiedAt":1526318997328,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Paul is the Product Director at DADI, responsible for the strategic development of the web services stack.\n\nHe has many years experience of digital and content strategy for publishers and broadcasters, specializing in the application of discrete web services to enable real time, personalized experiences – a key feature of DADI technology.\n\nPaul joined DADI after working as a client to the business over many years, for brands including BBC Worldwide, Discovery Channel, Monocle, Haymarket and the BBC.","linkedIn":"paulrgn","name":"Paul Regan","order":8,"personalSite":"http://paulrgn.com","role":"Product Director ","slug":"paul-regan","twitter":"paulrgn","advisor":false,"_createdBy":"api-client","_id":"2e0fd8d3-97e6-4398-b55a-3c8b41dd8214","meta":{"revision":0,"created":1530494291948,"version":0},"$loki":9},{"_apiVersion":"1.0","_createdAt":1530494292962,"_lastModifiedAt":1526319018011,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"David is the Design Director at DADI, responsible for user experience and design across the stack.\n\nHis skillset blends design and front-end development and he prefers to ‘sketch in code’ rather than work with static creative – as such he is a capable engineer with expertise in HTML, CSS and JS.\n\nBefore joining DADI, David worked at production agency Stink Studios and was responsible for design work for Redbull, Google, Chanel, Ray-Ban and Samsung among other high profile brands. His work has won many awards and has featured more than once in Apple keynote presentations.","linkedIn":"davelongworth","name":"David Longworth","order":9,"personalSite":"https://davidlongworth.com/","role":"Design Director","slug":"david-longworth","twitter":"abovedave","advisor":false,"_createdBy":"api-client","_id":"0c46e2ee-d524-468d-9aff-c0e6439f191e","meta":{"revision":0,"created":1530494292962,"version":0},"$loki":10},{"_apiVersion":"1.0","_createdAt":1530494293972,"_lastModifiedAt":1526319027817,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Viktor is a Principal Engineer at DADI, focused on the development of the web services layer of the stack.\n\nHe has extensive experience with microservice architectures, blockchain technologies, load balancing (nginx, nginx modules), containerization (Docker) and deployment methodologies.\n\nViktor was previously the Technical Lead for Nike and oversaw the development of distributed video delivery networks for Renault TV and the London 2012 Olympics during his time at Airlock.\n\nViktor was DADI’s first employee.","linkedIn":"viktorfero","name":"Viktor Fero","order":10,"role":"Principal Engineer","slug":"viktor-fero","twitter":"viktorfero","advisor":false,"_createdBy":"api-client","_id":"cb086c83-814d-4ce9-bbe0-c27edd7d684a","meta":{"revision":0,"created":1530494293973,"version":0},"$loki":11},{"_apiVersion":"1.0","_createdAt":1530494294988,"_lastModifiedAt":1526319037032,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Arthur is a Principal Engineer at DADI, focused on machine learning technologies in decentralized environments.\n\nHe is an expert in multi-agent technologies (with a focus on Node.js, Python, PHP and Go), and has extensive experience in machine learning algorithm development. He has also worked with creative technologies, including real-time physical data capture and analytics.\n\nArthur was previously the Technical Lead for Monocle, where he was responsible for the implementation of broadcast services including live playout radio, podcast distribution and video management platforms, delivering a distributed infrastructure with points of presence in every major market.","linkedIn":"arthurmingard","name":"Arthur Mingard","order":11,"personalSite":"https://arthurmingard.com/","role":"Principal Engineer","slug":"arthur-mingard","twitter":"ArthurMingard","advisor":false,"_createdBy":"api-client","_id":"2647a07e-0b1f-45c0-8e90-0b55475d838b","meta":{"revision":0,"created":1530494294989,"version":0},"$loki":12},{"_apiVersion":"1.0","_createdAt":1530494296004,"_lastModifiedAt":1526549619020,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Eduardo is a Principal Engineer at DADI with a passion for API-first content management for publishers.\n\nHe has carved a niche for himself in the industry, speaking at many events on the benefits of COPE, ‘headless’ CMS and discrete web services. He plays a key role in the development of many DADI products.\n\nBefore joining DADI, Eduardo managed the development and delivery of several products across a portfolio of 50 publications at Time Inc, including Wallpaper, NME and Uncut.","linkedIn":"eduardoboucas","name":"Eduardo Bouças","order":12,"personalSite":"https://eduardoboucas.com/","role":"Principal Engineer","slug":"eduardo-boucas","twitter":"eduardoboucas","advisor":false,"_createdBy":"api-client","_id":"5a7cde69-353b-4718-88d4-29a4cd2608a5","meta":{"revision":0,"created":1530494296006,"version":0},"$loki":13},{"_apiVersion":"1.0","_createdAt":1530494297021,"_lastModifiedAt":1526319046771,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Adam is a Senior Engineer with a background in full stack software development & systems administration, and a keen interest in distributed computing & automation. He has been involved with containerization since 2013 and has participated in numerous community projects including Docker, Deis, and Rancher.\n\nHe has extensive experience across a range of relevant disciplines including virtualization, containerization, network topology, as well as common networking protocols such as UDP, TCP, DNS, HTTP, AMQP, and ZMTP. He has a solid understanding of site reliability, having led devops efforts in a high-traffic commercial environment. He is well versed in an array of programming languages, multiple database technologies (SQL/NoSQL), and blockchain technology.\n\nAdam joined DADI after leading development & devops for a major UK retailer.","linkedIn":"adamkdean","name":"Adam K Dean","order":13,"personalSite":"http://akd.sh/","role":"Senior Engineer","slug":"adam-k-dean","twitter":"imdsm","advisor":false,"_createdBy":"api-client","_id":"cdeeab35-b8e7-4aba-ae10-54bffead2ef1","meta":{"revision":0,"created":1530494297022,"version":0},"$loki":14},{"_apiVersion":"1.0","_createdAt":1530494298036,"_lastModifiedAt":1526319057766,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Jean-Luc is a Senior Engineer at DADI and has spent many years working with the founders of the company.\n\nHe has extensive experience with distributed and technologies including load balancing (nginx, nginx modules), containerization (Docker) and multi-agent tech (with a focus on Node.js, Go, C#, and PHP).\n\nHe was the Technical Manager at Airlock, leading the development of large-scale platforms, most notably for the BBC’s coverage of Wimbledon and the 2012 London Olympics.\n\nJean-Luc has also successfully launched a whisky trading platform ([Whisky Marketplace](http://www.whiskymarketplace.com/)) and a social network for whisky connoisseurs ([Connosr](https://www.connosr.com/)).","linkedIn":"jean-luc-thiebaut","name":"Jean-Luc Thiébaut","order":14,"role":"Senior Engineer","slug":"jean-luc-thiebaut","twitter":"jeanylucky","advisor":false,"_createdBy":"api-client","_id":"a7fc921f-abd2-490f-ab18-ed6c5f98a0b3","meta":{"revision":0,"created":1530494298038,"version":0},"$loki":15},{"_apiVersion":"1.0","_createdAt":1530494299050,"_lastModifiedAt":1526319067560,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Rob is a Senior Engineer at DADI focused on content management technologies and distributed publishing models.\n\nHis 20 years’ in software engineering brings experience with Node.js, Ruby, Redis and the usual frontend suspects.\n\nRob joined DADI after working as the Development Lead for Comic Relief. He was previously a Senior Engineer for MTV, Virgin and the BBC, where he led the migration of traditional technologies to discrete web services designed for performance at scale.","linkedIn":"orinocai","name":"Robert Stanford","order":15,"role":"Senior Engineer","slug":"robert-stanford","twitter":"orinocai","advisor":false,"_createdBy":"api-client","_id":"2902224e-daee-4bea-bf64-3ca15f1333c4","meta":{"revision":0,"created":1530494299051,"version":0},"$loki":16},{"_apiVersion":"1.0","_createdAt":1530494300067,"_lastModifiedAt":1526319074361,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"body":"Peishan is a Senior Project Manager at DADI, working to support the development of DADI technology.\n\nShe brings to the role an interest in digital products that leverage machine learning and big data to personalise CRM – and has worked to deliver DADI-powered solutions that do just that.\n\nPeishan joined DADI three years ago from a position with a creative agency, where she looked after accounts for Facebook, WWF, Threadless and W Hotels.","linkedIn":"peishanchen","name":"Peishan Chen","order":16,"role":"Senior Project Manager","slug":"peishan-chen","twitter":"shelfconscious","advisor":false,"_createdBy":"api-client","_id":"ee45efcf-1bb4-43f5-a5fe-686958004d38","meta":{"revision":0,"created":1530494300069,"version":0},"$loki":17},{"_apiVersion":"1.0","_createdAt":1530494301081,"_createdBy":"api-client","_lastModifiedAt":1526472533001,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"advisor":false,"body":"Dave is a Software Test Engineer at DADI and is an experienced Test Analyst.\n\nHe has over 14 years experience in manual and automated testing across various technologies and has worked in the insurance, banking and telecoms industries. He is involved in setting up testing frameworks and test automation strategies for DADI’s core software components.\n\nPreviously Dave was a Senior Test Analyst at Suncorp-Metway Ltd (one of Australia’s major financial corporations), a Performance Tester at Telstra (Australia’s leading Telecoms company) and Performance/System Tester at Royal Bank of Scotland.","linkedIn":"david-macpherson-b007","name":"Dave Macpherson","order":17,"role":"Software Test Engineer","slug":"dave-macpherson","_id":"c08cc0dd-4730-4143-82c5-355cfa4191f8","meta":{"revision":0,"created":1530494301082,"version":0},"$loki":18},{"_apiVersion":"1.0","_createdAt":1530494302092,"_createdBy":"api-client","_lastModifiedAt":1526465027882,"_lastModifiedBy":"cloud-client","_refImage":{},"_version":1,"advisor":false,"body":"Anton is a Senior Engineer at DADI.\n\nHe has over a decade of software engineering experience with various technologies (from low-level C++ to front-end javascript).\n\nBefore joining DADI, Anton has launched several popular mobile and web apps independently. Previously, he developed the core libraries behind widely used 3d-printing and medical applications.","name":"Anton Naumovets","order":18,"role":"Senior Engineer","slug":"anton-naumovets","_id":"ed0d01cf-21bb-4480-a28f-f30df4d2e137","meta":{"revision":0,"created":1530494302093,"version":0},"$loki":19},{"_apiVersion":"1.0","_createdAt":1530494303105,"_createdBy":"api-client","_lastModifiedAt":1526897080338,"_lastModifiedBy":"cloudlive","_refImage":{},"_version":1,"advisor":false,"body":"Bolaji is Community Manager at DADI and is responsible for managing communication between the team and the community, including the industry. He is also responsible for running and attending events, building community engagement while working with the wider team to implement the marketing and communication strategy.\n\nHe has many years of international experience in customer service, marketing communication and financial services for brands such as Standard Chartered and Investors Group.","name":"Bolaji Oyewole","order":19,"role":"Community Manager","slug":"bolaji-oyewole","twitter":"BjayOyewole","_id":"9d03c513-5d4c-49ed-9eff-09e6c04c1ae6","meta":{"revision":0,"created":1530494303106,"version":0},"$loki":20},{"_apiVersion":"1.0","_createdAt":1530494304117,"_createdBy":"api-client","_lastModifiedAt":1528989149131,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refImage":{},"_version":1,"advisor":false,"body":"Joshua Overbye is a digital product designer at DADI. His responsibilities are focused on building the interface and interaction mechanics for the DADI web services & products.\nCombining research & behavioural insights into digital experiences that deliver clarity, elegance and performance.\n\nHis is an Australian designer and has a multitude of experience & skills from working with startups, organisations and individuals around the world.\n\nJoshua is a recent member to the team at DADI, joining early 2018 as we begin to build the next-generation of web services for the age of digital intelligence.","name":"Joshua Overbye","order":20,"role":"Product Designer","slug":"joshua-overbye","_id":"5b8aa38d-d54c-4cb5-b7af-19f72a0d910b","meta":{"revision":0,"created":1530494304117,"version":0},"$loki":21}],"idIndex":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21],"binaryIndices":{"order":{"name":"order","dirty":false,"values":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]}},"constraints":null,"uniqueNames":[],"transforms":{},"objType":"team","dirty":false,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":false,"cloneMethod":"parse-stringify","asyncListeners":false,"disableChangesApi":true,"autoupdate":false,"ttl":null,"maxId":21,"DynamicViews":[],"events":{"insert":[null],"update":[null],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[]},{"name":"web-services","data":[{"_apiVersion":"1.0","_createdAt":1530494307142,"_lastModifiedAt":1530205026720,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refIcon":{},"_version":1,"color":"27dce0","launch":"Q3 2019+","launchOrder":1561935780000,"name":"Predict","overview":"A machine learning layer that predicts user behaviour based on past interactions.","slug":"predict","published":false,"_createdBy":"api-client","_id":"b3618f0a-f72c-44b3-95a4-7f304384fa5d","meta":{"revision":0,"created":1530494307143,"version":0},"$loki":1},{"_apiVersion":"1.0","_createdAt":1530494308155,"_lastModifiedAt":1530205006771,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refIcon":{},"_version":1,"color":"71afac","launch":"Q3 2019+","launchOrder":1561935840000,"name":"Match","overview":"Taxonomic framework for automated content classification through machine learning.","slug":"match","published":false,"_createdBy":"api-client","_id":"7e535457-c4db-490f-806c-a6af6b12bfec","meta":{"revision":0,"created":1530494308156,"version":0},"$loki":2},{"_apiVersion":"1.0","_createdAt":1530494309166,"_lastModifiedAt":1530204932831,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refIcon":{},"_version":1,"color":"5cc618","launch":"Q3 2019","launchOrder":1561935600000,"name":"Identity","overview":"CRM layer that works with anonymous and known records to make user data directly actionable.","slug":"identity","published":false,"_createdBy":"api-client","_id":"50fba93e-53bc-4ced-9ba1-d03eed2469dc","meta":{"revision":0,"created":1530494309167,"version":0},"$loki":3},{"_apiVersion":"1.0","_createdAt":1530494310180,"_lastModifiedAt":1530204843555,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refIcon":{},"_version":1,"color":"ef711d","launch":"Q3 2019+","launchOrder":1561935720000,"name":"Visualize","overview":"Data visualization for Identity and Track, but capable of taking data feeds from any source.","slug":"visualize","published":false,"_createdBy":"api-client","_id":"fd8de2f0-d781-4324-bcb5-42f37bce76ec","meta":{"revision":0,"created":1530494310181,"version":0},"$loki":4},{"_apiVersion":"1.0","_createdAt":1530494311189,"_lastModifiedAt":1530204814346,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refIcon":{},"_version":1,"color":"2dba7a","launch":"Q3 2019+","launchOrder":1561935660000,"name":"Track","overview":"Real-time, streaming data layer providing accurate metrics at individual and product level.","slug":"track","published":false,"_createdBy":"api-client","_id":"8929b56d-4527-48d6-a81d-b24761d4ebd2","meta":{"revision":0,"created":1530494311189,"version":0},"$loki":5},{"_apiVersion":"1.0","_createdAt":1530494312202,"_lastModifiedAt":1530093459589,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refIcon":{},"_version":1,"color":"dd4343","launch":"Q3 2018","launchOrder":1530399600000,"name":"Store","overview":"A cloud storage solution for all types of data, with built-in security, privacy and redundancy.","slug":"store","published":false,"_createdBy":"api-client","_id":"bece0859-2a89-4ba9-9b1c-d4badbd377ef","meta":{"revision":0,"created":1530494312203,"version":0},"$loki":6},{"_apiVersion":"1.0","_createdAt":1530494313223,"_lastModifiedAt":1530095193430,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refIcon":{},"_refImage":{},"_version":1,"color":"8773dd","description":"**DADI Queue** makes asynchronous task processing easy. It features a broker for queue management and routing, and a worker framework for processing messages. Being powered by RSMQ means it only needs a simple Redis instance to work.\n\n**Queue** has support for optional throttling to avoid congestion, as well as the ability to defer specific messages with scheduled processing. Interacting with Queue is as easy as using [@dadi/queue-wrapper](https://github.com/dadi/queue-wrapper), but you an also use libraries such as [rsmq-cli](https://github.com/mpneuried/rsmq-cli) and [rest-rsmq](https://github.com/smrchy/rest-rsmq).","features":"## Powered by Redis\n\nAs messages are stored in a Redis queue, all you need is a simple Redis instance to get started.\n\n## Super-fast async task processing\n\nBroker for queue management and routing, and a worker framework for processing messages.\n\n## Throttling support\n\nAvoid congestion with optional throttling.\n\n## Flexible logging \n\n **Queue** uses [@dadi/logger](https://github.com/dadi/logger) to provide flexible logging support.\n\n## Robust error handling\n\nQuickly debug issues in a production environment.\n\n## Lightweight, high-performance\n\n**Queue** provides high throughput and low latency by using Redis, a battle proven service.\n","github":"dadi/queue","headlineFeatures":"* Super-fast async task processing\n* Broker for queue management and routing\n* Worker framework for processing messages\n* Optional throttling to avoid congestion\n* Scheduled processing to defer specific messages","image-caption":"Example applications of Queue.","install":"https://docs.dadi.cloud/queue","launch":"Q3 2019+","launchOrder":1561935900000,"name":"Queue","npm":"@dadi/queue","overview":"A lightweight, RSMQ-powered asynchronous task queue featuring simple task routing and throttling.","published":true,"slug":"queue","_createdBy":"api-client","_id":"b3fbbacc-dcd6-45c2-b2ff-914aa1502506","meta":{"revision":0,"created":1530494313224,"version":0},"$loki":7},{"_apiVersion":"1.0","_createdAt":1530494314240,"_lastModifiedAt":1530204771803,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refIcon":{},"_refImage":{},"_version":1,"color":"e02975","demo":"https://docs.dadi.cloud/sandbox/dadi-cdn","description":"DADI CDN is analogous to a traditional CDN (Content Distribution Network) such as Akamai and Limelight. It is designed to carry the processing and delivery load associated with image manipulation and asset delivery (CSS/JS/fonts) and acts autonomously as a layer on top of your core product.\n\nCDN has full support for caching, header control, image manipulation, image compression and image format conversion. An authenticated API allows for fine grained cache control in the form of content invalidation on an individual file or collective path basis.","features":"## Crop based on subject matter\n\nLet CDN decide the focal point of the image and deliver an image containing only the essentials, cropped to your specifications.\n\n## Simplified media storage\n\nNo need to maintain multiple sizes of each image you upload into an article - CDN generates new variations from a single master image.\n\n## One CDN for all asset types\n\nStore and deliver font files and minify all your CSS and JavaScript assets for delivery.\n\n## Intelligent delivery\n\nLet CDN choose the best transformations based on device, network, location or language.\n\n## Delivery Recipes\n\nStore frequently used image manipulation parameters as a 'delivery recipe'.\n\n## Cache crops for faster delivery\n\nConverted images can be cached using a Redis server or the local filesystem, providing faster delivery on subsequent requests.","github":"dadi/cdn","headlineFeatures":"* Serve from Amazon S3, a remote server or the local filesystem\n* Dynamically generate images for responsive design and mobile devices\n* Extensive set of image enhancement and transformation tools\n* Image format conversion","image-caption":"CDN lets you edit images easily using query strings.","install":"https://docs.dadi.cloud/cdn","name":"CDN","npm":"@dadi/cdn","overview":"A just-in-time asset manipulation and delivery layer designed for faster content distribution.","published":true,"slug":"cdn","_createdBy":"api-client","_id":"4693150d-f75c-4563-bb2d-934214b9e9bc","meta":{"revision":0,"created":1530494314241,"version":0},"$loki":8},{"_apiVersion":"1.0","_createdAt":1530494315252,"_lastModifiedAt":1530204371193,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refIcon":{},"_refImage":{},"_version":1,"color":"0aa6dd","description":"DADI API is a rapid set-up data layer ready to act as the backbone to your digital platform. It is designed to be plugged into a templating layer, a mobile application or to be used with any other data consumer – and can be used alongside other apps in the DADI technology suite.\n\nIt can contain your business/domain logic (the part of a platform that encodes the real-world business rules that determine how data is created, displayed, stored and changed). DADI API allows you to get a complete data layer up and running in minutes.","features":"## Filter and sort your data with ease\n\nHarness the power of MongoDB-style querying to filter and sort your data.\n\n## Flexible database support\n\nSupport for multiple database connectors including MongoDB, CouchDB and RethinkDB.\n\n## Automatic database indexing\n\nIndexing your underlying database is as simple as adding configuration elements to collections.\n\n## Automatic input validation\n\nEasily configure fields within collections for input validation.\n\n## Collection-level access control\n\nWith a fine-grained authorisation layer, control who can access data and how.\n\n## Cache data for faster retrieval\n\nCache data using a Redis server or the local filesystem, providing faster response times.","github":"dadi/api","headlineFeatures":"* Filter and sort your data with ease\n* Flexible database support\n* Automatic document versioning\n* Built in support for oAuth2","image-caption":"Collections are defined quickly with JSON files.","install":"https://docs.dadi.cloud/api","launch":"Q4 2018","launchOrder":1538348400000,"name":"API","npm":"@dadi/api","overview":"A high-performance RESTful API layer designed in support of API-first development and COPE.","published":true,"slug":"api","_createdBy":"api-client","_id":"27d1f07c-a77c-4fea-af32-78680674c1f1","meta":{"revision":0,"created":1530494315253,"version":0},"$loki":9},{"_apiVersion":"1.0","_createdAt":1530494316266,"_lastModifiedAt":1530204340821,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refIcon":{},"_refImage":{},"_version":1,"color":"ffbf0e","description":"DADI Publish is developed to optimize editorial workflow with flexible interfaces that adapt to your requirements, meaning more time can be spent making content than wasting time managing it. It is built to support the principles of COPE (Create Once Publish Everywhere).\n\nJournalists and editors can seamlessly manage output across multiple products from one place, while engineers can make light work of even complex content collections. Publish currently works with DADI API but our development roadmap will soon allow use with a variety of data stores.","features":"## Centralized media management\n\nPower a suite of digital products from a single content store. Saves time and effort for engineers and editors alike.\n\n## Article publish scheduling\n\nPre-load content for automatic publishing at a time that suits the content, not your working day. Also create drafts for internal review.\n\n## Setup and connect in 30 seconds\n\nThe perfect partner for DADI API so you are up and running in days and weeks not weeks and months. Soon to work with other data stores.\n\n## Custom user permissions\n\nEnhanced features for large teams – keeping the right hands on the right parts of the system. Extends to support editorial workflow.\n\n## Collaborative document editing\n\nSee where colleagues are in shared articles and avoid treading on toes – work together seamlessly to update content.\n\n## Revision history with quick revert\n\nProvides a complete audit trail allowing you to restore to any point in time. Document differentials provide an overview of changes.","github":"dadi/publish","headlineFeatures":"* Extendable custom fields\n* Manage documents across multiple APIs\n* Intuitive in-browser editor with the simple drag and drop tool\n* Background tasks to manage big-data operations, analytics and much more","image-caption":"DADI uses Publish to power this site.","install":"https://docs.dadi.cloud/publish","launch":"Q1 2019","launchOrder":1546300800000,"name":"Publish","npm":"@dadi/publish","overview":"A writer’s window to the world of content creation. Flexible interfaces to optimize editorial workflow.","published":true,"slug":"publish","_createdBy":"api-client","_id":"20ccf255-452b-48fd-9a89-4afccfe4d8d7","meta":{"revision":0,"created":1530494316268,"version":0},"$loki":10},{"_apiVersion":"1.0","_createdAt":1530494317282,"_lastModifiedAt":1530205106731,"_lastModifiedBy":"cloud-stJ8lUzIWT","_refIcon":{},"_refImage":{},"_version":1,"color":"e42918","description":"DADI Web makes it easy to build custom enterprise-grade web applications. Easily create static pages or connect to APIs to generate data-driven apps giving you the power to search, paginate, sort and filter your data, on the server or in browser.\n\nDADI Web can use a variety of templating languages, providing a simple yet powerful template layer for displaying your data. These include LinkedIn’s Dust, Pug or Handlebars – or you can easily craft your own.","features":"## Flexible template engine support\n\nUse Web's default Dust.js template engine or swap in one for Pug.js or Handlebars.js – even craft your own!\n\n## URL Rewriting Support\n\nIncludes Apache-like URL rewriting, with full support for redirects and proxying.\n\n## Events Layer\n\nCreate custom JavaScript events to modify the request and response, or take action based on data loaded from a source.\n\n## Data from anywhere\n\nYou're not limited to using DADI API - serve data from static Markdown files or retrieve feeds from Twitter and Wordpress.\n\n## Preload Data\n\nSpeed up access to commonly required data by preloading it when the application boots.\n\n## Cache pages for faster retrieval\n\nCache data using a Redis server or the local filesystem, providing faster response times to users. Includes full support for Cache-Control headers.\n\n","github":"dadi/web","headlineFeatures":"* Connect with REST APIs to generate data-driven pages\n* Search, paginate, sort and filter your data\n* Easily create static pages\n* Support for database-backed sessions","image-caption":"Use web to quickly get a front-end up and running.","install":"https://docs.dadi.cloud/web","launch":"Q2 2019","launchOrder":1554073200000,"name":"Web","npm":"@dadi/web","overview":"A schemaless templating layer that can operate as a standalone platform or with DADI API.","published":true,"slug":"web","_createdBy":"api-client","_id":"f0d957ef-7cc5-4c71-bbb6-978c1cb43951","meta":{"revision":0,"created":1530494317283,"version":0},"$loki":11}],"idIndex":[1,2,3,4,5,6,7,8,9,10,11],"binaryIndices":{"order":{"name":"order","dirty":false,"values":[10,9,8,7,6,5,4,3,2,1,0]}},"constraints":null,"uniqueNames":[],"transforms":{},"objType":"web-services","dirty":false,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":false,"cloneMethod":"parse-stringify","asyncListeners":false,"disableChangesApi":true,"autoupdate":false,"ttl":null,"maxId":11,"DynamicViews":[],"events":{"insert":[null],"update":[null],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[]},{"name":"tokenData","data":[],"idIndex":[],"binaryIndices":{"updatedAt":{"name":"updatedAt","dirty":false,"values":[]}},"constraints":null,"uniqueNames":[],"transforms":{},"objType":"tokenData","dirty":false,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":false,"cloneMethod":"parse-stringify","asyncListeners":false,"disableChangesApi":true,"autoupdate":false,"ttl":null,"maxId":0,"DynamicViews":[],"events":{"insert":[null],"update":[null],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[]},{"name":"categories","data":[{"_apiVersion":"1.0","_createdAt":1530494249518,"_lastModifiedAt":1526566826328,"_lastModifiedBy":"cloud-client","_version":1,"desc":"Information, core principles and general light reading on DADI technology","icon":"light-bulb","name":"Knowledge","slug":"knowledge","_createdBy":"api-client","_id":"6d898244-6df5-40ed-adaf-7378f1fd0a55","meta":{"revision":0,"created":1530494249524,"version":0},"$loki":1},{"_apiVersion":"1.0","_createdAt":1530494250535,"_lastModifiedAt":1526566888380,"_lastModifiedBy":"cloud-client","_version":1,"desc":"Step-by-step guides and practical examples of our technology written by our team","icon":"chat-star","name":"Tutorials","slug":"tutorials","_createdBy":"api-client","_id":"3bf24914-17bd-4116-9412-b7b76fc8f0f6","meta":{"revision":0,"created":1530494250536,"version":0},"$loki":2},{"_apiVersion":"1.0","_createdAt":1530494251549,"_lastModifiedAt":1526567004196,"_lastModifiedBy":"cloud-client","_version":1,"desc":"Roadmap milestones, announcements and DADI product launches","icon":"dial","name":"Updates","slug":"updates","_createdBy":"api-client","_id":"0eda8d2c-e76b-484e-a95b-6ea4b3b1eaa3","meta":{"revision":0,"created":1530494251549,"version":0},"$loki":3}],"idIndex":[1,2,3],"binaryIndices":{},"constraints":null,"uniqueNames":[],"transforms":{},"objType":"categories","dirty":false,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":false,"cloneMethod":"parse-stringify","asyncListeners":false,"disableChangesApi":true,"autoupdate":false,"ttl":null,"maxId":3,"DynamicViews":[],"events":{"insert":[null],"update":[null],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[]},{"name":"pages","data":[{"_apiVersion":"1.0","_createdAt":1530494257589,"_version":1,"excerpt":"DADI has assembled a world class engineering, strategic and design team with over 300 years of collective experience. The team is 18 strong – all full time. DADI today represents the culminiation of four years of R&D by the team and $2 million of direct investment by the founders.","icon":"group","published":true,"slug":"the-team","title":"The Team","_createdBy":"api-client","_id":"aee25dda-0c92-4a1c-9c2e-03c952790d15","meta":{"revision":0,"created":1530494257590,"version":0},"$loki":1},{"_apiVersion":"1.0","_createdAt":1530494258603,"_lastModifiedAt":1523525283352,"_version":1,"body":"My partners and I have been running businesses of various sizes for over fifteen years, from a two man operation to organisations with global scale and reach.\n\nDespite huge differences in scale and offering, the physical structures of work remained constant throughout. We had an office. We had a working day. We structured teams around disciplines. We had regular meetings. Basically everything that jumps to mind when you hear the word \"work\".\n\nThat's not to say that it wasn't fun. This is our office on the curtain road, in the early days of Airlock (look at the size of some of those monitors!) -\n\n![Our old office](/assets/how-we-work/airlock.jpg)\n\n...but we structured our lives around our work: pulling all nighters was common and interruptions to holiday a given.\n\nWhen we started DADI we decided to rip up everything we thought we knew about work. We started again from scratch, structuring everything around one core principle -\n\n**Work should fit around our lives, not the other way round.**\n\n## Dropping the office\n\nThe most obvious change for us was also the hardest: getting rid of our office.\n\nThere is an incredibly strong link between environment and work. If you spend your life \"leaving for work\", taking away a physical destination asks some serious questions about your definition and understanding of the very concept of work.\n\nWhich is of course exactly what we wanted to challenge.\n\nAt our last business we choose our offices based on finding the most equal commute for all of our staff. In practice this meant that the average trip into the office was a little over an hour. That's over a day a week spent commuting for every single person in the company: 300 hours a week lost to the underground and the pollution of central London.\n\nEven if you can't see the obvious human strain that this represents, consider it financially: 300 hours a week is time worth around £100,000 a month. That's not to say that this is time that would otherwise be worked, simply that the collective time lost to the commute can be staggering high.\n\nWe also noticed a trend that will be familiar to many: everyone wearing large headphones in an open plan office.\n\nTechnology started significantly and noticeably shifting the way that we work a decade or so ago. It is now common to use instant messaging to speak with the person two desks down from you, rather than actually talking to them. This along with the prevalence of headphones is a signifier of change in working practices; a move away from a traditional, synchronous method of work to one of asynchronicity.\n\nNot having a central office embraces this change. It enables us to work in entirely new ways, and has removed the traditional structures and politics that come with location-based working. For us, everything is asynchronous.\n\nThis change has also enabled us to think about hiring entirely differently. It is a false conceit to think that the best coders, designers and strategists live within an hour of central London. Not having an office means that we can look far and wide for the best possible talent.\n\nWe have team members who prefer a traditional working environment for whom we provide cover for the rental of a shared office space. We have team members who cafe hop. We have team members who have a home office (I'm sat in mine as I type). We have team members in London. We have team members outside of London. We have team members in different countries.\n\nAnd of course it is impossible to generalise about the time of day that we are at our most productive. A commute is an avoidable disruption, just like the time we choose to eat or whether or not we put on clothes. Giving individuals the freedom to organise their time according to their own productivity peaks and troughs avoids the breaking of a train of thought in order to jump on a train.\n\n### Extreme remoting\n\nThis is Jim -\n\n![Mr James Lambie](/assets/how-we-work/jim.jpg)\n\nHe's one of our Lead Developers. Tomorrow he leaves for the first day of a 15,000 mile - ! - bike ride that will take him all the way to New Zealand. He's starting in Brighton.\n\nWhile he'll be on the road for the majority of the time, he is staying on as part of our team, leading the development on some of our core underlying technologies. We like to think that he'll be cycling with one hand on the handlebars and one hand typing furiously on a laptop strapped to the side of his bike, but in reality he'll be contributing in fits and starts, working blocks of time from key cities en route, predominantly through the cold winter months.\n\nThis is an extreme example of what asynchronous working enables, but it proves the point. There is no way that this trip could have been accommodated at our last agency.\n\nYou can follow Jim's amazing journey on [agreatdistance.org](http://www.agreatdistance.org), which includes a wealth of data, from his current coordinates - useful if you want to wave him on! - to the distance he has travelled.\n\n## Meetings are still ███████ toxic\n\n37signals originated \"meetings are toxic\" in their book, [Getting Real](https://gettingreal.37signals.com/).\n\nMeetings are traditionally called when something needs hashing out. They tend to invite more people than necessary, reflect the hierarchy and politics within the organisation and usually last much longer than necessary.\n\nIn short, meetings kill productivity.\n\nIn technology it makes far more sense and is far more productive to just get on with it - to push a branch up, check out the diff, and then iterate on that diff rather than to assume that you're going to perfectly whiteboard a system architecture ahead of time.\n\nWhat is true of programming is true for every other aspect of business.\n\nEven if you take notes in a meeting, you can't capture everything. You make a judgement at a point in time as to what to write down. Then a week later you try to remember what wasn't written down as it becomes apparent that that portion of the discussion was more important.\n\nYou don't have this problem with chat transcripts. Forcing people to reduce their otherwise rambling thoughts into concrete sentences helps focus discussion.\n\nOf course we still have meetings at DADI from time to time, but they are the exception rather than the rule.\n\n### Rules to live by\n\n* Do not attend any meeting without an agenda\n* If a decision can be taken over phone or by chat, do so instead of arranging or attending a meeting\n* No meeting should be more than 30 minutes in length\n* Only the most important people should attend a meeting. Others can be informed by email\n* Never hold meetings to review output. Reviews should be handled offline first, with a meeting for discussion of the output only held where absolutely necessary\n\n## Working asynchronously\n\nA synchronous working style is one where some or all of the members of a team regularly stop what they’re doing, meet, discuss and agree on the next steps to take, before getting back to work implementing those steps. These synchronous \"join points\" are frequent, unavoidable, and built into the structure of the work.\n\nAn asynchronous working style is one where the entire team rarely, if ever, gets together for big agreements and discussions. Each individual team member more or less works on their own and at their own pace, with collaboration and agreement being handled by asynchronous mechanisms.\n\n![The future of work](/assets/how-we-work/working-asynchronously.png)\n\nAsynchronous working centres around shared artefacts, with each member of a team gradually accreting layers and complexity onto those artefacts. The most common example is commits to a source code repository. Another good example is online document sharing, with multiple editors and commenters. Team members usually have multiple related threads of work running simultaneously, so that if one is blocked on input from someone else, another thread can be picked up in order to maintain forward momentum.\n\nWorking asynchronously enables our team to be geographically distributed and to choose the hours that suit them the best.\n\nIn short it enables our team to put their lives first.\n\n## The right tools for the job\n\nOf course none of this would work without the right tools in support.\n\nAfter our staff, the most important aspect of how we work is the tools that we use on a daily basis. In the event that a particular tool doesn't work for us, we'll move on to something else. We think about this stuff frequently. Are we still working well together today, even though we are double the size we were six months ago?\n\nThe tools make the difference.\n\n### Slack\n\nSlack is probably most important tool in use at DADI. It's a chat platform organised around public and private channels.\n\nSlack is asynchronous. Asynchronous communication means that I can step out for lunch and catch up on transcripts when I get back. It means that I can ask a colleague a question and not worry about bothering her, as she'll get back to me when she's available. Asynchronous communication means I can go to rural Somerset and work a normal day.\n\n![Slack](/assets/how-we-work/slack.png)\n\nFind out more: [slack.com](https://slack.com/)\n\n### Github\n\nGithub allows you to collaborate on anything. It's a version control system, but one that doesn't suck.\n\nWe predominantly use it for source code control, but are increasingly using it for everything else as well.\n\n![Github](/assets/how-we-work/github.png)\n\nLearn more: [github.com](https://github.com/)\n\n## The result of all this?\n\nWe are more productive than we ever have been. Startlingly so. And we are producing a standard of work way and above even our highest expectations. Change is good.\n\n## Join us\n\nInterested in shaking up your working life? We're hiring. [Find out more](/careers).","excerpt":"At DADI we've ripped up everything we thought we knew about work and have started again, structuring the business around how we want to live.","icon":"favorite","published":true,"slug":"culture","title":"Culture","_createdBy":"api-client","_id":"17f36f34-2b9d-4847-830c-d3d981596a6f","meta":{"revision":0,"created":1530494258604,"version":0},"$loki":2},{"_apiVersion":"1.0","_createdAt":1530494259627,"_lastModifiedAt":1520505541443,"_version":1,"excerpt":"DADI is a global, decentralized cloud platform with a suite of web services to help you build, launch and grow your digital products.","icon":"globe","published":true,"slug":"network","title":"Network","_createdBy":"api-client","_id":"73d8bb4f-9be7-4498-ad68-e0447e0e66ff","meta":{"revision":0,"created":1530494259628,"version":0},"$loki":3},{"_apiVersion":"1.0","_createdAt":1530494260645,"_version":1,"excerpt":"DADI is working with a select group of partners and advisors from relevant industries to bring experience and consultation to its revolutionary new decentralized cloud services platform.","icon":"cup","published":true,"slug":"partners","title":"Partners","_createdBy":"api-client","_id":"ff7751e9-b9a0-48fc-b352-a9c948edb75f","meta":{"revision":0,"created":1530494260645,"version":0},"$loki":4},{"_apiVersion":"1.0","_createdAt":1530494261658,"_lastModifiedAt":1526565300838,"_lastModifiedBy":"cloud-client","_version":1,"excerpt":"Everything you need to know to be a part of the DADI network – or to build amazing digital products with our web services","icon":"group","published":true,"slug":"community","title":"Community","_createdBy":"api-client","_id":"3f719881-adff-4a31-b7e5-49c1e768e731","meta":{"revision":0,"created":1530494261659,"version":0},"$loki":5},{"_apiVersion":"1.0","_createdAt":1530494262670,"_lastModifiedAt":1526911965530,"_lastModifiedBy":"cloudlive","_version":1,"body":"DADI is expanding and we currently have open positions for:\n\n• Full stack engineers\n• Front end engineers\n• New business manager\n• Marketing director\n• Senior PHP+Node engineer\n• Product manager\n\nApplication form and more details [this way](http://dadi.link/lX).","excerpt":"Join the DADI team.","icon":"star","published":true,"slug":"careers","title":"Careers","_createdBy":"api-client","_id":"d2193f73-6ed8-4142-a84e-ac0b30bc12e8","meta":{"revision":0,"created":1530494262671,"version":0},"$loki":6},{"_apiVersion":"1.0","_createdAt":1530494263683,"_lastModifiedAt":1527866966207,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"body":"## 1. INTRODUCTION \n\n1.1 This privacy notice (Privacy Notice) sets out the ways in which we, DADI Cloud Limited (we, us, our), collect and use your personal data (your personal information) in connection with our business. It also explains what rights you have to access or change your personal data. \n\n1.2 Our website is not intended for children. We do not knowingly collect or maintain the personal information of children under the age of 18. If you are under the age of 18, please do not access our website at any time or in any manner. We will take appropriate steps to delete the personal information of persons under the age of 18. \n\n## 2. ABOUT US\n\n2.1 We are a company registered in England under company number 08599900, with our registered address as set out below. \n\n2.2 You can contact us as follows:\n\n**FAO:**      Data Information Manager\n**Address:**  7 Marylebone Lane, London, W1U 1DF\n**Email:**    data@dadi.cloud \n\n## 3. INFORMATION WE MAY COLLECT ABOUT YOU\n\n### 3.1 Information that you provide to us. \n\n3.1.1 We will collect any information that you provide to us when you:\n\n* (a) make an enquiry, provide feedback or make a complaint over the phone, by email or on our website – whether that is about our services, a Token sale, or any other matter;\n\n* (b) submit correspondence to us by post, email or via our website;\n\n* (c) update your profile;\n\n* (d) subscribe to our newsletter and mailing lists;\n\n* (e) fill in a form, conduct a search, respond to surveys, participate in promotions or use any other features of the website;\n\n* (f) register to and/or attend our events;\n\n* (g) submit a CV or an application to a job vacancy;\n\n* (h) attend an interview or assessment; and\n\n* (i) ‘follow’, ‘like’, post to or interact with our digital platforms, blogs and social media accounts, including GitHub, Twitter, Telegram, Reddit, Medium and Discord.\n\n3.1.2 The information you provide to us will include (depending on the circumstances):\n\n#### (a) Identity and contact data:\n\ntitle, names, addresses, email addresses and phone numbers, and we may also collect proof of identity (for Know Your Customer regulations) if applicable;\n\n#### (b) Employment and background data:\n\nIf you are submitting a job application, you may also provide additional information about your academic and work history, qualifications, skills, projects and research that you are involved in, references, proof of your entitlement to work in the UK, your national security number, your passport or other identity document details and any other such similar information that you may provide to us; and\n\n#### (c) Survey data:\n\nfrom time to time we might ask if you would be willing to participate in our surveys; if you agree, we will also collect any information that you provide as part of that survey.\n\n### 3.1.3 Information we collect about you:\n\n#### Information contained in correspondence:\n\nwe will collect any information contained in any correspondence between us. For example, if you contact us using a query button on our website or by email or telephone, we may keep a record of that correspondence;\n\n#### Information transmitted on the website:\n\nwe will collect information that you upload or post to your website account and/or any correspondence or interactions that you may have with other website users;\n\n#### Website usage and technical data:\n\nwe will also collect certain information about how you use our website and the device that you use to access our website, even where you have not created an account or logged in. This might include your geographical location, device information (such as your hardware model, mobile network information, unique device identifiers), the data transmitted by your browser (such as your IP address, date and type of the request, content of the request regarding the specific site, time zone settings, access status/HTTP status code, volume of data transmitted, browser type and version, language settings, time zone settings referral source, length of visit to the website, date and time of the request, operating system and interface) number of page views, the search queries you make on the website and similar information. This information may be collected by a third-party website analytics service provider on our behalf and/or may be collected using cookies or similar technologies. For more information on cookies please read the COOKIES section below.\n\n### 3.2 Special Category Information \n\n3.2.1 “Special categories” of particularly sensitive personal information require higher levels of protection. Special categories of data include details about your race or ethnicity, religious or philosophical beliefs, sex life, sexual orientation, political opinions, trade union membership, information about your health and genetic and biometric data.\n\n3.2.2 We need to have further justification for collecting, storing and using this type of personal information. We have in place an appropriate policy document and safeguards which we are required by law to maintain when processing such data. We process special categories of personal information in the following circumstances: \n\n3.2.3 If you are submitting a job application, you may provide information about:\n\n* (a) your race, ethnicity, religious or philosophical beliefs and sexual orientation for the purpose of our diversity and equal opportunities records (on the basis that it is needed for reasons of substantial public interest, for equal opportunities monitoring); and\n\n* (b) your health as necessary for the purpose of arranging your interview if you are an applicant (on the basis of compliance with our legal obligations to make reasonable adjustments for your circumstances).\n\n### 3.3 Information we receive from third parties\n\n3.3.1 In certain circumstances, we will receive information about you from third parties. For example: \n\n* (a) **Service providers:** we may collect personal information from our IT support provider, ZenDesk who as based in the US outside the EU;\n\n* (b) **Fraud detection agencies:** Where permitted or required by law, we may receive information about you, including demographic data or fraud detection information from third party service providers and/or partners, for instance to carry out identity checks through Trulioo, who are based outside the EU in Canada;\n\n* (c) **Employers, recruitment agencies and referees:** if you are a job applicant we may contact your recruiter, current and former employers and/or referees, who may be based inside or outside the EU, to provide information about you and your application; and\n\n3.3.2 We might also receive information about you from third parties if you have indicated to such third party that you would like to hear from us. \n\n## 4. HOW WE USE INFORMATION ABOUT YOU AND RECIPIENTS OF YOUR INFORMATION\n\n### 4.1 We will use your information for the purposes listed below either on the basis of: \n\n4.1.1 performance of your contract with us and the provision of our services to you; \n\n4.1.2 your consent (where we request it); \n\n4.1.3 where we need to comply with a legal or regulatory obligation; or\n\n4.1.4 our legitimate interests or those of a third party (see section 4 below). \n\n### 4.2 We use your information for the following purposes:\n\n#### 4.2.1 To provide access to our website:\n\nto provide you with access to our website in a manner convenient and optimal and with personalised content relevant to you including sharing your information with our website hosts and developers (on the basis of our legitimate interest to ensure our website is presented in an effective and optimal manner);\n\n#### 4.2.2 To register your account:\n\nwhen you sign up to use our website or otherwise register for our Token sale, we will use the details provided on your account registration form (on the basis of performing our contract with you);\n\n#### 4.2.3 To process and facilitate transactions with us:\n\nwe will use your information to process transactions and payments, and to collect and recover money owed to us (on the basis of performing our contract with you and on the basis of our legitimate interest to recover debts due);\n\n#### 4.2.4 Relationship management:\n\nto manage our relationship with you, which will include notifying you about changes to our terms of use or privacy notice, and asking you to leave a review or take a survey (on the basis of performing our contract with you, to comply with our legal obligations and on the basis of our legitimate interests to keep our records updated and study how our website and services are used);\n\n#### 4.2.5 User and customer support:\n\nto provide customer service and support (on the basis of our contract with you or on the basis of our legitimate interests to provide you with customer service), deal with enquiries or complaints about the website and share your information with our website developer, IT support provider, payment services provider [insert details of any other service provider] as necessary to provide customer support (on the basis of our legitimate interest in providing the correct products and services to our website users and to comply with our legal obligations);\n\n#### 4.2.6 Prize draws, competitions and surveys:\n\nto enable you to take part in prize draws, competitions and surveys (on the basis of performing our contract with you and our legitimate interest in studying how our website and services are used, to develop them and grow our business);\n\n#### 4.2.7 Recruitment:\n\nto process any job applications you submit to us, whether directly or via an agent or recruiter including sharing this with our third party recruitment agency (on the basis of our legitimate interest to recruit new employees or contractors);\n\n#### 4.2.8 Marketing:\n\nto keep in contact with you about our news, events, new website features products or services that we believe may interest you, provided that we have the requisite permission to do so, and sharing your information with our email marketing services provide Mailchimp (either on the basis of your consent where we have requested it, or our legitimate interests to provide you with marketing communications where we may lawfully do so);\n\n#### 4.2.9 Advertising:\n\nto deliver relevant website content and advertisements to you and measure or understand the effectiveness of the advertising we serve to you (on the basis of our legitimate interests in studying how our website/services are used, to develop them, to grow our business and to inform our marketing strategy);\n\n#### 4.2.10 Social media interactions:\n\nto interact with users on social media platforms including Twitter, Telegram, and Discord for example, responding to comments and messages, posting, ‘retweeting’ and ‘liking’ posts (on the basis of our legitimate interest in promoting our brand and communicating with interested individuals);\n\n#### 4.2.11 Analytics:\n\nto use data analytics to improve our website, products/services, marketing, customer relationships and experiences (on the basis of our legitimate interests in defining types of customers for our website and services, to keep our website updated and relevant, to develop our business and to inform our marketing strategy);\n\n#### 4.2.12 Suggestions and recommendations:\n\nto share your information with selected third parties such as suppliers and partners, to enable them to contact you with information about things that may interest you (where we have your consent to do so);\n\n#### 4.2.13 Research:\n\nto carry out aggregated and anonymised research about general engagement with our website (on the basis of our legitimate interest in providing the right kinds of products and services to our website users);\n\n#### 4.2.14 Fraud and unlawful activity detection:\n\nto protect, investigate, and deter against fraudulent, unauthorised, or illegal activity, including identity fraud (on the basis of our legitimate interests to operate a safe and lawful business or where we have a legal obligation to do so); and\n\n#### 4.2.15 Compliance with policies, procedures and laws:\n\nto enable us to comply with our policies and procedures and enforce our legal rights, or to protect the rights, property or safety of our employees and share your information with our technical and legal advisors (on the basis of our legitimate interests to operate a safe and lawful business or where we have a legal obligation to do so).\n\n4.3 As outlined above, in certain circumstances we may use your personal information to pursue legitimate interests of our own or those of third parties. Where we refer to using your information on the basis of our “legitimate interests”, we mean our legitimate business interests in conducting and managing our business and our relationship with you, including the legitimate interest we have in:\n\n* 4.3.1 personalising, enhancing, modifying or otherwise improving the services and/or communications that we provide to you; \n\n* 4.3.2 detecting and preventing fraud and operating a safe and lawful business; and\n\n* 4.3.3 improving security and optimisation of our network, sites and services.\n\n4.4 Where we use your information for our legitimate interests, we make sure that we take into account any potential impact that such use may have on you. Our legitimate interests don’t automatically override yours and we won’t use your information if we believe your interests should override ours unless we have other grounds to do so (such as your consent or a legal obligation). If you have any concerns about our processing please refer to details of “Your Rights” in paragraph 9 below. \n\n## 5. WHO WE MIGHT SHARE YOUR INFORMATION WITH\n\n5.1 In connection with the purposes and on the lawful grounds described above and in addition to the recipients of your information as described above, we will share your personal information when relevant with third parties such as:\n\n### 5.1.1 Our service providers:\n\nservice providers we work with to deliver our business, who are acting as processors and provide us with:\n\n* (a) IT, support, system administration and security services (including ZenDesk) based in the US; \n\n* (b) marketing and advertising services (including the Google AdWords service), analytics providers (including Google Analytics), email marketing services (including Mailchimp) all based in the US;\n\n* (c) identity verification, fraud prevention and detection services based Canada; and\n\n* (d) legal, accountancy, auditing and insurance services and other professional advisers based in the United Kingdom.\n\n### 5.1.2 Corporate affiliates:\n\nother companies of the DADI group, including but not limited to DADI+ Limited and DADI Tech Limited, based in the UK;\n\n### 5.1.3 Regulators and governmental bodies:\n\nHM Revenue & Customs, regulators, governmental bodies and other authorities acting as processors or joint controllers based in the United Kingdom, who require reporting of processing activities in certain circumstances;\n\n### 5.1.4 Marketing parties:\n\nany selected third party that you consent to our sharing your information with for marketing purposes; and\n\n### 5.1.5 Other third parties (including professional advisers):\n\nany other third parties (including legal or other advisors, regulatory authorities, courts, law enforcement agencies and government agencies) based in the United Kingdom where necessary to enable us to enforce our legal rights, or to protect the rights, property or safety of our employees or where such disclosure may be permitted or required by law.\n\n5.2 We require third parties to maintain appropriate security to protect your information from unauthorised access or processing.\n\n## 6. COOKIES\n\n6.1 We use cookies to ensure that you get the most out of our website. Cookies are small amounts of information in the form of text files which we store on the device you use to access our website. Cookies allow us to monitor your use of the software and simplify your use of the website. For example, a temporary cookie is also used to keep track of your \"session\". Without that temporary cookie you would not be able to purchase goods via our website.\n\n6.2 If you do not wish for cookies to be installed on your device, you can change the settings on your browser or device to reject cookies. For more information about how to reject cookies using your internet browser settings please consult the “Help” section of your internet browser (or alternatively visit http://www.aboutcookies.org). Please note that, if you do set your Internet browser to reject cookies, you may not be able to access all of the functions of the website. \n\n6.3 The names of the cookies used on our website and the purposes for which these cookies are used are set out in the table below:\n\n|Cookie name|Purpose|Duration|\n|---|---|---|\n|Google analytics|Analysis and tracking of users on the website|Session|\n|DADI| Anonymous tracking of users|Session|\n\n6.4 Our website may contain content and links to other sites that are operated by third parties that may also operate cookies. We don’t control these third party sites or cookies and this Privacy Notice does not apply to them. Please consult the terms and conditions and Privacy Notice of the relevant third party site to find out how that site collects and uses your information and to establish whether and for what purpose they use cookies.\n\n## 7. HOW WE LOOK AFTER YOUR INFORMATION AND HOW LONG WE KEEP IT FOR\n\n7.1 We operate a policy of “privacy by design” by looking for opportunities to minimise the amount of personal information we hold about you. We use appropriate technological and operational security measures to protect your information against any unauthorised access or unlawful use, such as: \n\n* 7.1.1 ensuring the physical security of our offices, warehouses  or other sites;\n\n* 7.1.2 ensuring the physical and digital security of our equipment and devices by using appropriate password protection and encryption;\n\n* 7.1.3 maintaining a data protection policy for, and delivering data protection training to, our employees; and\n\n* 7.1.4 limiting access to your personal information to those in our company who need to use it in the course of their work. \n\n7.2 We will retain your information for as long as is necessary to provide you with the services that you have requested from us or for as long as we reasonably require to retain the information for our lawful business purposes, such as for the purposes of exercising our legal rights or where we are permitted to do. We operate a data retention policy and look to find ways to reduce the amount of information we hold about you and the length of time that we need to keep it. For example:\n\n* 7.2.1 we archive our email and paper correspondence regularly and destroy information older than 5 years;\n\n* 7.2.2 we retain information relating to website user queries for approximately 90 days;\n\n* 7.2.3 we retain information relating to identity checks our Token sale as long as is necessary to comply with any requirements around “know your customer” regulations where DADI tokens purchased and for those not successful in purchasing DADI tokens we destroy information 30 days after the end of the Token sale; and\n\n7.3 we maintain a suppression list of email addresses of individuals who no longer wish to be contacted by us. So that we can comply with their wishes we must store this information permanently.\n\n## 8. HELP KEEP YOUR INFORMATION SAFE\n\n8.1 You can also play a part in keeping your information safe by:\n\n8.1.1 choosing a strong account password and changing it regularly;\n\n8.1.2 using different passwords for different online accounts;\n\n8.1.3 keeping your login and password confidential and avoiding sharing these details with others;\n\n8.1.4 making sure you log out of the website each time you have finished using it. This is particularly important when using a shared computer;\n\n8.1.5 letting us know if you know or suspect that your account has been compromised, or if someone has accessed your account without your permission;\n\n8.1.6 keeping your devices protected by using the latest version of your operating system and maintaining any necessary anti-virus software; \n\n8.1.7 being vigilant to any fraudulent emails that may appear to be from us. Any emails that we send will come from an email address ending in @dadi.cloud; and\n\n## 9. INTERNATIONAL TRANSFERS OF YOUR INFORMATION\n\n9.1 Our company is located in the UK.\n\n9.2 Some of our external third parties are based outside the European Economic Area (EEA) so their processing of your personal data will involve a transfer of data outside the EEA. \n\n9.3 Whenever we transfer your personal data out of the EEA, we ensure a similar degree of protection is afforded to it by ensuring at least one of the following transfer solutions are implemented:\n\n* (a) we will only transfer your personal data to countries that have been deemed to provide an adequate level of protection for personal data by the European Commission. For further details, see European Commission: Adequacy of the protection of personal data in non-EU countries; \n\n* (b) where we use certain service providers, we may use specific contracts approved by the European Commission which give personal data the same protection it has in Europe. For further details, European Commission: Model contracts for the transfer of personal data to third countries; or\n\n* (c) where we use providers based in the US, we may transfer data to them if they are part of the Privacy Shield which requires them to provide similar protection to personal data shared between the Europe and the US. For further details, see European Commission: EU-US Privacy Shield.\n\n9.4 Countries outside of the EEA to which we may transfer your personal information includes the United States\n\n9.5 Please contact us using the contact details at the top of this Privacy Notice if you want further information on the specific mechanism used by us when transferring your personal data out of the EEA.\n\n## 10. YOUR RIGHTS TO THE INFORMATION WE HOLD ABOUT YOU  \n\n10.1 You have certain rights in respect of the information that we hold about you, including: \n\n10.1.1 the right to be informed of the ways in which we use your information, as we seek to do in this Privacy Notice;\n\n10.1.2 the right to ask us not to process your personal data for marketing purposes; \n\n10.1.3 the right to request access to the information that we hold about you; \n\n10.1.4 the right to request that we correct or rectify any information that we hold about you which is out of date or incorrect; \n\n10.1.5 the right to withdraw your consent for our use of your information in reliance of your consent (refer to section 2 above to see when we are relying on your consent), which you can do by contacting us using any of the details at the top of this Privacy Notice; \n\n10.1.6 the right to object to our using your information on the basis of our legitimate interests (refer to section 2 above to see when we are relying on our legitimate interests) (or those of a third party)) and there is something about your particular situation which makes you want to object to processing on this ground;\n\n10.1.7 the right to receive a copy of any information we hold about you (or request that we transfer this to another service provider) in a structured, commonly-used, machine readable format, in certain circumstances;\n\n10.1.8 in certain circumstances, the right to ask us to limit or cease processing or erase information we hold about you; and\n\n10.1.9 the right to lodge a complaint about us to the UK Information Commissioner’s Office (https://ico.org.uk/) as well as a right to lodge a complaint with the relevant authority in your country of work or residence.\n\n### 10.2 How to exercise your rights\n\n10.2.1 You may exercise your rights above by contacting us using the details in paragraph 2 of this Privacy Notice, or in the case of preventing processing for marketing activities also by unsubscribing using the links provided.\n\n10.2.2 Please note that we may need to retain certain information for our own record-keeping and research purposes. We may also need to send you service-related communications relating to your user account even when you have requested not to receive marketing communications. \n\n### 10.3 What we need from you to process your requests\n\n10.3.1 We may need to request specific information from you to help us confirm your identity and to enable you to access your personal data (or to exercise any of your other rights). This is a security measure to ensure that personal data is not disclosed to any person who has no right to receive it. We may also contact you to ask you for further information in relation to your request to speed up our response.\n\n10.3.2 You will not have to pay a fee to access your personal data (or to exercise any of the other rights). However, we may charge a reasonable fee if your request is clearly unfounded, repetitive or excessive. Alternatively, we may refuse to comply with your request in these circumstances. We will try to respond to all legitimate requests within one month. Occasionally it may take us longer than a month if your request is particularly complex or you have made a number of requests. In this case, we will notify you and keep you updated.\n\n## 11. SHARING DATA DIRECTLY WITH THIRD PARTIES \n\n11.1 You might end up providing personal information directly to third parties as a consequence of your interactions with our website and other services offered by us. For example, you may attend an event hosted by us where you communicate personal information directly with other attendees. We are not responsible for how such third parties use personal data provided by you.  \n\n11.2 Please be responsible with personal information of others when using our website and the services available on it. We are not responsible for your misuse of personal information, or for the direct relationship between you and others when takes place outside of the website or our services. \n\n## 12. THIRD-PARTY LINKS  \n\nThe website may include links to third-party websites, plug-ins and applications. Clicking on those links or enabling those connections may allow third parties to collect or share data about you. We do not control these third-party websites and are not responsible for their privacy statements. When you leave our website, we encourage you to read the privacy notice of every website you visit.\n\n## 13. CHANGES TO THIS PRIVACY NOTICE AND YOUR DUTY TO INFORM US OF CHANGES\n\n13.1 We may make changes to this Privacy Notice from time to time. We will post any changes to our site, or notify you of any material changes by e-mail.\n\n13.2 It is important that the personal data we hold about you is accurate and current. Please keep us informed if your personal data changes during your relationship with us by updating your profile account information or contacting us via the contact details at the top of this Privacy Notice.\n\n**This Privacy Notice was updated on 24th May 2018.**","excerpt":"How we approach storing your data.","published":true,"slug":"privacy-policy","title":"Privacy Policy","_createdBy":"api-client","_id":"b9461402-adec-479f-a5ff-3321146c02e0","meta":{"revision":0,"created":1530494263683,"version":0},"$loki":7},{"_apiVersion":"1.0","_createdAt":1530494264694,"_lastModifiedAt":1526978535067,"_lastModifiedBy":"cloudlive","_version":1,"excerpt":"## Fast, scalable, secure and inexpensive web services. This is the cloud, decentralized.","metaDescription":"DADI decentralized web services: a new era of cloud computing services, powered by blockchain technology.","metaTitle":"Decentralized web services","published":true,"slug":"index","title":"Index","_createdBy":"api-client","_id":"4dd6fe1e-843f-4dfe-9cf1-3f420876c461","meta":{"revision":0,"created":1530494264695,"version":0},"$loki":8},{"_apiVersion":"1.0","_createdAt":1530494265707,"_lastModifiedAt":1525791679329,"_lastModifiedBy":"cloud-client","_version":1,"body":"","excerpt":"DADI uses its own ERC20 token (DADI), an integral part of the DADI Cloud network. Consumers will be charged tokens for their usage of DADI Cloud services, contributors can earn them by offering up computing power.","icon":"coins","published":true,"slug":"token","title":"Token","_createdBy":"api-client","_id":"4b60f047-eed6-4354-9e00-07b3a22d14b2","meta":{"revision":0,"created":1530494265709,"version":0},"$loki":9},{"_apiVersion":"1.0","_createdAt":1530494266722,"_lastModifiedAt":1520520974089,"_version":1,"excerpt":"Have a suggestion? Want to partner with us? Looking for a job?","icon":"mail","published":true,"slug":"contact","title":"Contact","_createdBy":"api-client","_id":"802f9414-cacb-42f1-af5f-5d507c15c66a","meta":{"revision":0,"created":1530494266723,"version":0},"$loki":10},{"_apiVersion":"1.0","_createdAt":1530494267740,"_createdBy":"api-client","_lastModifiedAt":1529938520167,"_lastModifiedBy":"cloud-stJ8lUzIWT","_version":1,"excerpt":"Marketplace is where you'll find decentralized apps that run on the DADI network – starting with those from our own engineers and partners and growing in time to include others contributed by the community","icon":"grid","published":true,"slug":"marketplace","title":"Marketplace","_id":"35690a68-c8e2-42a4-bae3-20f6b8c67e6d","meta":{"revision":0,"created":1530494267740,"version":0},"$loki":11},{"_apiVersion":"1.0","_createdAt":1530494268752,"_lastModifiedAt":1526662319647,"_lastModifiedBy":"cloudlive","_version":1,"body":"","excerpt":"A high level roadmap for DADI technology, showing development milestones for both network and web services. ","icon":"direction","metaDescription":"","metaTitle":"","published":true,"slug":"roadmap","title":"Roadmap","_createdBy":"api-client","_id":"cd945324-6b56-48b3-b9a2-c482b0697d50","meta":{"revision":0,"created":1530494268752,"version":0},"$loki":12}],"idIndex":[1,2,3,4,5,6,7,8,9,10,11,12],"binaryIndices":{},"constraints":null,"uniqueNames":[],"transforms":{},"objType":"pages","dirty":false,"cachedIndex":null,"cachedBinaryIndex":null,"cachedData":null,"adaptiveBinaryIndices":true,"transactional":false,"cloneObjects":false,"cloneMethod":"parse-stringify","asyncListeners":false,"disableChangesApi":true,"autoupdate":false,"ttl":null,"maxId":12,"DynamicViews":[],"events":{"insert":[null],"update":[null],"pre-insert":[],"pre-update":[],"close":[],"flushbuffer":[],"error":[],"delete":[null],"warning":[null]},"changes":[]}],"databaseVersion":1,"engineVersion":1.1,"autosave":true,"autosaveInterval":10000,"autosaveHandle":null,"throttledSaves":true,"options":{"autoload":true,"autosave":true,"autosaveInterval":10000,"persistenceAdapter":null,"serializationMethod":"normal","destructureDelimiter":"$<\n","recursiveWait":true,"recursiveWaitLimit":false,"recursiveWaitLimitDuration":2000,"started":1530612411244},"persistenceMethod":"fs","persistenceAdapter":null,"verbose":false,"events":{"init":[null],"loaded":[null],"flushChanges":[],"close":[],"changes":[],"warning":[]},"ENV":"NODEJS"}